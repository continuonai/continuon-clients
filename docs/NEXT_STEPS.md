# ContinuonXR: Next Steps Roadmap

**Last Updated:** 2026-01-25  
**Current State:** Learning Partner + Chief Scientist actively training

---

## Current System Status

### Active Training Systems

| System | Status | Cycle | Focus |
|--------|--------|-------|-------|
| **Chief Scientist** | ğŸŸ¢ Running | Every 15 min | Family-oriented goals |
| **Learning Partner** | ğŸŸ¢ Running | Every 5 min | TRAIN phase - simulator episodes |

### Seed Model v4.2.0

- **Parameters:** 12.8M
- **Benchmark Score:** 0.84 (17/23 tests)
- **Level Achieved:** ADVANCED (L3 of 6)
- **RLDS Episodes:** 4,218+
- **Training Cycles:** 124+

---

## Immediate Priorities (This Week)

### 1. ğŸ¯ Navigation Goal (0% â†’ 25%)

The Chief Scientist's top priority. Current blockers:

- [ ] **Room Scanner calibration** - HomeScan simulator needs real room data
- [ ] **Obstacle detection tuning** - OAK-D depth thresholds for furniture
- [ ] **Path planning integration** - Connect A* planner to Brain B

**Test:** Robot navigates living room without collision

### 2. ğŸ‘¤ Face Recognition (0% â†’ 25%)

- [ ] **Capture family faces** - Use trainer_ui to enroll 3-5 family members
- [ ] **Train recognition model** - Fine-tune on captured faces
- [ ] **Greeting behavior** - Brain B says hello when recognizing someone

**Test:** Robot greets family member by name

### 3. ğŸ”§ Hardware Integration

- [ ] **OAK-D streaming** - Verify RGB+Depth pipeline stable
- [ ] **Arm calibration** - 6-axis arm pose accuracy
- [ ] **Audio pipeline** - Voice command recognition

---

## Next Milestones (This Month)

### Phase 1: Basic Home Navigation âœ…â†’ğŸ”„

```
Room Scanner â†’ HomeScan Sim â†’ Train Navigation â†’ Deploy
     â†“              â†“              â†“              â†“
  3D Model    Synthetic Data   Brain B learns   Real robot
```

**Target:** Robot can navigate between rooms on command

### Phase 2: Family Recognition ğŸ”„

- Enroll all family members (face + voice)
- Personalized greetings and interactions
- Remember preferences per person

**Target:** "Hey [Name], want me to get you a drink?"

### Phase 3: Fetch Tasks ğŸ“‹

- Object detection (cups, remotes, phones)
- Grasp planning with arm
- Delivery to person

**Target:** "Bring me the TV remote"

---

## Architecture Overview (Current)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Chief Scientist                        â”‚
â”‚            (Claude Code - Strategic Direction)           â”‚
â”‚         Goals: navigate, recognize, fetch, remind        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ directs
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Learning Partner                       â”‚
â”‚                 (Autonomous Training)                    â”‚
â”‚      OBSERVE â†’ SIMULATE â†’ TRAIN â†’ TRANSFER â†’ DEPLOY     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ trains
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Brain B                             â”‚
â”‚              (Teachable Robot Brain)                     â”‚
â”‚         Commands â†’ Behaviors â†’ Hardware Control          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ controls
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Hardware                             â”‚
â”‚     OAK-D Camera â”‚ 6-Axis Arm â”‚ Mecanum Drive â”‚ Audio   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Deprecated/Legacy

The following are **no longer primary focus**:

- ~~HOPE Architecture~~ â†’ Replaced by Learning Partner
- ~~Pi 5 + Hailo NPU~~ â†’ OAK-D is primary perception
- ~~Gemma-370m on-device~~ â†’ Using Seed Model v4.2.0
- ~~Manual curiosity sessions~~ â†’ Chief Scientist automates this

---

## Success Metrics

| Goal | Current | Target | Timeline |
|------|---------|--------|----------|
| Navigation accuracy | 0% | 80% | 2 weeks |
| Face recognition | 0% | 90% | 2 weeks |
| Fetch success rate | 0% | 50% | 1 month |
| Voice command accuracy | ~60% | 90% | 2 weeks |

---

## How to Contribute

1. **Check Chief Scientist goals:** `python scripts/compound/chief_scientist.py --goals`
2. **Run a training cycle:** `python scripts/compound/learning_partner.py --train`
3. **Test on hardware:** `cd trainer_ui && python server.py`
4. **Review RLDS episodes:** Check `brain_b_data/` for training data quality

---

*This roadmap is auto-updated by Chief Scientist daemon*
