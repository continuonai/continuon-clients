# Seed Model: Universal Robot Initialization

**Version:** 4.2.0  
**Status:** Production Ready (Benchmark Verified)  
**Date:** 2026-01-02

## Quick Stats

| Metric | Value |
|--------|-------|
| **Parameters** | 12,813,577 (12.8M) |
| **Memory** | 51 MB (model) + 27 MB (encoder) |
| **Architecture** | WaveCore Mamba SSM + CMS 3-Level Memory |
| **Embedding** | Self-contained (6.7M, 768-dim) or EmbeddingGemma-300m |
| **Inference** | 50+ Hz (20ms/step) - real-time capable |
| **Benchmark Score** | 0.84 (14/15 progressive tests) |
| **Highest Level** | ADVANCED (L3 of 5) |
| **CMS Levels** | 3 (Fast/Mid/Slow) with write-back |
| **RLDS Episodes** | 4,218 |
| **HAL Discovery** | USB/I2C/PCIe accessory detection |

## Progressive Benchmark Results

The seed model has been validated through a 5-level progressive benchmark:

| Level | Tests | Score | Capabilities Verified |
|-------|-------|-------|----------------------|
| **L1 BASIC** | 3/3 âœ… | 1.00 | Output stability, inference speed (50+ Hz), non-trivial output |
| **L2 INTERMEDIATE** | 3/3 âœ… | 0.82 | Command differentiation, state evolution, spatial understanding |
| **L3 ADVANCED** | 3/3 âœ… | 0.84 | Memory persistence, context switching, hierarchical commands |
| **L4 EXPERT** | 2/3 âš ï¸ | 0.71 | Error recovery, multi-step planning (safety handled by Ring 0) |
| **L5 AUTONOMOUS** | 3/3 âœ… | 0.92 | Self-monitoring, continuous learning, world model prediction |

**Overall: 0.84 score, 14/15 tests passed, Highest Level: ADVANCED**

Run benchmark: `python -m continuonbrain.eval.progressive_benchmark`

## Overview

The **Seed Model** is the universal initialization point for every robot in the Continuon ecosystem. It is not a temporary bootstrapâ€”it is a permanent, hardware-agnostic core that:

- **Initializes** every new robot that connects to the ecosystem
- **Runs on any chip**: ARM, x64, RISC-V, quantum, neuromorphic
- **Provides** the foundation for all higher-level capabilities
- **Evolves** through continuous learning while maintaining core stability
- **Golden Rule**: Must run on devices with <8GB RAM

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SEED MODEL: UNIVERSAL FOUNDATION                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  New Robot Connects                                                          â”‚
â”‚       â”‚                                                                      â”‚
â”‚       â–¼                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                         SEED MODEL                                      â”‚â”‚
â”‚  â”‚                   (Hardware-Agnostic Core)                              â”‚â”‚
â”‚  â”‚                                                                          â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚â”‚
â”‚  â”‚  â”‚ ARM (Pi5)   â”‚  â”‚ x64 (PC)    â”‚  â”‚ RISC-V      â”‚  â”‚ Quantum     â”‚    â”‚â”‚
â”‚  â”‚  â”‚ Jetson      â”‚  â”‚ Server      â”‚  â”‚ Edge        â”‚  â”‚ Future      â”‚    â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚â”‚
â”‚  â”‚                                                                          â”‚â”‚
â”‚  â”‚  Universal Capabilities:                                                 â”‚â”‚
â”‚  â”‚  â€¢ World Model (next-token prediction)                                  â”‚â”‚
â”‚  â”‚  â€¢ Context Graph (relational reasoning)                                 â”‚â”‚
â”‚  â”‚  â€¢ Semantic Search (768-dim embeddings)                                 â”‚â”‚
â”‚  â”‚  â€¢ Decision Traces (explainability)                                     â”‚â”‚
â”‚  â”‚  â€¢ CMS Memory (multi-timescale)                                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚       â”‚                                                                      â”‚
â”‚       â–¼                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                    CONTINUOUS EVOLUTION                                  â”‚â”‚
â”‚  â”‚  Seed â†’ Experience â†’ Local Learning â†’ Cloud Aggregation â†’ OTA Update   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The ContinuonBrain seed model demonstrates advanced embodied AI capabilities through a unified architecture that integrates:

- **World Models** for predictive next-token generation
- **Context Graphs** for relational reasoning
- **Semantic Search** via EmbeddingGemma-300m
- **Decision Traces** for explainable behavior
- **Multi-Timescale Memory** (CMS) for temporal abstraction
- **Ring 0 Safety Kernel** for guaranteed safety at the highest privilege level

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SEED MODEL CAPABILITIES                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ World Model  â”‚  â”‚ Context      â”‚  â”‚ Semantic     â”‚  â”‚ Decision     â”‚    â”‚
â”‚  â”‚ Prediction   â”‚  â”‚ Graph        â”‚  â”‚ Search       â”‚  â”‚ Traces       â”‚    â”‚
â”‚  â”‚              â”‚  â”‚ Reasoning    â”‚  â”‚              â”‚  â”‚              â”‚    â”‚
â”‚  â”‚ Next-token   â”‚  â”‚ Entity/      â”‚  â”‚ EmbeddingGemmaâ”‚  â”‚ Explainable  â”‚    â”‚
â”‚  â”‚ + action     â”‚  â”‚ Relation     â”‚  â”‚ 768-dim      â”‚  â”‚ Provenance   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                 â”‚                 â”‚                 â”‚            â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                    WaveCore (Mamba SSM + Spectral)                     â”‚â”‚
â”‚  â”‚                    3.4M params | O(n) complexity | 231 step/s          â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                    CMS Memory (3-level hierarchical)                   â”‚â”‚
â”‚  â”‚                    Fast (100ms) | Mid (10s) | Slow (âˆ)                 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Scaling Roadmap (Golden Rule: <8GB RAM)

The seed model is designed to scale over time while always fitting within devices with less than 8GB RAM.

### Memory Budget (8GB Device)

| Component | Current | After Optimization |
|-----------|---------|-------------------|
| OS + Python + JAX | 2.0 GB | 2.0 GB |
| Seed Model (WaveCore) | 0.05 GB | 0.05 GB |
| EmbeddingGemma | 1.2 GB | âŒ REMOVED |
| Self-Contained Encoder | â€” | 0.03 GB |
| CMS Memory | 0.5 GB | 0.5 GB |
| RLDS Episodes | 1.0 GB | 1.0 GB |
| Context Graph + Traces | 0.3 GB | 0.3 GB |
| Safety Kernel | 0.1 GB | 0.1 GB |
| **TOTAL** | **5.15 GB** | **3.98 GB** |
| **HEADROOM** | **2.85 GB** | **4.02 GB** |

**Maximum Parameters:** ~1.2B (float32) or ~2.4B (float16)

### Scaling Tiers

| Version | Parameters | Memory | Speed | Benchmark | Status |
|---------|------------|--------|-------|-----------|--------|
| v2.0 | 1M | 4 MB | 404 step/s | â€” | âœ… Released |
| v3.0 | 3.4M | 14 MB | 231 step/s | â€” | âœ… Released |
| **v4.2** | **12.8M** | **51 MB** | **50 step/s** | **0.84** | **âœ… Current** |
| v5.0 | 50M | 200 MB | ~20 step/s | 0.90+ | ğŸ”¶ Q1 2026 |
| v6.0 | 200M | 800 MB | ~10 step/s | 0.95+ | ğŸ”¶ Q2 2026 |

### Dimension Scaling

| Version | d_s | d_w | d_p | d_e | CMS Sizes | CMS Dims |
|---------|-----|-----|-----|-----|-----------|----------|
| v2.0 | 128 | 128 | 64 | 128 | 32/64/128 | 64/128/256 |
| v3.0 | 256 | 256 | 128 | 256 | 64/128/256 | 128/256/512 |
| **v4.2** | **512** | **512** | **256** | **512** | **128/256/512** | **256/512/1024** |
| v5.0 | 1024 | 1024 | 512 | 1024 | 256/512/1024 | 512/1024/2048 |
| v6.0 | 2048 | 2048 | 1024 | 2048 | 512/1024/2048 | 1024/2048/4096 |

See `continuonbrain/jax_models/scaling_configs.py` for tier definitions.

---

## 0. Ring 0 Safety Kernel

The **Safety Kernel** operates at Ring 0 (highest privilege) like the Unix kernel. It is the foundation upon which all other capabilities are built, ensuring safe operation at all times.

### Ring Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           RING 0 - SAFETY KERNEL                             â”‚
â”‚                       (HIGHEST PRIVILEGE - CANNOT BE BYPASSED)               â”‚
â”‚                                                                              â”‚
â”‚  â€¢ Emergency Stop - Always available, triggers within 100ms                 â”‚
â”‚  â€¢ Safety Bounds - Enforces workspace/velocity/force limits                 â”‚
â”‚  â€¢ Protocol 66 - 23 safety rules covering motion, force, thermal, etc.     â”‚
â”‚  â€¢ Watchdog - Self-monitoring at 10Hz, triggers E-Stop on failure          â”‚
â”‚  â€¢ Hardware E-Stop - Direct GPIO control (bypasses all software)           â”‚
â”‚                                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                       RING 1 - HARDWARE ABSTRACTION                          â”‚
â”‚  â€¢ Sensor drivers (camera, depth, IMU, force/torque)                        â”‚
â”‚  â€¢ Actuator interfaces (motors, servos, grippers)                           â”‚
â”‚  â€¢ Safety kernel has direct access to these                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                         RING 2 - CORE RUNTIME                                â”‚
â”‚  â€¢ Seed Model (WaveCore + CMS + Context Graph)                              â”‚
â”‚  â€¢ Inference Router, Decision Traces                                        â”‚
â”‚  â€¢ ALL actions filtered through Ring 0 before execution                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          RING 3 - USER SPACE                                 â”‚
â”‚  â€¢ Chat interface, API server, UI / Applications                            â”‚
â”‚  â€¢ LOWEST privilege - cannot modify safety parameters                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Principles

| Principle | Implementation |
|-----------|----------------|
| **First to Boot** | Safety kernel initializes on import, before any other component |
| **Cannot Disable** | No code path exists to disable the safety kernel |
| **Veto Power** | All actions pass through `SafetyKernel.allow_action()` |
| **Highest Priority** | Real-time scheduling (SCHED_FIFO) when available |
| **Hardware E-Stop** | Direct GPIO pin for hardware emergency stop |
| **Self-Monitoring** | Watchdog thread detects failures and triggers E-Stop |
| **Survives Shutdown** | atexit and signal handlers ensure safe shutdown |

### Boot Sequence

```
1. Python process starts
   â†“
2. Any module imports continuonbrain.safety
   â†“
3. SafetyKernel.__init__() runs automatically (Ring 0)
   â†“
4. Ring 0 protections activated:
   â€¢ atexit handler registered
   â€¢ Signal handlers registered (SIGTERM, SIGINT)
   â€¢ Real-time priority set (if available)
   â€¢ Watchdog thread started (10Hz monitoring)
   â€¢ Hardware E-Stop initialized (if GPIO available)
   â†“
5. Safety kernel ready - all other components can now initialize
   â†“
6. All runtime actions pass through SafetyKernel.allow_action()
```

### Protocol 66 Safety Rules

| Category | Rules | Examples |
|----------|-------|----------|
| **Motion** | 4 | Max joint velocity (2 rad/s), E-Stop response (<100ms) |
| **Force** | 3 | Max contact force (50N), collision detection (5N threshold) |
| **Workspace** | 3 | Boundary enforcement (0.8m sphere), forbidden zones |
| **Human** | 3 | Human detection (2m), reduced speed near humans (0.25 m/s) |
| **Thermal** | 2 | CPU/motor temperature limits |
| **Electrical** | 2 | Voltage/current monitoring |
| **Software** | 3 | Watchdog, command validation, fallback mode |
| **Emergency** | 3 | E-Stop, safe state, recovery procedure |

### Usage

```python
from continuonbrain.safety import SafetyKernel

# All actions must pass through Ring 0
def execute_action(action):
    if SafetyKernel.allow_action(action):
        # Action is safe to execute
        return actuator.execute(action)
    else:
        # Action blocked by safety kernel
        return safe_fallback()

# Emergency stop (always works, cannot be blocked)
SafetyKernel.emergency_stop("Collision detected")

# Check system safety
if SafetyKernel.is_safe():
    # Normal operation
    pass
else:
    # System in safe mode, waiting for reset
    pass
```

See `continuonbrain/safety/README.md` for full documentation.

---

## 1. World Model: Predictive Next-Token Generation

The seed model implements a **generative world model** that predicts future states, enabling:

- **Action-conditioned prediction**: Given current state `s_t` and action `a_t`, predict `s_{t+1}`
- **Multi-step rollouts**: Plan trajectories in latent space
- **Counterfactual reasoning**: "What if I had done X instead?"

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WORLD MODEL PREDICTION                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Observation (o_t)                                               â”‚
â”‚       â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Encoder: VQ-VAE / Vision Transformer                         â”‚â”‚
â”‚  â”‚ o_t â†’ z_t (discrete tokens or continuous embedding)          â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚       â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ WaveCore (Mamba SSM)                                         â”‚â”‚
â”‚  â”‚ h_t = SSM(h_{t-1}, z_t, a_{t-1})                             â”‚â”‚
â”‚  â”‚                                                               â”‚â”‚
â”‚  â”‚ State evolution: dh/dt = Ah + Bx                             â”‚â”‚
â”‚  â”‚ Selective gating: Î” = softplus(Linear(x))                    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚       â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Next-Token Prediction Head                                    â”‚â”‚
â”‚  â”‚ p(z_{t+1} | h_t, a_t) = softmax(W_z Â· h_t)                   â”‚â”‚
â”‚  â”‚ p(r_t | h_t) = Reward prediction                              â”‚â”‚
â”‚  â”‚ p(done | h_t) = Episode termination                           â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation

```python
# continuonbrain/jax_models/core_model.py

class WorldModelHead(nn.Module):
    """Next-token prediction for world modeling."""
    config: CoreModelConfig
    
    @nn.compact
    def __call__(self, h_t: jnp.ndarray, a_t: jnp.ndarray) -> Dict[str, jnp.ndarray]:
        # Combine hidden state with action
        ha = jnp.concatenate([h_t, a_t], axis=-1)
        
        # Predict next latent
        z_next_logits = nn.Dense(self.config.num_vq_vocab)(ha)
        
        # Predict reward
        reward_pred = nn.Dense(1)(ha)
        
        # Predict done
        done_logits = nn.Dense(2)(ha)
        
        return {
            'z_next_logits': z_next_logits,
            'reward_pred': reward_pred,
            'done_logits': done_logits,
        }
```

### Training Objective

```python
# World model loss (next-token + reward + done)
L_world = L_reconstruction + Î»_r * L_reward + Î»_d * L_done

# Where:
# L_reconstruction = CrossEntropy(z_next_pred, z_next_true)
# L_reward = MSE(r_pred, r_true)
# L_done = CrossEntropy(done_pred, done_true)
```

---

## 2. Context Graph Reasoning

The seed model maintains a **context graph** for relational reasoning about entities, objects, and their relationships.

### Graph Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTEXT GRAPH                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Nodes (Entities):                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Object  â”‚  â”‚ Person  â”‚  â”‚ Locationâ”‚  â”‚ Concept â”‚            â”‚
â”‚  â”‚ "cup"   â”‚  â”‚ "user"  â”‚  â”‚ "table" â”‚  â”‚ "hot"   â”‚            â”‚
â”‚  â”‚ emb[768]â”‚  â”‚ emb[768]â”‚  â”‚ emb[768]â”‚  â”‚ emb[768]â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â”‚
â”‚       â”‚            â”‚            â”‚            â”‚                  â”‚
â”‚  Edges (Relations):                                              â”‚
â”‚       â”‚            â”‚            â”‚            â”‚                  â”‚
â”‚       â””â”€â”€â”€â”€ on â”€â”€â”€â”€â”¼â”€â”€â”€â”€ near â”€â”€â”˜            â”‚                  â”‚
â”‚                    â”‚                         â”‚                  â”‚
â”‚                    â””â”€â”€â”€ wants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                  â”‚
â”‚  Edge Attributes:                                                â”‚
â”‚  - type: spatial | causal | semantic | temporal                  â”‚
â”‚  - weight: attention-based salience                              â”‚
â”‚  - provenance: which observation created this edge               â”‚
â”‚  - timestamp: when last updated                                  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Graph Operations

```python
# continuonbrain/core/context_graph_store.py

class ContextGraphStore:
    """Graph-based reasoning for embodied agents."""
    
    def add_observation(self, obs: Observation) -> None:
        """Update graph from visual/sensor observation."""
        # Extract entities
        entities = self.vision_core.detect_objects(obs.image)
        
        # Add/update nodes
        for entity in entities:
            node = GraphNode(
                id=entity.track_id,
                type=entity.category,
                embedding=self.embed(entity.label),
                attributes={
                    'bbox': entity.bbox,
                    'confidence': entity.score,
                    'depth': entity.depth,
                },
                timestamp=obs.timestamp,
            )
            self.graph.upsert_node(node)
        
        # Infer spatial relations
        for e1, e2 in itertools.combinations(entities, 2):
            relation = self.infer_spatial_relation(e1, e2)
            if relation:
                self.graph.add_edge(e1.id, e2.id, relation)
    
    def query(self, question: str) -> List[GraphNode]:
        """Semantic graph traversal for question answering."""
        query_emb = self.embed(question)
        
        # Find relevant nodes
        candidates = self.graph.semantic_search(query_emb, k=10)
        
        # Expand via relations
        expanded = self.graph.expand_neighbors(candidates, hops=2)
        
        return self.rank_by_relevance(expanded, query_emb)
    
    def reason(self, goal: str) -> ReasoningTrace:
        """Multi-hop reasoning over graph."""
        trace = ReasoningTrace()
        
        # Parse goal into subgoals
        subgoals = self.parse_goal(goal)
        
        for subgoal in subgoals:
            # Find relevant context
            context = self.query(subgoal)
            
            # Apply reasoning rules
            conclusions = self.apply_rules(context, subgoal)
            
            trace.add_step(subgoal, context, conclusions)
        
        return trace
```

### Graph Attention for Reasoning

```python
# Multi-head graph attention for relation reasoning
class GraphAttention(nn.Module):
    """Graph attention for context reasoning."""
    
    @nn.compact
    def __call__(self, nodes: jnp.ndarray, edges: jnp.ndarray, query: jnp.ndarray):
        # nodes: [N, d], edges: [E, 2], query: [d]
        
        # Compute attention scores
        Q = nn.Dense(64)(query)
        K = nn.Dense(64)(nodes)
        V = nn.Dense(64)(nodes)
        
        # Edge-aware attention
        attn = jnp.einsum('d,nd->n', Q, K) / jnp.sqrt(64)
        attn = nn.softmax(attn)
        
        # Aggregate
        context = jnp.einsum('n,nd->d', attn, V)
        
        return context, attn
```

---

## 3. Semantic Search

The seed model uses **EmbeddingGemma-300m** for 768-dimensional semantic embeddings, enabling:

- **Experience retrieval**: Find similar past interactions
- **Knowledge grounding**: Connect observations to learned concepts
- **Cross-modal search**: Match text queries to visual memories

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SEMANTIC SEARCH PIPELINE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Query: "Where did I put my keys?"                               â”‚
â”‚       â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ EmbeddingGemma-300m                                          â”‚â”‚
â”‚  â”‚ "Where did I put my keys?" â†’ q âˆˆ â„^768                       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚       â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Vector Index (FAISS / ScaNN)                                  â”‚â”‚
â”‚  â”‚ top_k = argmax_i cos(q, e_i)                                 â”‚â”‚
â”‚  â”‚                                                               â”‚â”‚
â”‚  â”‚ Indexed memories:                                             â”‚â”‚
â”‚  â”‚ - RLDS episodes (91K steps)                                   â”‚â”‚
â”‚  â”‚ - Conversation history                                        â”‚â”‚
â”‚  â”‚ - Visual observations                                         â”‚â”‚
â”‚  â”‚ - Context graph nodes                                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚       â†“                                                          â”‚
â”‚  Retrieved: [                                                    â”‚
â”‚    "You placed keys on the kitchen counter (0.89)",              â”‚
â”‚    "Keys were last seen near the door (0.76)",                   â”‚
â”‚    "Conversation about losing keys yesterday (0.71)",            â”‚
â”‚  ]                                                               â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation

```python
# continuonbrain/services/embedding_gemma.py

class EmbeddingGemmaEncoder:
    """768-dim semantic embeddings for search and retrieval."""
    
    DEFAULT_MODEL_ID = "google/embeddinggemma-300m"
    
    def encode_query(self, query: str) -> np.ndarray:
        """Encode search query with query-specific prompt."""
        prompt = f"search_query: {query}"
        return self.model.encode([prompt])[0]
    
    def encode_document(self, doc: str) -> np.ndarray:
        """Encode document/memory for indexing."""
        prompt = f"search_document: {doc}"
        return self.model.encode([prompt])[0]
    
    def encode_batch(self, texts: List[str], is_query: bool = False) -> np.ndarray:
        """Batch encode for efficiency."""
        prefix = "search_query: " if is_query else "search_document: "
        prompts = [prefix + t for t in texts]
        return self.model.encode(prompts)

# Usage in experience logger
class ExperienceLogger:
    def search_conversations(self, query: str, max_results: int = 5) -> List[Dict]:
        query_embedding = self.encoder.encode_query(query)
        
        # Search vector index
        distances, indices = self.index.search(
            query_embedding.reshape(1, -1), 
            max_results
        )
        
        # Return ranked results with relevance scores
        return [
            {
                **self.memories[idx],
                'relevance': 1 - distances[0][i],  # Convert distance to similarity
            }
            for i, idx in enumerate(indices[0])
        ]
```

---

## 4. Decision Traces

The seed model logs **decision traces** for explainable AI, enabling:

- **Provenance tracking**: Why did the agent take this action?
- **Debugging**: Trace failures back to root causes
- **Learning from mistakes**: Identify and correct decision patterns

### Trace Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION TRACE                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  trace_id: "dt_20260102_100523_abc123"                          â”‚
â”‚  timestamp: 2026-01-02T10:05:23.456Z                            â”‚
â”‚  agent: "hope_agent_manager"                                     â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Input Context                                              â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ observation: { image: "...", depth: "...", pose: [...] }   â”‚  â”‚
â”‚  â”‚ user_query: "Pick up the red cup"                          â”‚  â”‚
â”‚  â”‚ memory_context: [                                          â”‚  â”‚
â”‚  â”‚   { text: "Red cup is on table", salience: 0.92 },         â”‚  â”‚
â”‚  â”‚   { text: "User prefers left hand", salience: 0.71 },      â”‚  â”‚
â”‚  â”‚ ]                                                          â”‚  â”‚
â”‚  â”‚ graph_context: [                                           â”‚  â”‚
â”‚  â”‚   { node: "red_cup", relation: "on", target: "table" },    â”‚  â”‚
â”‚  â”‚ ]                                                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Reasoning Steps                                            â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ 1. Goal parsing: "pick up" â†’ GRASP action                  â”‚  â”‚
â”‚  â”‚ 2. Object grounding: "red cup" â†’ object_id: "cup_42"       â”‚  â”‚
â”‚  â”‚ 3. Spatial reasoning: cup at (0.3, 0.5, 0.1) in base_link  â”‚  â”‚
â”‚  â”‚ 4. Path planning: 5-waypoint trajectory generated          â”‚  â”‚
â”‚  â”‚ 5. Safety check: PASSED (no obstacles, reachable)          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Decision                                                   â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ action: "grasp"                                            â”‚  â”‚
â”‚  â”‚ target: "cup_42"                                           â”‚  â”‚
â”‚  â”‚ confidence: 0.94                                           â”‚  â”‚
â”‚  â”‚ alternatives: [                                            â”‚  â”‚
â”‚  â”‚   { action: "ask_clarify", confidence: 0.03 },             â”‚  â”‚
â”‚  â”‚   { action: "point_to", confidence: 0.02 },                â”‚  â”‚
â”‚  â”‚ ]                                                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Outcome                                                    â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ success: true                                              â”‚  â”‚
â”‚  â”‚ reward: 1.0                                                â”‚  â”‚
â”‚  â”‚ feedback: null                                             â”‚  â”‚
â”‚  â”‚ error: null                                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation

```python
# continuonbrain/core/decision_trace_logger.py

@dataclass
class DecisionTrace:
    trace_id: str
    timestamp: datetime
    agent: str
    input_context: InputContext
    reasoning_steps: List[ReasoningStep]
    decision: Decision
    outcome: Optional[Outcome] = None

class DecisionTraceLogger:
    """Log decision traces for explainability and learning."""
    
    def start_trace(self, agent: str, observation: Observation) -> DecisionTrace:
        return DecisionTrace(
            trace_id=f"dt_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid4().hex[:6]}",
            timestamp=datetime.now(),
            agent=agent,
            input_context=InputContext(observation=observation),
            reasoning_steps=[],
            decision=None,
        )
    
    def add_reasoning_step(self, trace: DecisionTrace, step: ReasoningStep) -> None:
        trace.reasoning_steps.append(step)
    
    def set_decision(self, trace: DecisionTrace, decision: Decision) -> None:
        trace.decision = decision
    
    def record_outcome(self, trace: DecisionTrace, outcome: Outcome) -> None:
        trace.outcome = outcome
        
        # Log to RLDS for learning
        self.log_to_rlds(trace)
        
        # Update context graph
        self.context_graph.add_decision_edge(trace)
    
    def log_to_rlds(self, trace: DecisionTrace) -> None:
        """Convert trace to RLDS episode step."""
        step = {
            'observation': trace.input_context.to_dict(),
            'action': trace.decision.action,
            'reward': trace.outcome.reward if trace.outcome else 0.0,
            'metadata': {
                'trace_id': trace.trace_id,
                'reasoning_steps': [s.to_dict() for s in trace.reasoning_steps],
                'confidence': trace.decision.confidence,
            }
        }
        self.rlds_logger.log_step(step)
```

---

## 5. Multi-Timescale Memory (CMS)

The **Continuous Memory System** provides hierarchical temporal abstraction:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CMS HIERARCHY                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Level 0: FAST (Episodic)                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Ï„ = 100ms | decay = 0.9 | slots = 64                        â”‚â”‚
â”‚  â”‚ Content: Raw sensory, motor commands, immediate context     â”‚â”‚
â”‚  â”‚ Update: Every perception cycle                               â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                         â”‚                                        â”‚
â”‚                         â–¼ consolidation                          â”‚
â”‚  Level 1: MID (Working)                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Ï„ = 10s | decay = 0.99 | slots = 128                        â”‚â”‚
â”‚  â”‚ Content: Current task state, active goals, recent events    â”‚â”‚
â”‚  â”‚ Update: Event-driven (significant changes)                   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                         â”‚                                        â”‚
â”‚                         â–¼ consolidation                          â”‚
â”‚  Level 2: SLOW (Semantic)                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Ï„ = âˆ | decay = 0.999 | slots = 256                         â”‚â”‚
â”‚  â”‚ Content: Learned skills, persistent knowledge, identity     â”‚â”‚
â”‚  â”‚ Update: Sleep-like consolidation (background)                â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                  â”‚
â”‚  Memory Operations:                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚
â”‚  READ:  c_t = CMS_Read(M, query)                                â”‚
â”‚         Attention-weighted retrieval across all levels           â”‚
â”‚                                                                  â”‚
â”‚  WRITE: M_t = (1-Î±)M_{t-1} + Î±Â·new_memory                       â”‚
â”‚         Content-addressable write with salience gating           â”‚
â”‚                                                                  â”‚
â”‚  CONSOLIDATE: M_slow = consolidate(M_fast, M_mid)               â”‚
â”‚         Pattern compression, schema extraction                   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation (JAX)

```python
# continuonbrain/jax_models/cms_jax.py

class CMSMemory(nn.Module):
    """JAX implementation of Continuous Memory System."""
    config: CMSConfig
    
    @nn.compact
    def __call__(
        self,
        query: jnp.ndarray,       # [B, d_query]
        memories: List[jnp.ndarray],  # [[B, N_l, d_l], ...]
        keys: List[jnp.ndarray],      # [[B, N_l, d_k], ...]
    ) -> Tuple[jnp.ndarray, List[jnp.ndarray]]:
        """Read from hierarchical memory."""
        
        contexts = []
        attentions = []
        
        for level, (M, K) in enumerate(zip(memories, keys)):
            # Compute attention
            # scores[b, n] = query[b] Â· K[b, n] / âˆšd_k
            scores = jnp.einsum('bd,bnd->bn', query, K) / jnp.sqrt(self.config.d_k)
            attn = nn.softmax(scores, axis=-1)
            
            # Retrieve context
            # c[b, d] = Î£_n attn[b, n] * M[b, n, d]
            c = jnp.einsum('bn,bnd->bd', attn, M)
            
            contexts.append(c)
            attentions.append(attn)
        
        # Hierarchical mixing
        # Learn to weight different timescales
        stacked = jnp.stack(contexts, axis=1)  # [B, L, d]
        mix_weights = nn.softmax(
            nn.Dense(self.config.num_levels)(query), axis=-1
        )  # [B, L]
        
        mixed = jnp.einsum('bl,bld->bd', mix_weights, stacked)
        
        return mixed, attentions
    
    def write(
        self,
        memories: List[jnp.ndarray],
        keys: List[jnp.ndarray],
        new_content: jnp.ndarray,
        new_key: jnp.ndarray,
        salience: float,
    ) -> Tuple[List[jnp.ndarray], List[jnp.ndarray]]:
        """Write to memory with decay and salience gating."""
        
        updated_memories = []
        updated_keys = []
        
        for level, (M, K) in enumerate(zip(memories, keys)):
            decay = self.config.decays[level]
            
            # Find least salient slot
            saliences = jnp.linalg.norm(M, axis=-1)  # [B, N]
            min_idx = jnp.argmin(saliences, axis=-1)  # [B]
            
            # Decay existing memories
            M_decayed = M * decay
            K_decayed = K * decay
            
            # Write new content if salient enough
            if salience > self.config.write_threshold:
                # Replace least salient slot
                M_new = M_decayed.at[:, min_idx].set(new_content)
                K_new = K_decayed.at[:, min_idx].set(new_key)
            else:
                M_new = M_decayed
                K_new = K_decayed
            
            updated_memories.append(M_new)
            updated_keys.append(K_new)
        
        return updated_memories, updated_keys
```

---

## 6. Advanced Embodied AI Concepts

### 6.1 Action-Perception Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EMBODIED ACTION-PERCEPTION LOOP               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Sensors â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Encoder â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â”‚
â”‚      â–²                                               â”‚          â”‚
â”‚      â”‚                                               â–¼          â”‚
â”‚      â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚      â”‚                                    â”‚ World Model      â”‚  â”‚
â”‚      â”‚                                    â”‚ State: h_t       â”‚  â”‚
â”‚      â”‚                                    â”‚ Predict: áº‘_{t+1} â”‚  â”‚
â”‚      â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚      â”‚                                             â”‚            â”‚
â”‚      â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚      â”‚         â”‚                                                â”‚
â”‚      â”‚         â–¼                                                â”‚
â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚      â”‚  â”‚ CMS Memory â”‚â—„â”€â”€â”€â–ºâ”‚ HOPE Core   â”‚â—„â”€â”€â”€â–ºâ”‚ Context    â”‚  â”‚
â”‚      â”‚  â”‚ Retrieval  â”‚     â”‚ Reasoning   â”‚     â”‚ Graph      â”‚  â”‚
â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚      â”‚                            â”‚                             â”‚
â”‚      â”‚                            â–¼                             â”‚
â”‚      â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚      â”‚                     â”‚ Policy     â”‚                       â”‚
â”‚      â”‚                     â”‚ Ï€(a|s,g)   â”‚                       â”‚
â”‚      â”‚                     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚      â”‚                           â”‚                              â”‚
â”‚      â”‚                           â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”´â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ Robot â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Actuators  â”‚                        â”‚
â”‚  â”‚ Body  â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜                                                      â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Intrinsic Motivation

```python
# Curiosity-driven exploration
class IntrinsicMotivation:
    """Compute intrinsic rewards for exploration."""
    
    def compute_curiosity(self, state: jnp.ndarray, next_state: jnp.ndarray) -> float:
        """Prediction error as curiosity signal."""
        predicted = self.forward_model(state)
        error = jnp.mean((predicted - next_state) ** 2)
        return float(error)
    
    def compute_empowerment(self, state: jnp.ndarray) -> float:
        """Mutual information between actions and outcomes."""
        # I(a; s' | s) - how much control does the agent have?
        return self.empowerment_estimator(state)
    
    def compute_novelty(self, state: jnp.ndarray) -> float:
        """Distance from visited states."""
        distances = self.memory.query_nearest(state, k=5)
        return float(jnp.mean(distances))
```

### 6.3 Skill Learning

```python
# Hierarchical skill learning
class SkillLibrary:
    """Learned reusable behaviors."""
    
    skills: Dict[str, Skill] = {
        'grasp': Skill(
            preconditions=['object_visible', 'arm_reachable'],
            policy=GraspPolicy(),
            postconditions=['object_held'],
        ),
        'place': Skill(
            preconditions=['object_held', 'target_visible'],
            policy=PlacePolicy(),
            postconditions=['object_placed'],
        ),
        'navigate': Skill(
            preconditions=['target_known'],
            policy=NavigatePolicy(),
            postconditions=['at_target'],
        ),
    }
    
    def select_skill(self, goal: str, context: Dict) -> Optional[Skill]:
        """Select appropriate skill for goal."""
        for name, skill in self.skills.items():
            if skill.matches_goal(goal) and skill.check_preconditions(context):
                return skill
        return None
```

---

## 7. Hardware-Agnostic Architecture

The seed model is designed to run on **any compute platform**â€”from edge devices to quantum processors.

### Supported Architectures

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HARDWARE PORTABILITY MATRIX                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Architecture      â”‚ Runtime        â”‚ Accelerator      â”‚ Status            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  ARM64 (Pi5)       â”‚ JAX CPU        â”‚ Hailo-8 NPU      â”‚ âœ… Primary        â”‚
â”‚  ARM64 (Jetson)    â”‚ JAX CUDA       â”‚ Tensor Cores     â”‚ âœ… Supported      â”‚
â”‚  x86_64 (PC)       â”‚ JAX CPU/CUDA   â”‚ NVIDIA GPU       â”‚ âœ… Supported      â”‚
â”‚  x86_64 (Cloud)    â”‚ JAX TPU        â”‚ TPU v4/v5        â”‚ âœ… Supported      â”‚
â”‚  RISC-V            â”‚ Portable C     â”‚ Custom NPU       â”‚ ğŸ”¶ Planned        â”‚
â”‚  Apple Silicon     â”‚ JAX Metal      â”‚ ANE              â”‚ ğŸ”¶ Planned        â”‚
â”‚  Quantum           â”‚ Pennylane/JAX  â”‚ QPU              â”‚ ğŸ”® Future         â”‚
â”‚  Neuromorphic      â”‚ Lava/Loihi     â”‚ Intel Loihi 2    â”‚ ğŸ”® Future         â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Portability Principles

1. **Pure JAX Core**: All core computations use JAX, which compiles to XLA for any backend
2. **No Hardware-Specific Code in Model**: Hardware abstraction at the inference router level
3. **Graceful Degradation**: Falls back to CPU when accelerators unavailable
4. **Quantization-Ready**: Weights can be quantized for edge deployment

### Backend Selection

```python
# Automatic backend selection based on available hardware
from continuonbrain.jax_models.export.inference_router import InferenceRouter

router = InferenceRouter()

# Automatically selects best available backend:
# 1. TPU (if on Cloud TPU VM)
# 2. CUDA (if NVIDIA GPU available)
# 3. Hailo (if Hailo-8 NPU detected)
# 4. CPU (always available fallback)

output = router.infer(observation)
print(f"Backend used: {router.active_backend}")  # e.g., "hailo", "cuda", "cpu"
```

### Edge Deployment

```python
# Optimized for edge devices (Pi5, Jetson, etc.)
from continuonbrain.jax_models.config import CoreModelConfig

# Automatic config based on detected hardware
config = CoreModelConfig.for_device()  # Detects and optimizes

# Manual override for specific hardware
config_pi5 = CoreModelConfig.pi5_optimized()      # 172K params, fits in 512MB
config_jetson = CoreModelConfig.jetson_optimized() # Larger model for Jetson
config_cloud = CoreModelConfig.cloud_optimized()   # Full model for TPU
```

### Seed Model Initialization Flow

Every new robot, regardless of hardware, starts with the same seed:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NEW ROBOT INITIALIZATION                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  1. Robot boots for first time                                               â”‚
â”‚       â”‚                                                                      â”‚
â”‚       â–¼                                                                      â”‚
â”‚  2. Hardware detection (RuntimeContext)                                      â”‚
â”‚       â”‚  Detects: CPU arch, RAM, GPU/NPU, sensors                           â”‚
â”‚       â–¼                                                                      â”‚
â”‚  3. Download/verify seed model                                               â”‚
â”‚       â”‚  From: continuon.cloud/seed/v1.0.0/                                 â”‚
â”‚       â”‚  Checksum: verified against manifest                                â”‚
â”‚       â–¼                                                                      â”‚
â”‚  4. Select optimal config for hardware                                       â”‚
â”‚       â”‚  Pi5 â†’ pi5_optimized (172K params)                                  â”‚
â”‚       â”‚  Jetson â†’ jetson_optimized (1M params)                              â”‚
â”‚       â”‚  Cloud â†’ cloud_optimized (10M params)                               â”‚
â”‚       â–¼                                                                      â”‚
â”‚  5. Initialize WaveCore + CMS + Context Graph                               â”‚
â”‚       â”‚  All capabilities ready                                              â”‚
â”‚       â–¼                                                                      â”‚
â”‚  6. Robot operational with full seed capabilities                           â”‚
â”‚       â”‚                                                                      â”‚
â”‚       â–¼                                                                      â”‚
â”‚  7. Begin experience collection â†’ local learning â†’ evolution                â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cross-Platform Serialization

```python
# Seed model weights are platform-agnostic
import numpy as np

# Save (any platform)
weights = {'params': jax.tree_util.tree_map(np.array, params)}
np.savez('seed_model.npz', **weights)

# Load (any platform)
loaded = np.load('seed_model.npz', allow_pickle=True)
params = jax.tree_util.tree_map(jnp.array, dict(loaded))
```

---

## 8. Training Pipeline

### Continuous Learning (Post-Initialization)

After seed initialization, robots learn continuously:

| Phase | Location | Data | Frequency |
|-------|----------|------|-----------|
| **Experience Collection** | Device | RLDS episodes | Continuous |
| **Local Learning** | Device | Fast/Mid loops | Every session |
| **Cloud Aggregation** | TPU | Aggregated RLDS | Daily/Weekly |
| **OTA Distribution** | Cloud â†’ Device | Updated weights | On-demand |

### Seed Model (Core - Never Deprecated)

| Component | Status | Details |
|-----------|--------|---------|
| **WaveCore** | âœ… 172K params | Mamba SSM, O(n) complexity |
| **CMS** | âœ… 3 levels | Fast/Mid/Slow with decay |
| **Scaffold** | ğŸ”¶ Gemma 3n | Chat generation (evolves with updates) |
| **Embeddings** | âœ… 768-dim | EmbeddingGemma (permanent) |
| **RLDS** | âœ… 4,219 episodes | 91K steps available |

### Production Phase (Post-Seed)

```
Device RLDS â”€â”€â–º Cloud TPU Slow Loop â”€â”€â–º OTA Bundle â”€â”€â–º Device Install
                     â”‚
                     â–¼
              WaveCore Production Weights
              (Gemma scaffold deprecated)
```

### Stable Seed Model

The **stable seed model** is the validated, production-ready checkpoint:

| Property | Value |
|----------|-------|
| Location | `/opt/continuonos/brain/model/seed_stable/` |
| Parameters | 644,099 |
| Architecture | WaveCore + CMS |
| Training Steps | 32+ |
| RLDS Episodes | 4,219 |

**Load the stable seed:**

```python
from continuonbrain.seed import load_stable_seed

# Load model and parameters
model, params, manifest = load_stable_seed()

# Run inference
output, info = model.apply(
    {'params': params},
    x_obs=observation,
    a_prev=action,
    r_t=reward,
    s_prev=state['s'],
    w_prev=state['w'],
    p_prev=state['p'],
    cms_memories=state['cms_memories'],
    cms_keys=state['cms_keys'],
)
```

**Promotion Path:**

```
1. Collect RLDS episodes â†’ /opt/continuonos/brain/rlds/episodes/
                 â†“
2. Train seed model â†’ python scripts/train_seed_model.py --steps 500
                 â†“
3. Validate checkpoint â†’ test inference, check for NaN/Inf
                 â†“
4. Promote to stable â†’ --promote flag or manual copy
                 â†“
5. Update manifest â†’ seed_stable/manifest.json
                 â†“
6. Ready for production inference
```

**Train and promote:**

```bash
# Train for 500 steps and promote to stable
python scripts/train_seed_model.py --steps 500 --promote

# Or continue training from existing checkpoint
python scripts/train_seed_model.py --continue --steps 200 --promote
```

---

## 9. References

- [WaveCore Spec](./wavecore-spec.md)
- [HOPE/CMS VLA](./hope-cms-vla.md)
- [CMS Formal Spec](../continuonbrain/docs/CMS_FORMAL_SPEC.md)
- [RCAN Protocol](./rcan-protocol.md)
- [Training Plan](./training-plan.md)
