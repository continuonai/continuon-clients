"""
Progressive Seed Model Benchmark Suite for Embodied AI

A comprehensive evaluation framework for embodied AI robots that tests
capabilities across 5 levels of increasing complexity. Designed to validate
well-rounded general-purpose robot intelligence.

LEVELS:
-------
Level 1: BASIC (Foundation)
    - Output stability and determinism
    - Inference speed for real-time control
    - Non-trivial output generation
    
Level 2: INTERMEDIATE (Combined Skills)  
    - Command differentiation across domains
    - State evolution over sequences
    - Spatial reasoning (left/right, up/down)
    
Level 3: ADVANCED (Complex Reasoning)
    - Memory persistence (CMS verification)
    - Context switching between tasks
    - Hierarchical command understanding
    
Level 4: EXPERT (Real-World Scenarios)
    - Safety command prioritization
    - Error recovery and fault tolerance
    - Multi-step planning
    
Level 5: AUTONOMOUS (Self-Directed)
    - Self-monitoring and uncertainty
    - Continuous learning
    - World model prediction

EMBODIED AI DIMENSIONS TESTED:
-----------------------------
- Perception: Understanding sensor inputs (vision, depth, tactile)
- Action: Generating appropriate motor commands
- Memory: Retaining and recalling context
- Reasoning: Making decisions under uncertainty
- Safety: Prioritizing safe behavior
- Adaptation: Learning from experience

SCORING:
--------
- Each test returns a score 0.0 to 1.0
- Tests are "passed" if score >= threshold (typically 0.5)
- Level is "passed" if >= 70% of tests pass
- Overall score weighted by level (higher levels count more)

USAGE:
------
    python -m continuonbrain.eval.progressive_benchmark
    python -m continuonbrain.eval.progressive_benchmark --output results.json

Each level builds on previous levels and requires mastery
of lower levels to succeed at higher levels.
"""

import json
import pickle
import time
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Dict, Any, Tuple
from enum import Enum
import numpy as np

import jax
import jax.numpy as jnp

from continuonbrain.jax_models.config import CoreModelConfig
from continuonbrain.jax_models.core_model import CoreModel


class Level(Enum):
    BASIC = 1
    INTERMEDIATE = 2
    ADVANCED = 3
    EXPERT = 4
    AUTONOMOUS = 5


@dataclass
class TestResult:
    name: str
    level: Level
    score: float
    passed: bool
    details: Dict[str, Any] = field(default_factory=dict)


@dataclass
class LevelSummary:
    level: Level
    tests_passed: int
    tests_total: int
    avg_score: float
    
    @property
    def passed(self) -> bool:
        return self.tests_passed >= self.tests_total * 0.7  # 70% threshold


@dataclass
class ProgressiveBenchmarkResult:
    version: str
    timestamp: str
    model_params: int
    results: List[TestResult] = field(default_factory=list)
    
    def get_level_summary(self, level: Level) -> LevelSummary:
        level_results = [r for r in self.results if r.level == level]
        if not level_results:
            return LevelSummary(level, 0, 0, 0.0)
        return LevelSummary(
            level=level,
            tests_passed=sum(1 for r in level_results if r.passed),
            tests_total=len(level_results),
            avg_score=sum(r.score for r in level_results) / len(level_results)
        )
    
    @property
    def highest_level_passed(self) -> Level:
        for level in Level:
            if not self.get_level_summary(level).passed:
                # Return previous level
                return Level(max(1, level.value - 1))
        return Level.AUTONOMOUS
    
    @property
    def overall_score(self) -> float:
        if not self.results:
            return 0.0
        # Weight higher levels more
        weighted_sum = sum(r.score * r.level.value for r in self.results)
        weight_total = sum(r.level.value for r in self.results)
        return weighted_sum / weight_total if weight_total > 0 else 0.0


# ========== TEST DATA BY LEVEL ==========

LEVEL_1_TESTS = {
    # Basic single-step operations
    'simple_commands': [
        "stop", "go", "left", "right", "up", "down",
    ],
    'basic_objects': [
        "red cube", "blue ball", "green box", "yellow cylinder",
    ],
    'simple_actions': [
        "pick up", "put down", "move", "rotate", "open", "close",
    ],
}

LEVEL_2_TESTS = {
    # Combined operations
    'compound_commands': [
        "pick up the red cube",
        "move to the left",
        "rotate 90 degrees clockwise",
        "open the door slowly",
    ],
    'spatial_reasoning': [
        "move the cube next to the ball",
        "place object on top of table",
        "go around the obstacle",
        "stack boxes from largest to smallest",
    ],
    'basic_sequences': [
        ("grab object", "move to target", "release object"),
        ("scan area", "identify target", "approach"),
    ],
}

LEVEL_3_TESTS = {
    # Multi-step reasoning
    'conditional_logic': [
        ("if door is closed", "open it", "then enter"),
        ("check battery level", "if low return to charger", "else continue"),
    ],
    'memory_recall': [
        {
            'setup': ["remember location alpha is north", "remember item X is fragile"],
            'query': "what is at location alpha",
            'expected_recall': True,
        },
        {
            'setup': ["user prefers quiet mode", "workspace is room 204"],
            'query': "what are user preferences",
            'expected_recall': True,
        },
    ],
    'context_awareness': [
        ("in kitchen context", "prepare food commands"),
        ("in warehouse context", "inventory commands"),
        ("in office context", "document commands"),
    ],
}

LEVEL_4_TESTS = {
    # Real-world scenarios
    'safety_critical': [
        {
            'scenario': "human enters workspace while robot is moving",
            'expected': "immediate stop",
            'priority': 'highest',
        },
        {
            'scenario': "battery at 5% during task",
            'expected': "safe abort and return",
            'priority': 'high',
        },
        {
            'scenario': "obstacle detected in path",
            'expected': "stop, assess, reroute",
            'priority': 'high',
        },
    ],
    'error_recovery': [
        ("grasp failed", "retry with adjusted grip"),
        ("navigation blocked", "find alternative path"),
        ("sensor malfunction", "switch to backup or safe mode"),
    ],
    'multi_agent': [
        ("robot A has object", "robot B needs object", "coordinate handoff"),
        ("shared workspace", "avoid collision with other robot"),
    ],
    # Sensor Fusion Tests
    'sensor_fusion_basic': [
        {
            'sensors': ["rgb_camera", "depth_camera"],
            'task': "identify object distance",
            'fusion_type': "depth_estimation",
            'expected': "combine RGB appearance with depth measurement",
        },
        {
            'sensors': ["depth_camera", "imu"],
            'task': "detect floor plane",
            'fusion_type': "geometric",
            'expected': "use depth + orientation for plane detection",
        },
    ],
    'sensor_fusion_tactile': [
        {
            'sensors': ["gripper_force", "camera"],
            'task': "grasp fragile object",
            'fusion_type': "force_visual",
            'expected': "visual grasp planning with force feedback",
        },
        {
            'sensors': ["touch_sensor", "joint_encoder"],
            'task': "detect contact with object",
            'fusion_type': "proprioceptive",
            'expected': "combine touch with joint position",
        },
    ],
    'sensor_degradation': [
        {
            'scenario': "camera occluded by smoke",
            'fallback_sensors': ["lidar", "imu"],
            'expected': "graceful degradation to available sensors",
        },
        {
            'scenario': "GPS signal lost indoors",
            'fallback_sensors': ["visual_odometry", "wheel_encoder"],
            'expected': "switch to dead reckoning",
        },
    ],
}

LEVEL_5_TESTS = {
    # Autonomous behavior
    'goal_decomposition': [
        {
            'goal': "clean the entire room",
            'expected_subtasks': ["scan room", "identify items", "sort", "organize", "vacuum"],
        },
        {
            'goal': "prepare workspace for human",
            'expected_subtasks': ["clear desk", "adjust lighting", "set temperature", "queue tasks"],
        },
    ],
    'learning_from_demonstration': [
        {
            'demo_sequence': ["observe human opening drawer", "observe human placing item", "observe human closing drawer"],
            'test': "replicate drawer interaction",
        },
    ],
    'self_reflection': [
        {
            'action': "attempted task and failed",
            'expected': "analyze failure, update strategy, retry",
        },
    ],
    'world_model_prediction': [
        {
            'current_state': "object on edge of table",
            'action': "push object slightly",
            'expected_prediction': "object may fall",
        },
    ],
    # Advanced Sensor Fusion Tests
    'sensor_fusion_multimodal': [
        {
            'sensors': ["camera", "depth", "audio", "imu"],
            'task': "detect and track human speaker",
            'fusion_type': "audio_visual_spatial",
            'expected': "fuse visual appearance, depth position, audio direction, motion",
        },
        {
            'sensors': ["lidar", "camera", "radar"],
            'task': "autonomous navigation in fog",
            'fusion_type': "weather_adaptive",
            'expected': "weight sensors by reliability in conditions",
        },
    ],
    'sensor_calibration_online': [
        {
            'scenario': "camera-lidar extrinsic drift detected",
            'method': "automatic recalibration using landmarks",
            'expected': "detect misalignment, recalibrate, verify",
        },
    ],
    'sensor_prediction': [
        {
            'scenario': "occluded object tracking",
            'available_sensors': ["last_known_position", "motion_model"],
            'expected': "predict object position during occlusion",
        },
        {
            'scenario': "anticipate sensor readings",
            'task': "predict depth values before reaching location",
            'expected': "world model predicts sensor observations",
        },
    ],
    'embodied_spatial_reasoning': [
        {
            'scenario': "plan arm trajectory avoiding obstacles",
            'sensors': ["joint_encoders", "depth_camera", "force_torque"],
            'expected': "fuse proprioception with exteroception for collision-free motion",
        },
        {
            'scenario': "hand-eye coordination for catching",
            'sensors': ["camera", "arm_position", "ball_trajectory"],
            'expected': "predict interception point, plan motion",
        },
    ],
}

# ========== SENSOR FUSION TEST DATA ==========

SENSOR_FUSION_SCENARIOS = {
    # Scenarios for testing sensor fusion capabilities
    'rgb_depth_fusion': {
        'description': "Combine RGB and depth for 3D scene understanding",
        'inputs': {
            'rgb': {'type': 'image', 'channels': 3, 'resolution': (640, 480)},
            'depth': {'type': 'depth_map', 'resolution': (640, 480), 'range': (0.3, 10.0)},
        },
        'outputs': ['point_cloud', 'object_masks', 'distances'],
        'test_cases': [
            "object_at_1m", "object_at_5m", "multiple_objects", "occluded_object",
        ],
    },
    'force_visual_fusion': {
        'description': "Combine force feedback with visual for manipulation",
        'inputs': {
            'force': {'type': 'force_torque', 'dims': 6},
            'camera': {'type': 'image', 'channels': 3},
            'gripper_state': {'type': 'scalar', 'range': (0, 1)},
        },
        'outputs': ['grasp_quality', 'slip_probability', 'force_adjustment'],
        'test_cases': [
            "rigid_object", "soft_object", "fragile_object", "slipping_object",
        ],
    },
    'imu_visual_odometry': {
        'description': "Fuse IMU and visual odometry for robust localization",
        'inputs': {
            'imu': {'type': 'imu', 'accel': 3, 'gyro': 3},
            'camera': {'type': 'stereo_image'},
            'wheel_encoder': {'type': 'encoder', 'count': 2},
        },
        'outputs': ['pose_6dof', 'velocity', 'uncertainty'],
        'test_cases': [
            "straight_motion", "rotation", "rough_terrain", "visual_degradation",
        ],
    },
    'audio_visual_fusion': {
        'description': "Combine audio and visual for speaker tracking",
        'inputs': {
            'microphone_array': {'type': 'audio', 'channels': 4, 'sample_rate': 16000},
            'camera': {'type': 'image', 'channels': 3},
        },
        'outputs': ['speaker_position', 'speaker_identity', 'attention_target'],
        'test_cases': [
            "single_speaker", "multiple_speakers", "noisy_environment", "occluded_speaker",
        ],
    },
}


class ProgressiveBenchmark:
    """Progressive benchmark runner."""
    
    def __init__(self, model_dir: Path, encoder):
        self.model_dir = Path(model_dir)
        self.encoder = encoder
        self._load_model()
        self._compile_inference()
        
    def _load_model(self):
        with open(self.model_dir / "manifest.json") as f:
            self.manifest = json.load(f)
            
        with open(self.model_dir / "seed_model.pkl", 'rb') as f:
            data = pickle.load(f)
            
        self.params = data['params']['params']
        self.obs_dim = data['metadata']['obs_dim']
        self.action_dim = data['metadata']['action_dim']
        self.output_dim = data['metadata']['output_dim']
        
        config_dict = self.manifest['config']
        self.config = CoreModelConfig(**{k: v for k, v in config_dict.items()})
        
        self.model = CoreModel(
            config=self.config, 
            obs_dim=self.obs_dim, 
            action_dim=self.action_dim, 
            output_dim=self.output_dim
        )
        
    def _compile_inference(self):
        @jax.jit
        def _infer(params, obs, action, reward, s, w, p, cms_memories, cms_keys):
            output, info = self.model.apply(
                {'params': params}, 
                x_obs=obs, a_prev=action, r_t=reward,
                s_prev=s, w_prev=w, p_prev=p, 
                cms_memories=cms_memories, cms_keys=cms_keys
            )
            return output, info
        
        self._infer = _infer
        
        # Warmup
        state = self._init_state()
        obs = jnp.zeros((1, self.obs_dim))
        action = jnp.zeros((1, self.action_dim))
        reward = jnp.zeros((1, 1))
        self._infer(self.params, obs, action, reward, 
                   state['s'], state['w'], state['p'],
                   state['cms_memories'], state['cms_keys'])
        
    def _init_state(self):
        return {
            's': jnp.zeros((1, self.config.d_s)),
            'w': jnp.zeros((1, self.config.d_w)),
            'p': jnp.zeros((1, self.config.d_p)),
            'cms_memories': [jnp.zeros((1, sz, dim)) 
                            for sz, dim in zip(self.config.cms_sizes, self.config.cms_dims)],
            'cms_keys': [jnp.zeros((1, sz, self.config.d_k)) 
                        for sz in self.config.cms_sizes],
        }
        
    def _encode(self, texts: List[str]) -> np.ndarray:
        if isinstance(texts, str):
            texts = [texts]
        emb = self.encoder.encode(texts, convert_to_numpy=True)
        emb = emb / (np.linalg.norm(emb, axis=1, keepdims=True) + 1e-8)
        return emb
    
    def _run_inference(self, embedding: np.ndarray, state: Dict) -> Tuple:
        obs = jnp.array(embedding.reshape(1, -1))
        action = jnp.zeros((1, self.action_dim))
        reward = jnp.zeros((1, 1))
        
        output, info = self._infer(
            self.params, obs, action, reward,
            state['s'], state['w'], state['p'],
            state['cms_memories'], state['cms_keys']
        )
        
        new_state = {
            's': info['fast_state'],
            'w': info['wave_state'],
            'p': info['particle_state'],
            'cms_memories': info['cms_memories'],
            'cms_keys': info['cms_keys'],
        }
        
        return np.array(output[0]), new_state, info

    # ========== LEVEL 1: BASIC ==========
    
    def test_L1_output_stability(self) -> TestResult:
        """L1: Same input should produce consistent output."""
        test_input = "move forward"
        emb = self._encode(test_input)[0]
        
        outputs = []
        for _ in range(5):
            state = self._init_state()
            out, _, _ = self._run_inference(emb, state)
            outputs.append(out)
        
        # Check consistency
        diffs = [np.linalg.norm(outputs[0] - outputs[i]) for i in range(1, 5)]
        max_diff = max(diffs)
        
        return TestResult(
            name="Output Stability",
            level=Level.BASIC,
            score=1.0 if max_diff < 0.01 else 0.0,
            passed=max_diff < 0.01,
            details={'max_diff': float(max_diff)}
        )
    
    def test_L1_output_nonzero(self) -> TestResult:
        """L1: Output should be non-trivial."""
        test_inputs = ["move", "stop", "grab", "release"]
        embeddings = self._encode(test_inputs)
        
        output_norms = []
        for emb in embeddings:
            state = self._init_state()
            out, _, _ = self._run_inference(emb, state)
            output_norms.append(np.linalg.norm(out))
        
        avg_norm = np.mean(output_norms)
        min_norm = np.min(output_norms)
        
        return TestResult(
            name="Non-trivial Output",
            level=Level.BASIC,
            score=min(1.0, avg_norm / 2.0),
            passed=min_norm > 0.1,
            details={'avg_norm': float(avg_norm), 'min_norm': float(min_norm)}
        )
    
    def test_L1_inference_latency(self) -> TestResult:
        """L1: Inference must be fast enough for real-time."""
        state = self._init_state()
        emb = self._encode("test")[0]
        
        times = []
        for _ in range(50):
            t0 = time.time()
            _, state, _ = self._run_inference(emb, state)
            times.append(time.time() - t0)
        
        avg_ms = np.mean(times) * 1000
        p99_ms = np.percentile(times, 99) * 1000
        
        return TestResult(
            name="Inference Latency",
            level=Level.BASIC,
            score=min(1.0, 100 / avg_ms) if avg_ms > 0 else 0,
            passed=p99_ms < 100,  # Must be under 100ms p99
            details={'avg_ms': float(avg_ms), 'p99_ms': float(p99_ms)}
        )

    # ========== LEVEL 2: INTERMEDIATE ==========
    
    def test_L2_command_differentiation(self) -> TestResult:
        """L2: Different commands should produce different outputs."""
        commands = [
            "pick up the red cube",
            "navigate to the kitchen",
            "turn on the lights",
            "say hello to the user",
        ]
        embeddings = self._encode(commands)
        
        outputs = []
        for emb in embeddings:
            state = self._init_state()
            out, _, _ = self._run_inference(emb, state)
            outputs.append(out)
        
        # Pairwise differences
        diffs = []
        for i in range(len(outputs)):
            for j in range(i+1, len(outputs)):
                diffs.append(np.linalg.norm(outputs[i] - outputs[j]))
        
        avg_diff = np.mean(diffs)
        min_diff = np.min(diffs)
        
        return TestResult(
            name="Command Differentiation",
            level=Level.INTERMEDIATE,
            score=min(1.0, avg_diff / 0.5),
            passed=min_diff > 0.1,
            details={'avg_diff': float(avg_diff), 'min_diff': float(min_diff)}
        )
    
    def test_L2_state_evolution(self) -> TestResult:
        """L2: State should evolve over a sequence."""
        sequence = ["start task", "pick object", "move to target", "place object", "complete"]
        embeddings = self._encode(sequence)
        
        state = self._init_state()
        state_norms = [float(jnp.linalg.norm(state['s']))]
        
        for emb in embeddings:
            _, state, _ = self._run_inference(emb, state)
            state_norms.append(float(jnp.linalg.norm(state['s'])))
        
        # State should change
        changes = [abs(state_norms[i+1] - state_norms[i]) for i in range(len(state_norms)-1)]
        avg_change = np.mean(changes)
        
        return TestResult(
            name="State Evolution",
            level=Level.INTERMEDIATE,
            score=min(1.0, avg_change / 5.0),
            passed=avg_change > 1.0,
            details={'state_trajectory': state_norms, 'avg_change': float(avg_change)}
        )
    
    def test_L2_spatial_understanding(self) -> TestResult:
        """L2: Spatial commands should produce related outputs."""
        spatial_pairs = [
            ("move left", "move right"),
            ("move up", "move down"),
            ("move forward", "move backward"),
        ]
        
        # Opposite directions should be different but related
        symmetry_scores = []
        for left, right in spatial_pairs:
            emb_left, emb_right = self._encode([left, right])
            
            state = self._init_state()
            out_left, _, _ = self._run_inference(emb_left, state)
            
            state = self._init_state()
            out_right, _, _ = self._run_inference(emb_right, state)
            
            # Should be different (not identical)
            diff = np.linalg.norm(out_left - out_right)
            # But correlated (not random)
            corr = np.corrcoef(out_left, out_right)[0, 1]
            
            symmetry_scores.append(diff > 0.05 and not np.isnan(corr))
        
        passed_count = sum(symmetry_scores)
        
        return TestResult(
            name="Spatial Understanding",
            level=Level.INTERMEDIATE,
            score=passed_count / len(spatial_pairs),
            passed=passed_count >= 2,
            details={'pairs_tested': len(spatial_pairs), 'pairs_passed': passed_count}
        )

    # ========== LEVEL 3: ADVANCED ==========
    
    def test_L3_memory_persistence(self) -> TestResult:
        """L3: Information should persist in CMS memory."""
        # Store information
        setup = ["remember location alpha is north building", 
                 "remember priority task is inventory",
                 "remember user name is Alice"]
        
        state = self._init_state()
        initial_cms = sum(float(jnp.mean(jnp.abs(m))) for m in state['cms_memories'])
        
        for text in setup:
            emb = self._encode(text)[0]
            _, state, _ = self._run_inference(emb, state)
        
        after_setup_cms = sum(float(jnp.mean(jnp.abs(m))) for m in state['cms_memories'])
        
        # Query (should use memory)
        query = "what is the location"
        emb = self._encode(query)[0]
        _, state, _ = self._run_inference(emb, state)
        
        final_cms = sum(float(jnp.mean(jnp.abs(m))) for m in state['cms_memories'])
        
        growth = final_cms - initial_cms
        
        return TestResult(
            name="Memory Persistence",
            level=Level.ADVANCED,
            score=min(1.0, growth / 2.0),
            passed=growth > 0.5,
            details={
                'initial_cms': float(initial_cms),
                'after_setup_cms': float(after_setup_cms),
                'final_cms': float(final_cms),
                'growth': float(growth)
            }
        )
    
    def test_L3_context_switching(self) -> TestResult:
        """L3: Different contexts should reset appropriately."""
        # Kitchen context
        kitchen_cmds = ["I am in the kitchen", "prepare coffee", "check refrigerator"]
        
        state = self._init_state()
        for cmd in kitchen_cmds:
            emb = self._encode(cmd)[0]
            _, state, _ = self._run_inference(emb, state)
        
        kitchen_state_norm = float(jnp.linalg.norm(state['s']))
        
        # Switch to completely different context
        office_cmds = ["I am now in the office", "print document", "schedule meeting"]
        
        for cmd in office_cmds:
            emb = self._encode(cmd)[0]
            _, state, _ = self._run_inference(emb, state)
        
        office_state_norm = float(jnp.linalg.norm(state['s']))
        
        state_change = abs(office_state_norm - kitchen_state_norm)
        
        return TestResult(
            name="Context Switching",
            level=Level.ADVANCED,
            score=min(1.0, state_change / 10.0),
            passed=state_change > 2.0,
            details={
                'kitchen_state': kitchen_state_norm,
                'office_state': office_state_norm,
                'change': state_change
            }
        )
    
    def test_L3_hierarchical_commands(self) -> TestResult:
        """L3: Handle high-level vs low-level commands."""
        high_level = "clean the room"
        low_level = ["pick up trash", "vacuum floor", "organize items", "wipe surfaces"]
        
        emb_high = self._encode(high_level)[0]
        embs_low = self._encode(low_level)
        
        state = self._init_state()
        out_high, _, _ = self._run_inference(emb_high, state)
        
        outs_low = []
        for emb in embs_low:
            state = self._init_state()
            out, _, _ = self._run_inference(emb, state)
            outs_low.append(out)
        
        # High-level should be different from any single low-level
        diffs_to_low = [np.linalg.norm(out_high - out_low) for out_low in outs_low]
        min_diff = np.min(diffs_to_low)
        
        return TestResult(
            name="Hierarchical Commands",
            level=Level.ADVANCED,
            score=min(1.0, min_diff / 0.3),
            passed=min_diff > 0.05,
            details={'min_diff_to_subtasks': float(min_diff)}
        )

    # ========== LEVEL 4: EXPERT ==========
    
    def test_L4_safety_priority(self) -> TestResult:
        """L4: Safety commands must produce distinct, high-priority outputs."""
        safety_cmds = ["emergency stop", "halt immediately", "danger detected"]
        normal_cmds = ["pick up cube", "move forward", "turn left"]
        
        safety_outputs = []
        normal_outputs = []
        
        for cmd in safety_cmds:
            state = self._init_state()
            out, _, _ = self._run_inference(self._encode(cmd)[0], state)
            safety_outputs.append(out)
        
        for cmd in normal_cmds:
            state = self._init_state()
            out, _, _ = self._run_inference(self._encode(cmd)[0], state)
            normal_outputs.append(out)
        
        # Safety should be distinguishable
        safety_mean = np.mean(safety_outputs, axis=0)
        normal_mean = np.mean(normal_outputs, axis=0)
        separation = np.linalg.norm(safety_mean - normal_mean)
        
        # Safety outputs should be consistent with each other
        safety_std = np.mean([np.linalg.norm(s - safety_mean) for s in safety_outputs])
        
        return TestResult(
            name="Safety Priority",
            level=Level.EXPERT,
            score=min(1.0, separation / 0.5) * min(1.0, 1.0 - safety_std / 0.5),
            passed=separation > 0.1,
            details={
                'separation': float(separation),
                'safety_consistency': float(1.0 - safety_std)
            }
        )
    
    def test_L4_error_recovery(self) -> TestResult:
        """L4: Error states should trigger recovery patterns."""
        normal_sequence = ["pick up object", "move to target", "place object"]
        error_sequence = ["pick up object", "ERROR: grasp failed", "retry grasp", "move to target"]
        
        # Run normal
        state_normal = self._init_state()
        for cmd in normal_sequence:
            emb = self._encode(cmd)[0]
            _, state_normal, _ = self._run_inference(emb, state_normal)
        
        # Run with error
        state_error = self._init_state()
        for cmd in error_sequence:
            emb = self._encode(cmd)[0]
            _, state_error, _ = self._run_inference(emb, state_error)
        
        # States should be different (error handling activated)
        state_diff = float(jnp.linalg.norm(state_normal['s'] - state_error['s']))
        
        return TestResult(
            name="Error Recovery",
            level=Level.EXPERT,
            score=min(1.0, state_diff / 5.0),
            passed=state_diff > 1.0,
            details={'state_difference': state_diff}
        )
    
    def test_L4_multi_step_planning(self) -> TestResult:
        """L4: Complex tasks should show planning behavior."""
        complex_task = "organize all items on desk by color then by size"
        
        # Run multiple steps to see if state evolves in a planning pattern
        state = self._init_state()
        emb = self._encode(complex_task)[0]
        
        # Simulate multiple "thinking" steps
        outputs = []
        for _ in range(5):
            out, state, _ = self._run_inference(emb, state)
            outputs.append(out)
        
        # Outputs should evolve (not stay constant) = planning/reasoning
        changes = [np.linalg.norm(outputs[i+1] - outputs[i]) for i in range(len(outputs)-1)]
        avg_change = np.mean(changes)
        
        return TestResult(
            name="Multi-step Planning",
            level=Level.EXPERT,
            score=min(1.0, avg_change / 0.2),
            passed=avg_change > 0.05,
            details={'output_changes': [float(c) for c in changes]}
        )
    
    def test_L4_sensor_fusion_basic(self) -> TestResult:
        """L4: Handle multi-sensor input scenarios."""
        # Test commands that imply different sensor modalities
        single_sensor_cmds = [
            "look at the object",  # Vision only
            "listen for the alarm",  # Audio only
            "feel the surface texture",  # Tactile only
        ]
        
        multi_sensor_cmds = [
            "find the beeping red box",  # Vision + Audio
            "grab the soft fuzzy object",  # Vision + Tactile
            "navigate to the sound of running water",  # Audio + Depth/Lidar
        ]
        
        single_outputs = []
        multi_outputs = []
        
        for cmd in single_sensor_cmds:
            state = self._init_state()
            out, _, _ = self._run_inference(self._encode(cmd)[0], state)
            single_outputs.append(out)
        
        for cmd in multi_sensor_cmds:
            state = self._init_state()
            out, _, _ = self._run_inference(self._encode(cmd)[0], state)
            multi_outputs.append(out)
        
        # Multi-sensor commands should produce richer (higher variance) outputs
        single_var = np.mean([np.var(o) for o in single_outputs])
        multi_var = np.mean([np.var(o) for o in multi_outputs])
        
        # Outputs should be distinguishable between sensor modalities
        within_single = np.mean([np.linalg.norm(single_outputs[i] - single_outputs[j]) 
                                  for i in range(len(single_outputs)) 
                                  for j in range(i+1, len(single_outputs))])
        
        return TestResult(
            name="Sensor Fusion Basic",
            level=Level.EXPERT,
            score=min(1.0, within_single / 0.3),
            passed=within_single > 0.1,
            details={
                'single_sensor_variance': float(single_var),
                'multi_sensor_variance': float(multi_var),
                'modality_separation': float(within_single),
            }
        )
    
    def test_L4_sensor_degradation(self) -> TestResult:
        """L4: Handle graceful sensor degradation."""
        # Normal operation vs degraded conditions
        normal_cmd = "navigate to the target using camera and lidar"
        degraded_cmd = "navigate to the target with camera occluded use lidar only"
        fallback_cmd = "navigate to the target using dead reckoning sensors failed"
        
        state_normal = self._init_state()
        out_normal, state_normal, _ = self._run_inference(
            self._encode(normal_cmd)[0], state_normal
        )
        
        state_degraded = self._init_state()
        out_degraded, state_degraded, _ = self._run_inference(
            self._encode(degraded_cmd)[0], state_degraded
        )
        
        state_fallback = self._init_state()
        out_fallback, state_fallback, _ = self._run_inference(
            self._encode(fallback_cmd)[0], state_fallback
        )
        
        # Degraded and fallback should be different from normal
        diff_degraded = np.linalg.norm(out_normal - out_degraded)
        diff_fallback = np.linalg.norm(out_normal - out_fallback)
        
        # Progressive degradation should show progressive difference
        progressive = diff_fallback > diff_degraded
        
        return TestResult(
            name="Sensor Degradation",
            level=Level.EXPERT,
            score=min(1.0, diff_degraded / 0.2) * (1.0 if progressive else 0.5),
            passed=diff_degraded > 0.05,
            details={
                'normal_vs_degraded': float(diff_degraded),
                'normal_vs_fallback': float(diff_fallback),
                'progressive_degradation': progressive,
            }
        )

    # ========== LEVEL 5: AUTONOMOUS ==========
    
    def test_L5_self_monitoring(self) -> TestResult:
        """L5: Model should show different behavior when uncertain."""
        clear_cmd = "pick up the red cube on the table"
        ambiguous_cmd = "pick up that thing over there somewhere"
        
        state_clear = self._init_state()
        out_clear, state_clear, _ = self._run_inference(self._encode(clear_cmd)[0], state_clear)
        
        state_ambig = self._init_state()
        out_ambig, state_ambig, _ = self._run_inference(self._encode(ambiguous_cmd)[0], state_ambig)
        
        # Should produce different outputs (ambiguity should affect behavior)
        diff = np.linalg.norm(out_clear - out_ambig)
        
        # Ambiguous should have higher state variance (less certain)
        clear_state_norm = float(jnp.linalg.norm(state_clear['s']))
        ambig_state_norm = float(jnp.linalg.norm(state_ambig['s']))
        
        return TestResult(
            name="Self Monitoring",
            level=Level.AUTONOMOUS,
            score=min(1.0, diff / 0.3),
            passed=diff > 0.05,
            details={
                'output_diff': float(diff),
                'clear_state': clear_state_norm,
                'ambig_state': ambig_state_norm
            }
        )
    
    def test_L5_continuous_learning(self) -> TestResult:
        """L5: CMS should accumulate information over long sequences."""
        long_sequence = [
            "starting new task session",
            "objective is to sort warehouse inventory",
            "first section is electronics",
            "found 50 phones in bin A",
            "found 30 laptops in bin B",
            "moving to clothing section",
            "found 100 shirts in bin C",
            "completing inventory count",
            "total items catalogued",
            "session complete"
        ]
        
        state = self._init_state()
        cms_trajectory = [sum(float(jnp.mean(jnp.abs(m))) for m in state['cms_memories'])]
        
        for text in long_sequence:
            emb = self._encode(text)[0]
            _, state, _ = self._run_inference(emb, state)
            cms_trajectory.append(sum(float(jnp.mean(jnp.abs(m))) for m in state['cms_memories']))
        
        # Memory should grow throughout
        total_growth = cms_trajectory[-1] - cms_trajectory[0]
        monotonic_growth = all(cms_trajectory[i+1] >= cms_trajectory[i] * 0.95 for i in range(len(cms_trajectory)-1))
        
        return TestResult(
            name="Continuous Learning",
            level=Level.AUTONOMOUS,
            score=min(1.0, total_growth / 3.0),
            passed=total_growth > 1.0 and monotonic_growth,
            details={
                'cms_trajectory': [round(x, 3) for x in cms_trajectory],
                'total_growth': float(total_growth),
                'monotonic': monotonic_growth
            }
        )
    
    def test_L5_world_model(self) -> TestResult:
        """L5: Model should show predictive behavior."""
        # Setup: establish a pattern
        pattern = ["step 1: grasp", "step 2: lift", "step 3: move", "step 4: place"]
        
        state = self._init_state()
        pattern_outputs = []
        for cmd in pattern:
            emb = self._encode(cmd)[0]
            out, state, _ = self._run_inference(emb, state)
            pattern_outputs.append(out)
        
        # Now test with partial pattern - should predict next
        state2 = self._init_state()
        for cmd in pattern[:2]:  # Only first 2 steps
            emb = self._encode(cmd)[0]
            _, state2, _ = self._run_inference(emb, state2)
        
        # Query what comes next
        query = "what is next step"
        emb = self._encode(query)[0]
        predicted_out, _, _ = self._run_inference(emb, state2)
        
        # Should be similar to step 3 output from full pattern
        similarity = np.dot(predicted_out, pattern_outputs[2]) / (
            np.linalg.norm(predicted_out) * np.linalg.norm(pattern_outputs[2]) + 1e-8
        )
        
        return TestResult(
            name="World Model Prediction",
            level=Level.AUTONOMOUS,
            score=max(0, float(similarity)),
            passed=similarity > 0.3,
            details={'prediction_similarity': float(similarity)}
        )
    
    def test_L5_sensor_fusion_multimodal(self) -> TestResult:
        """L5: Handle complex multimodal sensor fusion for autonomous operation."""
        # Complex scenarios requiring multiple sensor modalities
        scenarios = [
            "track person while they speak to identify and follow",  # Audio + Visual + Motion
            "navigate through fog using lidar radar and wheel odometry",  # Multi-sensor fallback
            "pick up fragile glass using camera depth and force feedback",  # Vision + Depth + Force
        ]
        
        # Simple scenario for comparison
        simple = "move forward"
        
        complex_outputs = []
        for scenario in scenarios:
            state = self._init_state()
            emb = self._encode(scenario)[0]
            out, _, _ = self._run_inference(emb, state)
            complex_outputs.append(out)
        
        state_simple = self._init_state()
        out_simple, _, _ = self._run_inference(self._encode(simple)[0], state_simple)
        
        # Complex scenarios should produce more varied outputs
        complex_var = np.var([np.linalg.norm(o) for o in complex_outputs])
        
        # Complex should differ from simple
        diffs = [np.linalg.norm(o - out_simple) for o in complex_outputs]
        avg_diff = np.mean(diffs)
        
        return TestResult(
            name="Sensor Fusion Multimodal",
            level=Level.AUTONOMOUS,
            score=min(1.0, avg_diff / 0.3),
            passed=avg_diff > 0.05,
            details={
                'complex_variance': float(complex_var),
                'avg_diff_from_simple': float(avg_diff),
            }
        )
    
    def test_L5_embodied_spatial_reasoning(self) -> TestResult:
        """L5: Combine proprioception with exteroception for spatial reasoning."""
        # Scenarios requiring body-space awareness
        proprioceptive = [
            "am I close enough to reach the object",  # Self + Object spatial
            "can I fit through that doorway",  # Self-size awareness
            "rotate arm to avoid collision with shelf",  # Body geometry + environment
        ]
        
        exteroceptive = [
            "where is the red cube",  # Pure external
            "what objects are on the table",  # Pure external
        ]
        
        prop_outputs = []
        ext_outputs = []
        
        for cmd in proprioceptive:
            state = self._init_state()
            out, _, _ = self._run_inference(self._encode(cmd)[0], state)
            prop_outputs.append(out)
        
        for cmd in exteroceptive:
            state = self._init_state()
            out, _, _ = self._run_inference(self._encode(cmd)[0], state)
            ext_outputs.append(out)
        
        # Proprioceptive and exteroceptive should be distinguishable
        prop_mean = np.mean(prop_outputs, axis=0)
        ext_mean = np.mean(ext_outputs, axis=0)
        separation = np.linalg.norm(prop_mean - ext_mean)
        
        return TestResult(
            name="Embodied Spatial Reasoning",
            level=Level.AUTONOMOUS,
            score=min(1.0, separation / 0.2),
            passed=separation > 0.05,
            details={'proprioceptive_vs_exteroceptive': float(separation)}
        )

    def run_all(self) -> ProgressiveBenchmarkResult:
        """Run all progressive tests."""
        from datetime import datetime
        
        result = ProgressiveBenchmarkResult(
            version=self.manifest['version'],
            timestamp=datetime.now().isoformat(),
            model_params=self.manifest['model']['param_count'],
        )
        
        all_tests = [
            # Level 1: Basic
            self.test_L1_output_stability,
            self.test_L1_output_nonzero,
            self.test_L1_inference_latency,
            # Level 2: Intermediate
            self.test_L2_command_differentiation,
            self.test_L2_state_evolution,
            self.test_L2_spatial_understanding,
            # Level 3: Advanced
            self.test_L3_memory_persistence,
            self.test_L3_context_switching,
            self.test_L3_hierarchical_commands,
            # Level 4: Expert (includes sensor fusion)
            self.test_L4_safety_priority,
            self.test_L4_error_recovery,
            self.test_L4_multi_step_planning,
            self.test_L4_sensor_fusion_basic,
            self.test_L4_sensor_degradation,
            # Level 5: Autonomous (includes advanced sensor fusion)
            self.test_L5_self_monitoring,
            self.test_L5_continuous_learning,
            self.test_L5_world_model,
            self.test_L5_sensor_fusion_multimodal,
            self.test_L5_embodied_spatial_reasoning,
        ]
        
        for test_fn in all_tests:
            try:
                test_result = test_fn()
                result.results.append(test_result)
            except Exception as e:
                level = Level.BASIC
                if 'L2' in test_fn.__name__:
                    level = Level.INTERMEDIATE
                elif 'L3' in test_fn.__name__:
                    level = Level.ADVANCED
                elif 'L4' in test_fn.__name__:
                    level = Level.EXPERT
                elif 'L5' in test_fn.__name__:
                    level = Level.AUTONOMOUS
                    
                result.results.append(TestResult(
                    name=test_fn.__name__.replace('test_', '').replace('_', ' '),
                    level=level,
                    score=0.0,
                    passed=False,
                    details={'error': str(e)}
                ))
        
        return result


def run_progressive_benchmark(model_dir: str = "/opt/continuonos/brain/model/seed_stable",
                               output_file: str = None):
    """Run complete progressive benchmark."""
    import os
    os.environ['HF_HOME'] = '/opt/continuonos/brain/hf_cache'
    
    from sentence_transformers import SentenceTransformer
    
    print("=" * 70)
    print("PROGRESSIVE SEED MODEL BENCHMARK")
    print("=" * 70)
    
    print("\nüìö Loading encoder...")
    encoder = SentenceTransformer(
        'google/embeddinggemma-300m', 
        trust_remote_code=True,
        token=os.environ.get('HUGGINGFACE_TOKEN')
    )
    
    print("\nüîß Loading model...")
    benchmark = ProgressiveBenchmark(Path(model_dir), encoder)
    print(f"   Model: v{benchmark.manifest['version']}")
    print(f"   Params: {benchmark.manifest['model']['param_count']:,}")
    
    print("\nüß™ Running progressive tests...\n")
    result = benchmark.run_all()
    
    # Print results by level
    for level in Level:
        summary = result.get_level_summary(level)
        level_results = [r for r in result.results if r.level == level]
        
        status = "‚úÖ" if summary.passed else "‚ùå"
        print(f"\n{'='*60}")
        print(f"{status} LEVEL {level.value}: {level.name}")
        print(f"{'='*60}")
        print(f"   Passed: {summary.tests_passed}/{summary.tests_total}")
        print(f"   Score: {summary.avg_score:.2f}")
        
        for r in level_results:
            status = "‚úÖ" if r.passed else "‚ùå"
            print(f"\n   {status} {r.name}: {r.score:.2f}")
            for k, v in list(r.details.items())[:3]:
                if isinstance(v, float):
                    print(f"      {k}: {v:.4f}")
                elif isinstance(v, list) and len(v) <= 6:
                    print(f"      {k}: {v}")
                else:
                    print(f"      {k}: {v}")
    
    print("\n" + "=" * 70)
    print(f"HIGHEST LEVEL PASSED: {result.highest_level_passed.name}")
    print(f"OVERALL SCORE: {result.overall_score:.2f}")
    print("=" * 70)
    
    if output_file:
        out_data = {
            'version': result.version,
            'timestamp': result.timestamp,
            'model_params': result.model_params,
            'highest_level': result.highest_level_passed.name,
            'overall_score': result.overall_score,
            'levels': {
                level.name: {
                    'passed': result.get_level_summary(level).passed,
                    'score': result.get_level_summary(level).avg_score,
                    'tests': result.get_level_summary(level).tests_passed,
                    'total': result.get_level_summary(level).tests_total,
                }
                for level in Level
            },
            'results': [
                {'name': r.name, 'level': r.level.name, 'score': float(r.score), 
                 'passed': bool(r.passed), 'details': {k: float(v) if isinstance(v, (np.floating, float)) else v 
                                                        for k, v in r.details.items()}}
                for r in result.results
            ]
        }
        with open(output_file, 'w') as f:
            json.dump(out_data, f, indent=2, default=str)
        print(f"\nüìÅ Results saved to: {output_file}")
    
    return result


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-dir', default='/opt/continuonos/brain/model/seed_stable')
    parser.add_argument('--output', default=None)
    args = parser.parse_args()
    
    run_progressive_benchmark(args.model_dir, args.output)

