# Continuon Brain runtime systemd environment overrides (example)
#
# Copy to: /etc/continuonbrain/continuonbrain.env
#
# Minimal knobs:
# - CONTINUON_REPO: path to this repository on the device
# - CONFIG_DIR: runtime state/config directory (RLDS, model dirs, safety protocol, etc.)
# - CONTINUON_PYTHON: python interpreter to run (often a venv python inside the repo)
#
# Notes:
# - Keep secrets out of this file. Use systemd credentials/keystore for tokens.
# - This file is sourced by systemd EnvironmentFile= and uses KEY=VALUE lines.

CONTINUON_REPO=/opt/continuonos/ContinuonXR
CONFIG_DIR=/opt/continuonos/brain
PYTHONPATH=/opt/continuonos/ContinuonXR
CONTINUON_PYTHON=/opt/continuonos/ContinuonXR/.venv/bin/python3

# Optional behavior flags
CONTINUON_HEADLESS=1
CONTINUON_ENABLE_BACKGROUND_TRAINER=0

# Hardware boot behavior (recommended defaults for reliability)
# - Leave FORCE_* unset to allow auto-detect with mock fallback.
# - If your arm/drivetrain hardware is flaky during boot, skip motion init so the UI still comes up.
CONTINUON_SKIP_MOTION_HW=1
# Default is real-hardware on boot; keep this explicit for clarity.
CONTINUON_FORCE_REAL_HARDWARE=1
# CONTINUON_FORCE_MOCK_HARDWARE=1

# Optional: "curiosity boot" (offline Wikipedia learning)
# Provide a local Wikipedia JSONL corpus (each line: {"title": "...", "text": "..."})
# and enable the curiosity sidecar at boot. This logs RLDS episodes `wiki_learn_<ts>.json`.
CONTINUON_WIKI_JSONL=/opt/continuonos/brain/wikipedia/wikipedia.jsonl
CONTINUON_ENABLE_WIKI_CURIOSITY=0
# Bounded defaults (safe for Pi-class boards)
CONTINUON_WIKI_BOOT_EPISODES=1
CONTINUON_WIKI_TOPICS_PER_EPISODE=2
CONTINUON_WIKI_MAX_SCAN=800
CONTINUON_WIKI_TOP_K=2
CONTINUON_WIKI_MAX_CHARS=800
CONTINUON_WIKI_BOOT_SLEEP_S=1.0

# If your corpus download is Parquet (common for Hugging Face datasets), convert once:
#   python -m continuonbrain.tools.convert_wikipedia_parquet_to_jsonl \
#     --input /path/to/parquet_dir --output "$CONTINUON_WIKI_JSONL" --max-rows 20000

# ============================================================================
# CLI Provider API Keys (for Ralph Layer / Meta Ralph Agent)
# ============================================================================
# These keys enable the swappable CLI integration in the Meta Ralph Agent.
# Use systemd credentials or a keystore for production deployments.

# Claude Code CLI (Opus 4.5) - Primary provider
# Get your key from: https://console.anthropic.com/
# ANTHROPIC_API_KEY=sk-ant-...

# Gemini CLI - Alternative provider
# Get your key from: https://makersuite.google.com/app/apikey
# GEMINI_API_KEY=...

# OpenAI API - Alternative provider
# Get your key from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-...

# Ollama - Local model provider (no API key needed, just endpoint)
# OLLAMA_ENDPOINT=http://localhost:11434

# Default CLI provider: claude, gemini, ollama, openai
RALPH_DEFAULT_CLI_PROVIDER=claude

# Ralph Layer Configuration
RALPH_CONTEXT_WINDOW_TOKENS=32768
RALPH_MAX_ITERATIONS=100
RALPH_ENABLE_GUARDRAILS=1

