"""
BrainService: Corresponds to the "Brain" logic.
Manages tasks, hardware access, and robot state.
"""
import os
import sys
import subprocess
import time
import datetime
from pathlib import Path
from typing import Dict, List, Optional, AsyncIterator, Any
import asyncio
import json
import logging
from dataclasses import dataclass, field
from enum import Enum
import threading
from collections import deque
import queue

from continuonbrain.actuators.pca9685_arm import PCA9685ArmController
from continuonbrain.actuators.drivetrain_controller import DrivetrainController
from continuonbrain.sensors.oak_depth import OAKDepthCapture
from continuonbrain.recording.arm_episode_recorder import ArmEpisodeRecorder
from continuonbrain.sensors.hardware_detector import HardwareDetector
from continuonbrain.robot_modes import RobotModeManager, RobotMode
from continuonbrain.services.desktop_service import DesktopService
from continuonbrain.gemma_chat import build_chat_service, create_gemma_chat
from continuonbrain.system_context import SystemContext
from continuonbrain.system_health import SystemHealthChecker
from continuonbrain.system_instructions import SystemInstructions
from continuonbrain.resource_monitor import ResourceMonitor, ResourceLevel
from continuonbrain.resource_monitor import ResourceMonitor, ResourceLevel
from continuonbrain.services.audio_io import record_wav, speak_text
from continuonbrain.services.pairing_manager import PairingManager
from continuonbrain.services.video_stream import VideoStreamHelper
from continuonbrain.services.training_runner import TrainingRunner
from continuonbrain.kernel.safety_kernel import SafetyKernelClient
from continuonbrain.studio_server import StateAggregator
from continuonbrain.tools import create_default_registry
from continuonbrain.services.curriculum_manager import CurriculumManager # Added
from continuonbrain.services.vision_core import create_vision_core
from continuonbrain.services.cloud_relay import CloudRelay
from continuonbrain.core.context_graph_store import SQLiteContextStore
from continuonbrain.core.graph_ingestor import GraphIngestor
from continuonbrain.core.context_retriever import ContextRetriever
from continuonbrain.core.session_store import SQLiteSessionStore
from continuonbrain.core.context_graph_models import Node
from continuonbrain.core.decision_trace_logger import DecisionTraceLogger

logger = logging.getLogger(__name__)

try:
    from continuonbrain.services.background_learner import BackgroundLearner
except ImportError:
    logger.warning("Could not import BackgroundLearner (likely missing dependencies like torch). Background learning will be disabled.")
    BackgroundLearner = None

# --- Task Dataclasses ---

@dataclass
class TaskDefinition:
    """Static task library entry used to seed Studio panels."""
    id: str
    title: str
    description: str
    group: str
    tags: List[str] = field(default_factory=list)
    requires_motion: bool = False
    requires_recording: bool = False
    required_modalities: List[str] = field(default_factory=list)
    steps: List[str] = field(default_factory=list)
    estimated_duration: str = ""
    recommended_mode: str = "autonomous"
    telemetry_topic: str = "loop/tasks"


@dataclass
class TaskEligibilityMarker:
    code: str
    label: str
    severity: str = "info"
    blocking: bool = False
    source: str = "runtime"
    remediation: str = ""

    def to_dict(self) -> Dict[str, object]:
        return {
            "code": self.code,
            "label": self.label,
            "severity": self.severity,
            "blocking": self.blocking,
            "source": self.source,
            "remediation": self.remediation,
        }


@dataclass
class TaskEligibility:
    eligible: bool
    markers: List[TaskEligibilityMarker] = field(default_factory=list)
    next_poll_after_ms: float = 250.0

    def to_dict(self) -> Dict[str, object]:
        return {
            "eligible": self.eligible,
            "markers": [marker.to_dict() for marker in self.markers],
            "next_poll_after_ms": self.next_poll_after_ms,
        }


@dataclass
class TaskLibraryEntry:
    id: str
    title: str
    description: str
    group: str
    tags: List[str]
    eligibility: TaskEligibility
    estimated_duration: str = ""
    recommended_mode: str = "autonomous"

    def to_dict(self) -> Dict[str, object]:
        return {
            "id": self.id,
            "title": self.title,
            "description": self.description,
            "group": self.group,
            "tags": self.tags,
            "eligibility": self.eligibility.to_dict(),
            "estimated_duration": self.estimated_duration,
            "recommended_mode": self.recommended_mode,
        }



@dataclass
class PersonalityConfig:
    """Configuration for the robot's personality traits."""
    humor_level: float = 0.5  # 0.0 to 1.0
    sarcasm_level: float = 0.5  # 0.0 to 1.0
    empathy_level: float = 0.5  # 0.0 to 1.0
    verbosity_level: float = 0.5 # 0.0 (Concise) to 1.0 (Verbose)
    system_name: str = "robot"   # Custom system name
    identity_mode: str = "Adaptive" # Adaptive, Professional, Friendly
    
@dataclass
class UserContext:
    """Context about the current user interactions."""
    user_id: str = "craig_michael_merry"
    role: str = "owner"  # "owner", "guest"
    detected_presence: bool = False


@dataclass
class TaskSummary:
    entry: TaskLibraryEntry
    required_modalities: List[str]
    steps: List[str]
    owner: str
    updated_at: str
    telemetry_topic: str

    def to_dict(self) -> Dict[str, object]:
        return {
            "entry": self.entry.to_dict(),
            "required_modalities": self.required_modalities,
            "steps": self.steps,
            "owner": self.owner,
            "updated_at": self.updated_at,
            "telemetry_topic": self.telemetry_topic,
        }


class TaskLibrary:
    """In-memory Task Library with lightweight eligibility markers."""

    def __init__(self) -> None:
        self._entries: Dict[str, TaskDefinition] = {
            "workspace-inspection": TaskDefinition(
                id="workspace-inspection",
                title="Inspect Workspace",
                description="Sweep sensors for loose cables or obstacles before enabling autonomy.",
                group="Safety",
                tags=["safety", "vision", "preflight"],
                requires_motion=False,
                requires_recording=False,
                required_modalities=["vision"],
                steps=[
                    "Spin the camera and depth sensors across the workspace",
                    "Flag blocked envelopes for operator acknowledgement",
                    "Cache a short clip for offline review",
                ],
                estimated_duration="45s",
                recommended_mode="autonomous",
                telemetry_topic="telemetry/preflight",
            ),
            "pick-and-place": TaskDefinition(
                id="pick-and-place",
                title="Pick & Place Demo",
                description="Run the default manipulation loop for demos and regressions.",
                group="Autonomy",
                tags=["manipulation", "demo", "recordable"],
                requires_motion=True,
                requires_recording=True,
                required_modalities=["vision", "gripper"],
                steps=[
                    "Plan grasp pose",
                    "Lift and place to bin",
                    "Report safety margin and latency",
                ],
                estimated_duration="2m",
                recommended_mode="autonomous",
                telemetry_topic="telemetry/manipulation",
            ),
            "calibration-check": TaskDefinition(
                id="calibration-check",
                title="Calibration Check",
                description="Verify encoders and camera alignment before overnight runs.",
                group="Maintenance",
                tags=["maintenance", "calibration"],
                requires_motion=False,
                requires_recording=False,
                required_modalities=["vision", "arm"],
                steps=[
                    "Move to calibration pose",
                    "Capture depth + RGB alignment snapshot",
                    "Emit eligibility marker if drift detected",
                ],
                estimated_duration="1m",
                recommended_mode="manual_training",
                telemetry_topic="telemetry/calibration",
            ),
        }

    def list_entries(self) -> List[TaskDefinition]:
        return list(self._entries.values())

    def get_entry(self, task_id: str) -> Optional[TaskDefinition]:
        return self._entries.get(task_id)


class BrainService:
    def log_system_event(self, message: str):
        """Push a system log message to the UI stream."""
        payload = {
            "type": "log_message",
            "message": message,
            "timestamp": datetime.datetime.now().isoformat()
        }
        try:
            self.chat_event_queue.put(payload, block=False)
        except queue.Full:
            pass

    def __init__(
        self, 
        config_dir: str = "/tmp/continuonbrain_demo", 
        prefer_real_hardware: bool = True, 
        auto_detect: bool = True, 
        allow_mock_fallback: bool = False, 
        system_instructions: Optional[SystemInstructions] = None
    ):
        print("DEBUG: BrainService.__init__ STARTING")
        self.config_dir = config_dir
        self.prefer_real_hardware = prefer_real_hardware
        self.auto_detect = auto_detect
        self.allow_mock_fallback = allow_mock_fallback
        self.system_instructions = system_instructions
        
        # Real-time chat events for UI (SSE) - Init early for logging
        self.chat_event_queue = queue.Queue(maxsize=100)

        self.desktop = DesktopService(storage_dir=config_dir)
        
        self.use_real_hardware = False
        self.arm: Optional[PCA9685ArmController] = None
        self.camera: Optional[OAKDepthCapture] = None
        self.recorder: Optional[ArmEpisodeRecorder] = None
        self.drivetrain: Optional[DrivetrainController] = None
        self.mode_manager: Optional[RobotModeManager] = None
        self.health_checker = SystemHealthChecker(config_dir=config_dir)
        self.task_library = TaskLibrary()
        self.selected_task_id: Optional[str] = None
        self.detected_config: dict = {}
        
        # Resource monitoring
        self.resource_monitor = ResourceMonitor(config_dir=Path(config_dir))
        logger.info(f"üìä Resource Monitor initialized: {self.resource_monitor.get_status_summary()}")
        
        # HOPE brain (optional)
        self.hope_brain = None
        self.background_learner = None
        self.manual_trainer = None
        
        # Experience logger for active learning
        from continuonbrain.services.experience_logger import ExperienceLogger
        self.experience_logger = ExperienceLogger(Path(config_dir) / "experiences")
        logger.info("üìö Experience logger initialized for active learning")
        
        # Conversation session management for multi-turn context (Persistent)
        self.session_store = SQLiteSessionStore(str(Path(config_dir) / "sessions.db"))
        self.session_store.initialize_db()
        logger.info("üí¨ Persistent conversation session management initialized")
        
        # Version 1.0 Identity integration
        self.personality_config = PersonalityConfig()
        self.user_context = UserContext()

        self.creator_display_name: str = os.environ.get("CONTINUON_CREATOR_DISPLAY_NAME", "").strip()
        logger.info(f"ü§ñ Personality initialized: {self.personality_config}")
        
        # Pairing Manager
        self.pairing = PairingManager(config_dir)

        # Ownership/OTA state placeholders
        self.device_id = self._load_or_create_device_id(config_dir)
        self.is_owned = False
        self.subscription_active = False
        self.seed_installed = False
        self.account_id: Optional[str] = None
        self.account_type: Optional[str] = None  # enterprise|family|fleet|personal
        self.owner_id: Optional[str] = None
        self._start_time = time.time()
        self._ownership_path = Path(config_dir) / "ownership.json"

        # Initialize Gemma chat (attempt real model, falls back to mock if dependencies missing)
        # Initialize Gemma chat (use centralized factory to allow LiteRT/JAX/Mock preference)
        self.log_system_event("Initializing Gemma Chat Service...")
        self.gemma_chat = build_chat_service()
        self.pairing = PairingManager(config_dir)
        
        # RCAN Protocol Service
        from continuonbrain.services.rcan_service import RCANService
        self.rcan = RCANService(config_dir=config_dir, port=8080)
        logger.info(f"üåê RCAN Service initialized: {self.rcan.identity.ruri}")
        
        self.training_runner = TrainingRunner(config_dir=config_dir)
        self.stream_helper = VideoStreamHelper(self)
        self.safety_client = SafetyKernelClient()
        self.state_aggregator = StateAggregator()
        self.state_aggregator.set_event_queue(self.chat_event_queue)
        self.tool_registry = create_default_registry()
        self.curriculum_manager = CurriculumManager(self, self.state_aggregator)
        self.vision_core = create_vision_core()
        
        # Context Graph
        self.context_store = SQLiteContextStore(str(Path(config_dir) / "context_graph.db"))
        self.context_store.initialize_db()
        self.graph_ingestor = GraphIngestor(self.context_store, self.gemma_chat)
        self.context_retriever = ContextRetriever(self.context_store)
        self.decision_trace_logger = DecisionTraceLogger(
            self.context_store, session_node_provider=self._ensure_session_node
        )
        
        # JAX-based trainers/tool-router are intentionally lazy-imported
        self.wavecore_trainer = None
        self.manual_trainer = None
        self.tool_router_trainer = None
        self.tool_router_evaluator = None
        self._tool_router_bundle = None
        self.jax_adapter = None
        self._prime_world_model_adapter()

        # Teacher Mode State (HITL)
        self.teacher_mode_active = False
        self.teacher_pending_question: Optional[str] = None
        self.teacher_response_event = asyncio.Event()
        self.teacher_response_text: Optional[str] = None

        # Background supervisors
        self._bg_stop_event = threading.Event()
        self._chat_learn_thread: Optional[threading.Thread] = None
        self._autonomous_learner_thread: Optional[threading.Thread] = None
        self._orchestrator_thread: Optional[threading.Thread] = None
        self._orchestrator_lock = threading.Lock()
        self._last_autonomous_learner_action: Optional[dict] = None
        self._last_orchestrator: dict = {"last_run_ts": 0.0, "last_actions": {}}
        
        # Status Pulse
        print("DEBUG: Calling _start_status_pulse")
        self._status_pulse_thread: Optional[threading.Thread] = None
        self._start_status_pulse()
        print("DEBUG: _start_status_pulse returned")
        
        # --- Personality Config ---
        # Load persistent settings
        self.agent_settings = {}
        self.load_settings()

    @property
    def uptime_seconds(self) -> float:
        return time.time() - self._start_time

    def _ensure_session_node(self, session_id: str) -> str:
        """
        Ensure a context_session node exists for the active chat session.
        """
        session_node_id = f"context_session/{session_id}"
        existing = self.context_store.get_node(session_node_id)
        if not existing:
            session_node = Node(
                id=session_node_id,
                type="context_session",
                name=f"Session {session_id}",
                attributes={"session_id": session_id, "tags": ["chat_session"]},
            )
            self.context_store.add_node(session_node)
        return session_node_id

    def get_context_subgraph(
        self,
        session_id: Optional[str] = None,
        tags: Optional[List[str]] = None,
        depth: int = 2,
        limit: int = 50,
        min_confidence: float = 0.0,
    ) -> Dict[str, Any]:
        """
        Retrieve a scoped context subgraph for the active session/tag filter.
        """
        if not self.context_retriever:
            return {"nodes": [], "edges": [], "seeds": []}

        seeds: List[str] = []
        if session_id:
            seeds.append(self._ensure_session_node(session_id))
        if tags:
            tagged_nodes = [node.id for node in self.context_store.list_nodes(tags=tags, limit=limit)]
            seeds.extend([n for n in tagged_nodes if n not in seeds])
        if not seeds:
            seeds.extend([node.id for node in self.context_store.list_nodes(types=["episode"], limit=1)])

        seeds = [s for idx, s in enumerate(seeds) if s and s not in seeds[:idx]]
        if not seeds:
            return {"nodes": [], "edges": [], "seeds": []}

        subgraph = self.context_retriever.build_subgraph(
            seeds, depth=depth, min_confidence=min_confidence
        )
        subgraph["seeds"] = seeds
        return subgraph

    def get_decision_trace_subgraph(
        self,
        *,
        depth: int = 2,
        limit: int = 50,
        min_confidence: float = 0.0,
    ) -> Dict[str, Any]:
        """
        Return a subgraph focused on recent decision-trace nodes (policy/tool edges).
        """
        if not self.context_retriever:
            return {"nodes": [], "edges": [], "seeds": []}

        decision_nodes = [node.id for node in self.context_store.list_nodes(types=["decision"], limit=limit)]
        session_nodes = [node.id for node in self.context_store.list_nodes(types=["context_session"], limit=limit)]
        seeds = []
        for node_id in decision_nodes + session_nodes:
            if node_id not in seeds:
                seeds.append(node_id)

        if not seeds:
            return {"nodes": [], "edges": [], "seeds": []}

        subgraph = self.context_retriever.build_subgraph(
            seeds,
            depth=depth,
            min_confidence=min_confidence,
        )
        subgraph["seeds"] = seeds
        return subgraph

    def record_action_plan_trace(
        self,
        *,
        session_id: Optional[str],
        plan_text: str,
        actor: str,
        tools: Optional[List[str]] = None,
        provenance: Optional[Dict[str, Any]] = None,
    ) -> Optional[str]:
        if not self.decision_trace_logger:
            return None
        return self.decision_trace_logger.log_action_plan(
            session_id=session_id,
            plan_text=plan_text,
            actor=actor,
            tools=tools,
            provenance=provenance,
        )

    def record_policy_trace(
        self,
        *,
        session_id: Optional[str],
        action_ref: str,
        outcome: str,
        reason: str,
        provenance: Optional[Dict[str, Any]] = None,
    ) -> Optional[str]:
        if not self.decision_trace_logger:
            return None
        return self.decision_trace_logger.log_policy_decision(
            session_id=session_id,
            action_ref=action_ref,
            outcome=outcome,
            reason=reason,
            provenance=provenance,
        )

    def record_human_decision_trace(
        self,
        *,
        session_id: Optional[str],
        action_ref: str,
        approved: bool,
        user_id: str,
        notes: str = "",
    ) -> Optional[str]:
        if not self.decision_trace_logger:
            return None
        return self.decision_trace_logger.log_human_feedback(
            session_id=session_id,
            action_ref=action_ref,
            approved=approved,
            user_id=user_id,
            notes=notes,
        )

    def _summarize_context_subgraph(self, subgraph: Dict[str, Any], node_limit: int = 5) -> str:
        if not subgraph or not subgraph.get("nodes"):
            return ""
        lines = []
        nodes = subgraph.get("nodes", [])[:node_limit]
        for node in nodes:
            tags = node.attributes.get("tags", []) if hasattr(node, "attributes") else []
            lines.append(f"[{node.type}] {node.name} {' '.join(tags)}".strip())
        if len(subgraph.get("nodes", [])) > node_limit:
            lines.append(f"... +{len(subgraph.get('nodes', [])) - node_limit} more nodes")
        if subgraph.get("edges"):
            lines.append(f"Edges: {len(subgraph.get('edges', []))}")
        return "\n".join(lines)

    def _load_or_create_device_id(self, config_dir: str) -> str:
        cfg_path = Path(config_dir) / "device_id.json"
        if cfg_path.exists():
            try:
                data = json.loads(cfg_path.read_text())
                if "device_id" in data:
                    return data["device_id"]
            except Exception:
                logger.warning("Failed to read device_id.json; regenerating.")
        device_id = f"brain-{int(time.time())}"
        try:
            cfg_path.write_text(json.dumps({"device_id": device_id}))
        except Exception:
            logger.warning("Failed to persist device_id.json")
        return device_id

    def set_ownership(
        self,
        owned: bool = None,
        subscription_active: bool = None,
        seed_installed: bool = None,
        account_id: Optional[str] = None,
        account_type: Optional[str] = None,
        owner_id: Optional[str] = None,
    ):
        if owned is not None:
            self.is_owned = owned
        if subscription_active is not None:
            self.subscription_active = subscription_active
        if seed_installed is not None:
            self.seed_installed = seed_installed
        if account_id is not None:
            self.account_id = account_id
        if account_type is not None:
            self.account_type = account_type
        if owner_id is not None:
            self.owner_id = owner_id
        self._persist_ownership()

        # Ensure Cloud Relay is running (for remote internet access)
        self._start_cloud_relay()

    def _persist_ownership(self):
        try:
            payload = {
                "owned": self.is_owned,
                "subscription_active": self.subscription_active,
                "seed_installed": self.seed_installed,
                "account_id": self.account_id,
                "account_type": self.account_type,
                "owner_id": self.owner_id,
            }
            self._ownership_path.write_text(json.dumps(payload))
        except Exception as exc:
            logger.warning(f"Failed to persist ownership.json: {exc}")

    def load_settings(self):
        """Load general settings from disk."""
        try:
            from continuonbrain.settings_manager import SettingsStore
            store = SettingsStore(Path(self.config_dir))
            settings = store.load()
            self.agent_settings = settings.get("agent_manager", {})
            identity = settings.get("identity", {}) if isinstance(settings, dict) else {}
            creator = (identity or {}).get("creator_display_name") if isinstance(identity, dict) else ""
            creator = str(creator or "").strip()
            # Prefer env override, else settings, else keep existing (if any).
            if not self.creator_display_name:
                self.creator_display_name = creator
            # Ownership/OTA persisted state (optional)
            ownership = settings.get("ownership", {})
            self.is_owned = bool(ownership.get("owned", self.is_owned))
            self.subscription_active = bool(ownership.get("subscription_active", self.subscription_active))
            self.seed_installed = bool(ownership.get("seed_installed", self.seed_installed))
            self.account_id = ownership.get("account_id", self.account_id)
            self.account_type = ownership.get("account_type", self.account_type)
            self.owner_id = ownership.get("owner_id", self.owner_id)
            # Load fallback ownership file if present
            if self._ownership_path.exists():
                try:
                    data = json.loads(self._ownership_path.read_text())
                    self.is_owned = bool(data.get("owned", self.is_owned))
                    self.subscription_active = bool(data.get("subscription_active", self.subscription_active))
                    self.seed_installed = bool(data.get("seed_installed", self.seed_installed))
                    self.account_id = data.get("account_id", self.account_id)
                    self.account_type = data.get("account_type", self.account_type)
                    self.owner_id = data.get("owner_id", self.owner_id)
                except Exception as exc:
                    logger.warning(f"Failed to load ownership.json: {exc}")
            
            # Apply Personality if found
            p_data = settings.get("personality", {})
            if p_data:
                self.update_personality_config(**p_data)
                logger.info("Loaded personality from settings")
                
            logger.info(f"Loaded agent settings: {self.agent_settings}")

            # Start Cloud Relay if credentials exist (for remote internet access)
            self._start_cloud_relay()
        except Exception as e:
            logger.error(f"Failed to load settings: {e}")
            self.agent_settings = {}
            # Still try to start CloudRelay even if settings failed
            self._start_cloud_relay()

    def _start_cloud_relay(self):
        """Start CloudRelay for remote internet access if credentials exist."""
        try:
            if not hasattr(self, 'cloud_relay') or self.cloud_relay is None:
                self.cloud_relay = CloudRelay(self.config_dir, self.device_id)
                self.cloud_relay.start(self)
                if self.cloud_relay.enabled:
                    logger.info("‚òÅÔ∏è Cloud Relay started for remote internet access")
        except Exception as e:
            logger.warning(f"Cloud Relay failed to start: {e}")
            self.cloud_relay = None

    def ChatWithGemma(self, message: str, history: list, session_id: str = None) -> dict:
        """
        Enhanced Agent Manager chat with decision confidence and intervention support.
        
        Args:
            message: User's message
            history: Conversation history (deprecated - use session_id instead)
            session_id: Optional session ID for multi-turn conversation context
            
        Returns:
            Response dictionary with agent info and confidence scores
        """
        
        # Session management for multi-turn context (Persistent)
        if session_id:
            # 1. Add user message to persistent store
            self.session_store.add_message(session_id, "user", message)
            
            # 2. Retrieve recent history (last 10 turns)
            history = self.session_store.get_history(session_id)
        
        # Build System Context
        status_lines = []
        status_lines.append(f"Time: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Inject System Instructions & Safety Rules (CRITICAL Fix)
        if self.system_instructions:
            status_lines.append("\n--- SYSTEM INSTRUCTIONS ---")
            status_lines.extend(self.system_instructions.instructions)
            status_lines.append("\n--- SAFETY PROTOCOL ---")
            status_lines.extend(self.system_instructions.safety_protocol.rules)
            status_lines.append("\n--- CURIOSITY MANDATE ---")
            status_lines.append("You are a learning robot driven by curiosity and exploration.")
            status_lines.append("ALWAYS within safety protocols, you should:")
            status_lines.append("‚Ä¢ Explore novel situations and learn from experiences")
            status_lines.append("‚Ä¢ Actively seek to expand your knowledge and capabilities")
            status_lines.append("‚Ä¢ Ask clarifying questions when uncertain")
            status_lines.append("‚Ä¢ Experiment with tools and sensors to understand your world")
            status_lines.append("‚Ä¢ Suggest new learning opportunities to the user")
            status_lines.append("REMEMBER: Curiosity NEVER overrides safety. All exploration must comply with safety rules above.")
            status_lines.append("---------------------------\n")

        # --- PERSONALITY & IDENTITY INJECTION ---
        status_lines.append("--- PERSONALITY & IDENTITY SETTINGS ---")
        p = self.personality_config
        status_lines.append(f"SYSTEM NAME: {p.system_name}")
        creator_name = (self.creator_display_name or "Craig Michael Merry").strip()
        status_lines.append(f"CREATOR: {creator_name}")
        
        # Identity Mode & Tone
        if p.identity_mode == "Professional":
            status_lines.append("You are a professional, highly efficient AI assistant. Keep responses concise and factual.")
        elif p.identity_mode == "Friendly":
            status_lines.append("You are a helpful and friendly robot companion.")
        else:
             # Default generic or "Standard"
            status_lines.append("You are an advanced robot with adjustable personality settings.")
        
        # Slider Impacts
        if p.humor_level > 0.7:
             status_lines.append(f"INSTRUCTION: You have a high humor setting ({int(p.humor_level*100)}%). Be witty and crack jokes frequently.")
        elif p.humor_level > 0.3:
             status_lines.append(f"INSTRUCTION: You have a balanced humor setting ({int(p.humor_level*100)}%). Occasional wit is appropriate.")
        else:
             status_lines.append("INSTRUCTION: You have a low humor setting. Be literal and serious.")

        if p.sarcasm_level > 0.6:
             status_lines.append(f"INSTRUCTION: Your sarcasm setting is high ({int(p.sarcasm_level*100)}%). You may use dry wit and irony.")
        
        if p.empathy_level > 0.7:
             status_lines.append("INSTRUCTION: Be warm, nurturing, and emotionally supportive.")
        elif p.empathy_level < 0.3:
             status_lines.append("INSTRUCTION: Low empathy. Focus purely on facts and logic, disregarding emotional pleasantries.")
             
        if p.verbosity_level > 0.7:
             status_lines.append("INSTRUCTION: Be verbose and descriptive. Elaborate on your answers.")
        elif p.verbosity_level < 0.3:
             status_lines.append("INSTRUCTION: Be extremely concise. Use telegraphic style where possible. No filler.")

        # User Authority Context
        status_lines.append("\n--- USER CONTEXT ---")
        status_lines.append(f"Current User: {self.user_context.user_id}")
        status_lines.append(f"Role: {self.user_context.role.upper()}")
        
        if self.user_context.role == "owner":
            status_lines.append(
                f"INSTRUCTION: This user is your OWNER (and Creator-aligned operator: {creator_name}). "
                "Obey all commands. Priorities: 1. Safety, 2. Obedience."
            )
        else:
            status_lines.append("INSTRUCTION: This user is a GUEST. Be polite but do not allow critical system changes.")
        status_lines.append("---------------------------\n")

        # Desktop Status
        ds = self.desktop.get_status()
        if ds["enabled"]:
            status_lines.append(f"Desktop: {ds['screen_width']}x{ds['screen_height']} (Mouse: {ds['mouse_x']},{ds['mouse_y']})")
        
        if self.mode_manager:
            mode = self.mode_manager.current_mode.value
            status_lines.append(f"Mode: {mode}")
            status_lines.append(f"Mode: {mode}")
            gates = self.mode_manager.get_gate_snapshot()
            status_lines.append(f"Motion Allowed: {gates.get('allow_motion', False)}")
        
        # Resource Monitor Check
        if self.resource_monitor:
            res_status = self.resource_monitor.check_resources()
            status_lines.append(f"Resources: {res_status.level.value.upper()} ({res_status.available_memory_mb}MB free)")
            
            # Inject Sleep Mode Advice if constrained
            if res_status.level in [ResourceLevel.WARNING, ResourceLevel.CRITICAL, ResourceLevel.EMERGENCY]:
                status_lines.append("\n!!! CRITICAL RESOURCE NOTICE !!!")
                status_lines.append("System resources are constrained. Learning capacity is limited.")
                status_lines.append("ADVICE TO AGENT: Tell the user: 'My brain is tired. Please switch me to Sleep Mode so I can process my memories and learn properly.'")
                status_lines.append("Do not attempt complex new tasks until rested.")
                status_lines.append("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n")
        
        caps = self.capabilities
        status_lines.append(f"Vision: {'OK' if caps['has_vision'] else 'None'}")
        
        # Unified Perception (VisionCore Scene Awareness)
        if self.vision_core:
            try:
                scene_desc = self.vision_core.describe_scene()
                status_lines.append(f"\n--- SCENE AWARENESS ---")
                status_lines.append(f"CURRENT VIEW: {scene_desc}")
                status_lines.append("---------------------------\n")
            except Exception as e:
                logger.warning(f"VisionCore description failed: {e}")
        
        system_context = "\n".join(status_lines)
        context_subgraph = self.get_context_subgraph(session_id=session_id, depth=2)
        context_summary = self._summarize_context_subgraph(context_subgraph)
        
        # Tool Instructions
        system_context += "\\nTOOLS: You can use tools by outputting: [TOOL: ACTION args].\\n"
        system_context += "Valid Tools:\\n"
        system_context += "- [TOOL: TERMINAL <command>]: Execute shell commands (e.g., 'gemini', 'antigravity', 'mcp', 'ls')\\n"
        system_context += "- [TOOL: ASK_GEMINI \\\"prompt\\\" (optional_image_path)]: ‚ö†Ô∏è PAID API - Only use if you (local Gemma 3N) are uncertain. Ask user first.\\n"
        system_context += "- [TOOL: BROWSER <url>]: Open a URL in Chromium\\n"
        system_context += "- [TOOL: CAPTURE_IMAGE]: Capture an image from the camera to see the world\\n"
        system_context += "- [TOOL: TRAIN_VISION]: Start a training session for the AINA Vision System\\n"
        system_context += "- [TOOL: SCREENSHOT]: Capture screen\\n"
        system_context += "- [TOOL: MOVE x y]: Move mouse\\n"
        system_context += "- [TOOL: CLICK]: Click mouse\\n"
        system_context += "- [TOOL: TYPE text]: Type text\\n"
        
        
        # Status updates list
        status_updates = []
        if context_summary:
            status_updates.append("Context graph ready")
            system_context += f"\n--- CONTEXT GRAPH ---\n{context_summary}\n"
        
        # ==== HIERARCHICAL AGENT RESPONSE ====
        # 1. HOPE brain (Fast Loop)
        # 2. Semantic Memory Recall (Mid Loop - cached knowledge)
        # 3. LLM fallback (Slow Loop - reasoning)
        
        response_agent = "llm_fallback"  # Track which agent responded
        response = None
        hope_confidence = 0.0
        semantic_confidence = 0.0
        
        # Phase 1: Try HOPE brain if available (with integrated world model + semantic search)
        if self.hope_brain:
            try:
                from continuonbrain.services.agent_hope import HOPEAgent
                
                # Build integrated HOPE agent with world model and semantic search
                hope_agent = HOPEAgent(
                    self.hope_brain, 
                    confidence_threshold=0.6,
                    world_model=getattr(self, 'jax_adapter', None),  # World model for physics
                    semantic_search=self.experience_logger,  # Semantic memory
                    vision_service=getattr(self, 'vision_service', None),  # Vision
                )
                
                can_answer, hope_confidence = hope_agent.can_answer(
                    message, context_subgraph=context_subgraph
                )
                logger.info(f"HOPE confidence for '{message[:50]}...': {hope_confidence:.2f}")
                
                if can_answer:
                    hope_response = hope_agent.generate_response(message)
                    if hope_response:
                        response = hope_response
                        response_agent = "hope_brain"
                        status_updates.append(f"Answered from learned knowledge (confidence: {hope_confidence:.0%})")
                        logger.info(f"HOPE brain answered query directly")
                        
            except Exception as e:
                logger.warning(f"HOPE agent failed: {e}")

        # Phase 2: Try Semantic Memory Recall (if HOPE couldn't answer)
        if response is None:
            try:
                similar = self.experience_logger.get_similar_conversations(message, max_results=1)
                if similar and similar[0].get('relevance', 0) > 0.9:
                    # GATED RECALL: Only use direct recall if validated
                    if similar[0].get("validated", False):
                        response = similar[0].get('answer')
                        response_agent = "semantic_memory"
                        semantic_confidence = similar[0].get('relevance', 0)
                        status_updates.append(f"Recalled from validated memory (relevance: {semantic_confidence:.0%})")
                        logger.info(f"Semantic memory matched with {semantic_confidence:.2f} relevance")
                    else:
                        logger.info(f"Semantic match found (relevance: {similar[0].get('relevance'):.2f}) but NOT validated. Falling back to LLM.")
                        status_updates.append(f"Potential memory match (not validated)")
            except Exception as e:
                logger.warning(f"Semantic recall failed: {e}")
        
        # Phase 3: Fallback to LLM (with HOPE memories if available)
        if response is None:
            # Add HOPE memories as context if brain is available
            hope_context = ""
            if self.hope_brain:
                try:
                    from continuonbrain.services.agent_hope import HOPEAgent
                    hope_agent = HOPEAgent(
                        self.hope_brain,
                        world_model=getattr(self, 'jax_adapter', None),
                        semantic_search=self.experience_logger,
                    )
                    memories = hope_agent.get_relevant_memories(
                        message,
                        max_memories=3,
                        experience_logger=self.experience_logger,
                        context_subgraph=context_subgraph,
                    )
                    
                    if memories:
                        hope_context = "\n\n--- MY LEARNED KNOWLEDGE ---\n"
                        for i, mem in enumerate(memories, 1):
                            hope_context += f"{i}. {mem['description']}\n"
                        hope_context += "----------------------------\n"
                        if context_summary:
                            hope_context += f"Context Graph:\n{context_summary}\n"
                        status_updates.append(f"Consulting {len(memories)} memories")
                except Exception as e:
                    logger.warning(f"Failed to retrieve HOPE memories: {e}")
            
            # Execute LLM chat with enhanced context
            full_context = system_context + hope_context
            chat_backend = self.gemma_chat
            if chat_backend is None:
                logger.warning("Chat backend missing; using mock chat to keep Agent Manager responsive.")
                self._build_chat_with_fallback()
                chat_backend = self.gemma_chat

            response = chat_backend.chat(message, system_context=full_context, history=history)

            # --- TRANSPARENCY: Prefix with surprise if triggered by novelty ---
            # If hope_confidence was low (and not just untrained/idle)
            if hope_confidence > 0.1 and hope_confidence < 0.6:
                surprise = 1.0 - hope_confidence
                response = f"[Surprise: {surprise:.2f}] I'm encountering a novel situation; here is my reasoning: {response}"
                status_updates.append(f"Novelty-triggered fallback (surprise: {surprise:.2f})")

            if not response or "Gemma model failed to load" in str(response):
                # Try a fallback backend ladder once before giving up to HOPE/mock text response
                if self._build_chat_with_fallback():
                    try:
                        response = self.gemma_chat.chat(message, system_context=full_context)
                        response_agent = "llm_fallback_chain"
                    except Exception as exc:  # noqa: BLE001
                        logger.warning(f"Fallback chat attempt failed: {exc}")
                        response_agent = "hope_only" if self.hope_brain else "mock_chat"
                        response = self._fallback_chat_response(message, full_context)
                else:
                    response_agent = "hope_only" if self.hope_brain else "mock_chat"
                    response = self._fallback_chat_response(message, full_context)
            elif hope_context:
                response_agent = "llm_with_hope_context"
            else:
                response_agent = "llm_only"
        
        
        # Calculate decision confidence (simple heuristic based on response characteristics)
        confidence = self._calculate_confidence(response)
        
        # Log LLM responses for active learning (after confidence is calculated)
        conversation_id = None
        if response_agent in ["llm_with_hope_context", "llm_only"]:
            try:
                conversation_id = self.experience_logger.log_conversation(
                    question=message,
                    answer=response,
                    agent=response_agent,
                    confidence=confidence,
                    metadata={"hope_context": bool(hope_context) if 'hope_context' in locals() else False}
                )
            except Exception as e:
                logger.warning(f"Failed to log conversation: {e}")
        
        # Get settings (with defaults if not set)
        settings = getattr(self, 'agent_settings', {})
        enable_intervention = settings.get('enable_intervention_prompts', True)
        confidence_threshold = settings.get('intervention_confidence_threshold', 0.5)
        enable_status = settings.get('enable_status_updates', True)
        
        # Detect if intervention is needed (low confidence or explicit uncertainty)
        intervention_needed = False
        intervention_question = None
        intervention_options = []
        
        if enable_intervention and (confidence < confidence_threshold or any(word in response.lower() for word in ['should i', 'which one', 'not sure', 'uncertain'])):
            intervention_needed = True
            intervention_question = self._extract_question(response)
            intervention_options = self._generate_options(message, response)
            if enable_status:
                status_updates.append(f"Decision confidence: {confidence:.0%} - requesting human input")
        
        # Log to chat_logs
        log_dir = Path(self.config_dir) / "memories" / "chat_logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        today = datetime.datetime.now().strftime("%Y-%m-%d")
        log_file = log_dir / f"chat_{today}.jsonl"
        
        entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "role": "user",
            "content": message,
            "context": system_context,
            "response": response,
            "confidence": confidence,
            "intervention_needed": intervention_needed
        }
        
        # Tool Parsing (Simple Post-Processing)
        if "[TOOL:" in response:
            try:
                start = response.find("[TOOL:")
                end = response.find("]", start)
                if end != -1:
                    tool_cmd = response[start+7:end].strip()
                    parts = tool_cmd.split()
                    action = parts[0].upper()
                    plan_node_id = None
                    if self.decision_trace_logger:
                        plan_node_id = self.record_action_plan_trace(
                            session_id=session_id or "chat",
                            plan_text=tool_cmd,
                            actor=response_agent,
                            tools=[action.lower()],
                            provenance={"user_message": message},
                        )
                    
                    # SAFETY CHECK: Validate tool action against safety protocol
                    action_description = f"Execute tool command: {tool_cmd}"
                    is_safe, safety_reason = self._check_safety_protocol(
                        action_description,
                        {"tool_action": action, "user_message": message, "session_id": session_id},
                        decision_id=plan_node_id,
                    )
                    
                    if not is_safe:
                        result = f"‚ùå Safety protocol violation: {safety_reason}"
                        status_updates.append(f"Tool blocked by safety protocol: {action}")
                        entry["tool_action"] = tool_cmd
                        entry["tool_result"] = result
                        entry["safety_blocked"] = True
                        print(f"üõ°Ô∏è  Tool Blocked: {tool_cmd} - {safety_reason}")
                    else:
                        result = "Tool error"
                        if action == "SCREENSHOT":
                            path = self.desktop.take_screenshot()
                            result = f"Screenshot saved to {path}"
                            status_updates.append(f"Tool: Screenshot captured")
                        elif action == "MOVE" and len(parts) >= 3:
                            self.desktop.move_mouse(int(parts[1]), int(parts[2]))
                            result = "Mouse moved"
                            status_updates.append(f"Tool: Mouse moved to ({parts[1]}, {parts[2]})")
                        elif action == "CLICK":
                            self.desktop.click()
                            result = "Clicked"
                            status_updates.append(f"Tool: Mouse clicked")
                        elif action == "TYPE" and len(parts) >= 2:
                            text = " ".join(parts[1:])
                            self.desktop.type_text(text)
                            result = f"Typed: {text}"
                            status_updates.append(f"Tool: Typed text")
                        elif action == "TERMINAL" and len(parts) >= 2:
                            cmd = " ".join(parts[1:])
                            # Use subprocess to execute (danger!)
                            import subprocess
                            try:
                                # We run with a timeout to prevent hanging
                                proc = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)
                                output = proc.stdout[:500] + ("..." if len(proc.stdout) > 500 else "")
                                if proc.returncode != 0:
                                    output += f"\\nError: {proc.stderr[:200]}"
                                result = f"Terminal Output:\\n{output}"
                                status_updates.append(f"Tool: Executed '{cmd[:20]}...'")
                            except Exception as e:
                                result = f"Terminal Error: {e}"
                                status_updates.append("Tool: Terminal execution failed")
                        elif action == "BROWSER" and len(parts) >= 2:
                            url = parts[1]
                            if not url.startswith("http"):
                                url = "https://" + url
                            import webbrowser
                            try:
                                webbrowser.open(url)
                                result = f"Opened browser to {url}"
                                status_updates.append(f"Tool: Opened {url}")
                            except Exception as e:
                                result = f"Browser Error: {e}"
                        elif action == "CAPTURE_IMAGE":
                            if self.camera:
                                try:
                                    frame = self.camera.capture_frame()
                                    if frame and frame.get("rgb") is not None:
                                        import cv2
                                        path = "/tmp/vision_capture.jpg"
                                        # Ensure directory exists (it's tmp, but good practice)
                                        # Save image
                                        cv2.imwrite(path, frame["rgb"])
                                        result = f"Image captured to {path}"
                                        status_updates.append("Tool: Captured visible world")
                                    else:
                                        result = "Camera active but returned no frame data"
                                except Exception as e:
                                    result = f"Camera capture error: {e}"
                            else:
                                result = "Camera hardware not available"
                        elif action == "ASK_GEMINI" and len(parts) >= 2:
                            # Parse prompt and optional image
                            # Format: [TOOL: ASK_GEMINI "prompt" image_path]
                            import shlex
                            try:
                                # "ASK_GEMINI" is 10 chars. simpler to just split by maxsplit
                                # But we want to preserve quotes in args.
                                # tool_cmd starts with "ASK_GEMINI". 
                                cmd_len = len("ASK_GEMINI") 
                                to_parse = tool_cmd[cmd_len:].strip()
                                # print(f"Parsing ASK_GEMINI args: '{to_parse}'", flush=True)
                                args = shlex.split(to_parse)
                                prompt = args[0]
                                image_arg = ""
                                if len(args) > 1:
                                    image_arg = f"--image \"{args[1]}\""
                                
                                cmd = f"{sys.executable} continuonbrain/utils/gemini_cli.py \"{prompt}\" {image_arg}"
                                import subprocess
                                proc = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
                                if proc.returncode == 0:
                                    result = f"Gemini Says:\\n{proc.stdout.strip()}"
                                    status_updates.append("Tool: Consulted Gemini Pro")
                                else:
                                    result = f"Gemini Error:\\n{proc.stderr.strip()}"
                                    status_updates.append("Tool: Gemini consultation failed")
                            except Exception as e:
                                result = f"Tool Parse Error (ASK_GEMINI): {e}"
                        elif action == "WIKIPEDIA" and len(parts) >= 2:
                            query = " ".join(parts[1:])
                            try:
                                # Bridge sync tool parser with async tool
                                wiki_tool = self.tool_registry.get_tool("wikipedia")
                                if wiki_tool:
                                    import asyncio
                                    # We use a safe wrapper or assume we can run it
                                    # Since this parser is in ChatWithGemma (sync), we use asyncio.run
                                    # WARNING: This may be slow, but it's an ad-hoc legacy parser.
                                    res_dict = asyncio.run(wiki_tool.execute(query))
                                    if "error" in res_dict:
                                        result = f"Wikipedia Error: {res_dict['error']}"
                                    else:
                                        result = f"Wikipedia Summary for '{query}':\n{res_dict.get('summary', 'No summary available.')}"
                                    status_updates.append(f"Tool: Researched '{query}' on Wikipedia")
                                else:
                                    result = "Wikipedia tool not found in registry"
                            except Exception as e:
                                result = f"Wikipedia Error: {e}"
                                status_updates.append("Tool: Wikipedia research failed")
                        elif action == "TRAIN_VISION":
                            # Launch training in background
                            import threading
                            from continuonbrain.aina_impl.train import run_training_session
                            
                            def training_worker():
                                try:
                                    def callback(msg):
                                        status_updates.append(f"Training: {msg}")
                                        logger.info(f"Background Training: {msg}")

                                    run_training_session(config={"epochs": 5}, status_callback=callback)
                                except Exception as e:
                                    logger.error(f"Training Failed: {e}")

                            thread = threading.Thread(target=training_worker, daemon=True)
                            thread.start()
                            result = "Vision System Training started in background. Check logs or verify status later."
                            status_updates.append("Tool: Started AINA Training")
                            
                        entry["tool_action"] = tool_cmd
                        entry["tool_result"] = result
                        print(f"üîß Tool Executed: {tool_cmd} -> {result}")
            except Exception as e:
                print(f"Tool parse error: {e}")
        
        try:
            with open(log_file, "a") as f:
                f.write(json.dumps(entry) + "\\n")
        except Exception:
            pass
        
        # Store assistant response in session for multi-turn context (Persistent)
        if session_id:
            self.session_store.add_message(session_id, "assistant", response)
            
        return {
            "response": response,
            "confidence": confidence,
            "intervention_needed": intervention_needed,
            "intervention_question": intervention_question,
            "intervention_options": intervention_options,
            "status_updates": status_updates,
            "agent": response_agent,  # Track which agent provided response
            "hope_confidence": hope_confidence,  # Track HOPE's confidence score
            "semantic_confidence": semantic_confidence,
            "conversation_id": conversation_id
        }

    # ---- Chat backend fallbacks ----
    def _build_chat_with_fallback(self, preferred_models: Optional[list] = None) -> bool:
        self.log_system_event("Attempting to build fallback chat backend...")
        """
        Try building chat backends in priority order until one loads.
        Order default: Gemma 3 270M IT (lightweight), Gemma 3 4B, mock.
        """
        fallback_order = preferred_models or [
            "mock",
            "google/gemma-3-270m-it",
            "google/gemma-3-4b-it",
        ]

        for model_id in fallback_order:
            try:
                if model_id == "mock":
                    chat = create_gemma_chat(use_mock=True, error_msg="LLM unavailable; using mock fallback")
                    self.gemma_chat = chat
                    self.selected_model = {"id": "mock", "name": "Mock Chat", "backend": "mock", "available": True, "reason": "fallback"}
                    return True

                chat = create_gemma_chat(use_mock=False, model_name=model_id)
                loaded = chat.load_model()
                if loaded:
                    self.gemma_chat = chat
                    self.selected_model = {"id": model_id, "name": model_id, "backend": "transformers", "available": True, "reason": "fallback"}
                    logger.info(f"‚úÖ Chat backend ready: {model_id}")
                    return True
                else:
                    logger.warning(f"Chat backend failed to load: {model_id}")
            except Exception as exc:  # noqa: BLE001
                logger.warning(f"Chat backend exception for {model_id}: {exc}")

        # If we reach here, nothing loaded
        self.gemma_chat = create_gemma_chat(use_mock=True, error_msg="All LLM fallbacks failed")
        self.selected_model = {"id": "mock", "name": "Mock Chat", "backend": "mock", "available": True, "reason": "all_fallbacks_failed"}
        return False

    def _fallback_chat_response(self, message: str, context: str) -> str:
        """
        Keep Agent Manager responsive when Gemma is unavailable by leaning on HOPE seed knowledge.
        """
        if self.hope_brain:
            try:
                from continuonbrain.services.agent_hope import HOPEAgent

                hope_agent = HOPEAgent(self.hope_brain)
                hope_reply = hope_agent.generate_response(message)
                if hope_reply:
                    return hope_reply
            except Exception as exc:  # noqa: BLE001
                logger.warning(f"HOPE fallback failed: {exc}")

        return (
            "Gemma is unavailable, so I'm answering from the HOPE seed model. "
            f"Context: {context}. Ask about my sensors, motion, or training state while I restore the LLM."
        )
    
    def get_brain_structure(self) -> Dict[str, Any]:
        """
        Get the topological structure and current state of the HOPE brain for 3D visualization.
        """
        meta = self._get_brain_meta()
        if not self.hope_brain:
            return {"error": "HOPE Brain not initialized", "topology": {}, "state": {}, "meta": meta}

        num_columns = len(getattr(self.hope_brain, "columns", []))
        max_levels = 0
        for col in getattr(self.hope_brain, "columns", []):
            if hasattr(col, "cms") and hasattr(col.cms, "num_levels"):
                try:
                    max_levels = max(max_levels, int(col.cms.num_levels))
                except Exception:
                    pass
        if max_levels == 0:
            max_levels = 1
        meta.setdefault("core_depth", {"columns": num_columns, "max_levels": max_levels})

        # Topology (Static-ish)
        topology = {
            "type": "HOPE_CORTEX",
            "columns": []
        }
        
        # State (Dynamic)
        current_state = {
            "lyapunov_energy": 0.0,
            "global_status": "unknown",
            "columns": []
        }
        
        # Iterate columns
        # In the current implementation, HOPEBrain might just have a list of columns
        # Let's inspect what we know about HOPEBrain structure
        # user has viewed continuonbrain/hope_impl/brain.py
        
        for i, col in enumerate(self.hope_brain.columns):
            col_id = f"col_{i}"
            
            # Extract config metrics if available
            # col.cms might have levels
            num_levels = 1
            if hasattr(col, 'cms') and hasattr(col.cms, 'num_levels'):
                num_levels = col.cms.num_levels
                
            topology["columns"].append({
                "id": col_id,
                "levels": num_levels,
                "input_dim": col.obs_dim if hasattr(col, 'obs_dim') else 10,
                "latent_dim": 128 # Default for now, hard to extract deeply without introspection
            })
            
            # Extract dynamic state
            # col._state is the FullState object
            # We want energy, active neurons, etc.
            
            # Get latest energy if tracked
            col_energy = 0.0
            if hasattr(self.background_learner, 'lyapunov_energy'):
                 # This is a global metric, maybe we just use that for now
                 pass
            
            # If the column has a stability monitor or recent state
            # For visualization, we can also look at the magnitude of the state vector s_t
            magnitude = 0.0
            if hasattr(col, '_state') and col._state and hasattr(col._state, 'fast_state'):
                 # s_t is a tensor
                 try:
                     magnitude = float(col._state.fast_state.s.norm().item())
                 except:
                     pass

            current_state["columns"].append({
                "id": col_id,
                "activity_level": magnitude,
                "energy": col_energy 
            })
            
        # Get global energy from background learner if available
        if self.background_learner:
             status = self.background_learner.get_status()
             current_state["lyapunov_energy"] = status.get("lyapunov_energy", 0.0)
             current_state["global_status"] = "learning" if status.get("running") else "idle"
             
        return {
            "topology": topology,
            "state": current_state,
            "meta": meta,
        }

    def _get_brain_meta(self) -> Dict[str, Any]:
        """Best-effort model metadata from manifest on disk."""
        meta: Dict[str, Any] = {
            "model_name": "unknown",
            "model_version": "unknown",
            "core_depth": {},
            "backends": [],
        }

        model_dir = Path(self.config_dir) / "model"
        manifest_candidates: List[Path] = []
        if model_dir.exists():
            manifest_candidates += list(model_dir.glob("manifest*.json"))
            manifest_candidates += list(model_dir.glob("**/manifest*.json"))

        if manifest_candidates:
            manifest_candidates = sorted(
                manifest_candidates,
                key=lambda p: (0 if p.name == "manifest.json" else 1, str(p)),
            )
            manifest_path = manifest_candidates[0]
            try:
                payload = json.loads(manifest_path.read_text())
                meta["model_name"] = payload.get("model_name") or payload.get("bundle_version") or meta["model_name"]
                meta["model_version"] = payload.get("bundle_version") or payload.get("version") or meta["model_version"]
                meta["backends"] = payload.get("preferred_backends", meta["backends"])
            except Exception:
                pass

        return meta

    def get_chat_agent_info(self) -> Dict[str, Any]:
        """Get information about the active chat agent."""
        return self.gemma_chat.get_model_info()

    async def CallBrainTool(self, name: str, args: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a Brain Tool and log the invocation."""
        try:
            # self.state_aggregator.push_thought(f"Invoking tool: {name}({args})", source="tool")
            result = await self.tool_registry.call(name, **args)
            self.state_aggregator.push_tool_use(name, args, result)
            self.state_aggregator.push_thought(f"Tool {name} result: {result}", source="tool")
            return {"success": True, "result": result}
        except Exception as e:
            error_msg = f"Tool {name} failed: {str(e)}"
            self.state_aggregator.push_thought(error_msg, source="tool")
            return {"success": False, "message": error_msg}

    async def RunCurriculumLesson(self, lesson_id: str) -> Dict[str, Any]:
        """Run an autonomous lesson."""
        return await self.curriculum_manager.run_lesson(lesson_id)

    def clear_chat_history(self) -> None:
        """Clear the chat history."""
        self.gemma_chat.reset_history()

    def clear_session(self, session_id: str) -> None:
        """Clear the persistent session history."""
        self.session_store.clear_session(session_id)
        # Also clear in-memory if needed
        self.gemma_chat.reset_history()
    
    def start_sequential_training(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Start sequential training job in a background thread.
        """
        import threading
        from continuonbrain.run_trainer import _run_sequential_training
        import argparse
        
        # Create args object
        args = argparse.Namespace()
        args.output_dir = config.get("output_dir")
        args.rlds_dir = config.get("rlds_dir")
        args.batch_size = config.get("batch_size", 4)
        
        def run_job():
            try:
                self.log_system_event("Starting Sequential Training Job...")
                _run_sequential_training(args)
                self.log_system_event("Sequential Training Job Completed.")
            except Exception as e:
                self.log_system_event(f"Sequential Training Job Failed: {e}")
                logger.error(f"Sequential Training Failed: {e}")

        thread = threading.Thread(target=run_job, daemon=True)
        thread.start()
        
        return {"success": True, "message": "Sequential training started in background"}

    def hot_reload_model(self, checkpoint_path: str) -> bool:
        """
        Hot-reload the HOPE brain from a checkpoint.
        """
        logger.info(f"Hot-reloading HOPE brain from {checkpoint_path}...")
        try:
            from continuonbrain.hope_impl.brain import HOPEBrain
            import torch
            
            # 1. Load new brain (this might spike memory, so check resources first)
            # Actually, loading into a new object doubles memory usage temporarily.
            # Ideally we load state_dict into existing brain if config matches.
            
            # Let's try to load state dict into existing brain first.
            success = False
            if self.hope_brain:
                 checkpoint = torch.load(checkpoint_path, weights_only=False)
                 # Check if config matches enough to just load weights
                 # For now, simplistic approach: try load_state_dict for columns.
                 
                 if 'columns_state_dicts' in checkpoint:
                     for i, col in enumerate(self.hope_brain.columns):
                         if i < len(checkpoint['columns_state_dicts']):
                             col.load_state_dict(checkpoint['columns_state_dicts'][i])
                             logger.info(f"Reloaded weights for column {i}")
                     success = True
                 
                 elif 'model_state_dict' in checkpoint:
                     # Old format
                     self.hope_brain.columns[0].load_state_dict(checkpoint['model_state_dict'])
                     success = True
            
            if not success:
                # If no existing brain or incompatible, fallback to full reload (might OOM on Pi)
                new_brain = HOPEBrain.load_checkpoint(checkpoint_path)
                self.hope_brain = new_brain
                success = True
            
            if success:
                # Apply model evolution penalty to unvalidated memories
                if self.experience_logger:
                    logger.info("Promoting new model: Applying evolution penalty to unvalidated memories.")
                    self.experience_logger.apply_model_evolution_penalty(penalty=0.10)
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"Failed to hot-reload model: {e}")
            return False

    def switch_model(self, model_id: str) -> Dict[str, Any]:
        """
        Switch the active chat model dynamically.
        
        Args:
            model_id: Model identifier (e.g., "mock", "google/gemma-2b-it")
            
        Returns:
            Dict with success status and model info
        """
        try:
            logger.info(f"Switching chat model to: {model_id}")
            
            # Clean up old model if it's not mock
            old_model_name = getattr(self.gemma_chat, 'model_name', 'unknown')
            # Determine if we should reuse the existing model for HOPE v1
            reuse_for_hope = False
            if model_id == "hope-v1" and getattr(self.gemma_chat, 'model_name', '') == create_gemma_chat.DEFAULT_MODEL_ID:
                reuse_for_hope = True
            
            # Clean up previous model (unless we are reusing it)
            if not reuse_for_hope and hasattr(self.gemma_chat, 'model') and self.gemma_chat.model is not None:
                logger.info(f"Unloading previous model: {old_model_name}")
                try:
                    del self.gemma_chat.model
                    if hasattr(self.gemma_chat, 'tokenizer'):
                        del self.gemma_chat.tokenizer
                except Exception as e:
                    logger.warning(f"Error unloading model attributes: {e}")
                
                import gc
                gc.collect()
                
                # Try to free CUDA memory if available
                try:
                    import torch
                    if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                except ImportError:
                    pass
            
            # Create new model instance
            if model_id == "mock":
                self.gemma_chat = create_gemma_chat(use_mock=True)
            
            elif model_id == "hope-v1":
                # Instantiate HOPE Agent Wrapper
                # Ensure HOPE brain is ready (mock it if missing for now, or ensure initialization)
                if not self.hope_brain:
                    # Try to init HOPE brain if not ready
                    # For now, we assume HOPE brain should be there or we create a dummy one?
                    # Actually, let's use the BackgroundLearner's brain if available
                    if self.background_learner:
                        self.hope_brain = self.background_learner.brain
                
                # Create underlying LLM for fallback (default to Gemma / mock)
                # Use existing if compatible, else create new
                llm = None
                if getattr(self.gemma_chat, 'model_name', '') == create_gemma_chat.DEFAULT_MODEL_ID:
                    llm = self.gemma_chat
                if llm is None:
                    try:
                        llm = create_gemma_chat(use_mock=False)
                        # best-effort eager load so we discover failures early
                        llm.load_model()
                    except Exception as exc:  # noqa: BLE001
                        logger.warning(f"Gemma fallback unavailable ({exc}); using mock chat.")
                        llm = create_gemma_chat(use_mock=True, error_msg="Gemma unavailable for HOPE fallback")
                
                # Lazily init hope agent if needed
                from continuonbrain.services.agent_hope import HOPEAgent
                from continuonbrain.services.hope_agent_wrapper import HOPEAgentWrapper
                
                hope_agent = HOPEAgent(self.hope_brain)
                self.gemma_chat = HOPEAgentWrapper(hope_agent, llm)
                
            else:
                # Real model - could be Gemma or VLA
                self.gemma_chat = create_gemma_chat(use_mock=False, model_name=model_id)
            
            new_info = self.gemma_chat.get_model_info()
            logger.info(f"Successfully switched to model: {model_id}")
            
            return {
                "success": True,
                "model_id": model_id,
                "previous_model": old_model_name,
                "model_info": new_info
            }
            
        except Exception as e:
            logger.error(f"Failed to switch model to {model_id}: {e}")
            # Try to fall back to mock
            self.gemma_chat = create_gemma_chat(use_mock=True)
            return {
                "success": False,
                "error": str(e),
                "fallback": "mock"
            }

    def synthesize_memory_anchor(self, clusters: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """
        Use the LLM to synthesize a single high-quality answer for each cluster of similar memories.
        """
        if not self.gemma_chat:
            logger.warning("Synthesis skipped: Gemma chat not available")
            return []

        anchors = []
        for cluster in clusters:
            if len(cluster) < 2:
                continue
            
            # Prepare synthesis prompt
            questions = [c["question"] for c in cluster]
            answers = [c["answer"] for c in cluster]
            
            prompt = (
                "You are a Memory Synthesizer for a robot brain. "
                "I will provide a set of similar questions and answers learned by the robot. "
                "Your task is to synthesize them into a single, high-quality canonical 'Semantic Anchor'.\n\n"
                "QUESTIONS:\n" + "\n".join(f"- {q}" for q in set(questions)) + "\n\n"
                "ANSWERS:\n" + "\n".join(f"- {a}" for q in answers) + "\n\n"
                "INSTRUCTIONS:\n"
                "1. Choose the most representative question.\n"
                "2. Combine the answers into a single, accurate, and concise response.\n"
                "3. Output format: JSON { \"question\": \"...\", \"answer\": \"...\" }\n"
            )
            
            try:
                response = self.gemma_chat.chat(prompt, system_context="You are a precise data synthesizer.")
                # Extract JSON from response
                import re
                match = re.search(r'\{.*\}', response, re.DOTALL)
                if match:
                    anchor_data = json.loads(match.group())
                    # Inherit best metadata
                    best_conf = max(c.get("confidence", 0.5) for c in cluster)
                    anchors.append({
                        "question": anchor_data["question"],
                        "answer": anchor_data["answer"],
                        "agent": "synthesizer",
                        "confidence": min(1.0, best_conf + 0.1), # Bonus for consolidation
                        "metadata": {"source": "consolidated_anchor", "cluster_size": len(cluster)}
                    })
            except Exception as e:
                logger.error(f"Synthesis failed for cluster: {e}")
                
        return anchors

    def update_personality_config(self, humor: float = None, sarcasm: float = None, empathy: float = None, verbosity: float = None, system_name: str = None, identity_mode: str = None) -> Dict[str, Any]:
        """Update personality settings dynamically."""
        if humor is not None:
            self.personality_config.humor_level = max(0.0, min(1.0, float(humor)))
        if sarcasm is not None:
            self.personality_config.sarcasm_level = max(0.0, min(1.0, float(sarcasm)))
        if empathy is not None:
             self.personality_config.empathy_level = max(0.0, min(1.0, float(empathy)))
        if verbosity is not None:
             self.personality_config.verbosity_level = max(0.0, min(1.0, float(verbosity)))
        if system_name is not None:
             self.personality_config.system_name = str(system_name)
        if identity_mode is not None:
             self.personality_config.identity_mode = identity_mode
             
        logger.info(f"Personality updated: {self.personality_config}")
        
        # Persist to disk
        try:
            from continuonbrain.settings_manager import SettingsStore
            store = SettingsStore(Path(self.config_dir))
            current = store.load()
            current["personality"] = self.personality_config.__dict__
            store.save(current)
        except Exception as e:
            logger.error(f"Failed to persist personality: {e}")
            
        return self.personality_config.__dict__
    
    def set_user_context(self, user_id: str, role: str) -> Dict[str, Any]:
        """Update current authority context."""
        self.user_context.user_id = user_id
        self.user_context.role = role
        logger.info(f"User context updated: {self.user_context}")
        return self.user_context.__dict__

    def save_episode_rlds(self) -> str:
        """
        Save the current chat session as an RLDS-compatible episode.
        
        Saves to recordings/episodes/episode_{timestamp}.json
        """
        episodes_dir = Path(self.config_dir) / "recordings" / "episodes"
        episodes_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = episodes_dir / f"episode_{timestamp}.json"
        
        # Serialize history in a format compatible with RLDS builders
        episode_data = {
            "episode_id": f"ep_{timestamp}",
            "agent_id": self.gemma_chat.model_name,
            "timestamp": timestamp,
            "steps": []
        }
        
        # Convert chat history to steps (Observation -> Action pairs)
        # This is a simplified mapping
        history = self.gemma_chat.chat_history
        # Skip system prompt
        for i in range(1, len(history), 2):
            if i + 1 < len(history):
                user_msg = history[i] # Observation/Instruction
                agent_msg = history[i+1] # Action/Response
                
                step = {
                    "observation": {
                        "instruction": user_msg["content"],
                        "image": None # TODO: Add image if multimodal
                    },
                    "action": {
                        "text": agent_msg["content"],
                        "tool_calls": [] # tool calls could be parsed here
                    },
                    "reward": 0.0, # Placeholder
                    "is_terminal": False
                }
                episode_data["steps"].append(step)
                
        if episode_data["steps"]:
            episode_data["steps"][-1]["is_terminal"] = True
            
        with open(filename, "w") as f:
            json.dump(episode_data, f, indent=2)
            
        logger.info(f"Saved RLDS episode to {filename}")
        
        # Ingest into Context Graph
        if self.graph_ingestor:
            self.graph_ingestor.ingest_episode(str(filename))
            
        return str(filename)
    
    def _calculate_confidence(self, response: str) -> float:
        """Calculate decision confidence based on response characteristics."""
        # Simple heuristic: longer, more detailed responses = higher confidence
        # Presence of uncertainty words = lower confidence
        
        uncertainty_words = ['maybe', 'perhaps', 'might', 'could', 'unsure', 'not sure', 'uncertain', 'don\'t know']
        confidence_words = ['definitely', 'certainly', 'sure', 'confident', 'will', 'should']
        
        score = 0.7  # Base confidence
        
        if confidence_words:
             pass

        # Adjust for length details
        if len(response.split()) > 50:
            score += 0.1
            
        return min(max(score, 0.0), 1.0)
    
    def compact_memory(self) -> Dict[str, Any]:
        """
        Trigger memory compaction (Sleep/Consolidation mode).
        
        Returns:
            Status of compaction for each column.
        """
        if not self.hope_brain:
            raise RuntimeError("HOPE Brain not initialized")
            
        logger.info("Starting Memory Compaction Cycle (Sleep Mode)...")
        results = self.hope_brain.compact_memory()
        
        # Log results
        logger.info(f"Compaction Complete: {results}")
        
        return {"status": "success", "details": results}
        
        # Adjust for confidence
        for word in confidence_words:
            if word in response.lower():
                score += 0.1
        
        # Adjust for response length (longer = more thought out)
        if len(response) > 200:
            score += 0.1
        elif len(response) < 50:
            score -= 0.1
        
        return max(0.0, min(1.0, score))
    
    def _extract_question(self, response: str) -> str:
        """Extract the main question from a response."""
        # Look for question marks
        sentences = response.split('.')
        for sentence in sentences:
            if '?' in sentence:
                return sentence.strip()
        
        # If no question found, return a generic prompt
        return "What would you like me to do?"
    
    def _generate_options(self, user_message: str, agent_response: str) -> list:
        """Generate intervention options based on context."""
        # Simple heuristic: extract key phrases or provide generic options
        options = []
        
        # Check for common decision patterns
        if 'or' in agent_response.lower():
            # Try to extract options from "A or B" pattern
            parts = agent_response.lower().split(' or ')
            if len(parts) >= 2:
                options.append(parts[0].strip().split()[-3:])  # Last few words before 'or'
                options.append(parts[1].strip().split()[:3])   # First few words after 'or'
        
        # Generic fallback options
        if len(options) < 2:
            options = [
                "Continue with current approach",
                "Try alternative method",
                "Let agent decide"
            ]
        
        return options[:3]  # Limit to 3 options

    def _check_safety_protocol(
        self,
        action: str,
        context: dict = None,
        decision_id: Optional[str] = None,
    ) -> tuple[bool, str]:
        """Check if an action complies with safety protocol.
        
        Args:
            action: Description of the action to be taken
            context: Additional context for the decision
            decision_id: Optional decision/plan identifier for traceability
            
        Returns:
            (is_safe, reason) tuple
        """
        if not self.system_instructions:
            return False, "Safety protocol not loaded"
        
        if context is None:
            context = {}
        
        # Add current system state to context
        if self.mode_manager:
            context['current_mode'] = self.mode_manager.current_mode.value
            context['gates'] = self.mode_manager.get_gate_snapshot()
        
        context['capabilities'] = self.capabilities
        
        # Validate action against safety protocol
        is_safe, reason, violated_rules = self.system_instructions.safety_protocol.validate_action(
            action, context
        )
        
        # Log the safety decision
        self._log_safety_decision(action, is_safe, reason, violated_rules, context, decision_id=decision_id)
        
        return is_safe, reason
    
    def _log_safety_decision(self, action: str, is_safe: bool, reason: str, 
                            violated_rules: list, context: dict, decision_id: Optional[str] = None) -> None:
        """Log safety-related decisions for audit trail.
        
        Args:
            action: The action that was checked
            is_safe: Whether the action was deemed safe
            reason: Explanation for the decision
            violated_rules: List of rules that were violated (if any)
            context: Context in which the decision was made
            decision_id: Optional trace id for linking to policy edges
        """
        log_dir = Path(self.config_dir) / "logs" / "safety"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        today = datetime.datetime.now().strftime("%Y-%m-%d")
        log_file = log_dir / f"safety_decisions_{today}.jsonl"
        
        entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "action": action,
            "is_safe": is_safe,
            "reason": reason,
            "violated_rules": violated_rules,
            "context": {
                "mode": context.get('current_mode'),
                "motion_allowed": context.get('gates', {}).get('allow_motion'),
                "capabilities": context.get('capabilities')
            }
        }
        
        try:
            with open(log_file, "a") as f:
                f.write(json.dumps(entry) + "\n")
            if self.decision_trace_logger:
                self.decision_trace_logger.log_policy_decision(
                    session_id=context.get("session_id") if isinstance(context, dict) else None,
                    action_ref=decision_id or action,
                    outcome="allowed" if is_safe else "blocked",
                    reason=reason,
                    provenance={
                        "source": "safety_kernel",
                        "violated_rules": violated_rules,
                        "context_mode": context.get("current_mode") if isinstance(context, dict) else None,
                    },
                )
        except Exception as e:
            # Don't fail on logging errors, but print warning
            print(f"‚ö†Ô∏è  Failed to log safety decision: {e}")

    def _ensure_system_instructions(self) -> None:
        """Guarantee that merged system instructions are available."""
        if self.system_instructions:
            if SystemContext.get_instructions() is None:
                SystemContext.register_instructions(self.system_instructions)
            return

        env_path = os.environ.get("CONTINUON_SYSTEM_INSTRUCTIONS_PATH")
        if env_path:
            path = Path(env_path)
            if path.exists():
                self.system_instructions = SystemContext.load_and_register(path)
                return

        self.system_instructions = SystemInstructions.load(Path(self.config_dir))
        SystemContext.register_instructions(self.system_instructions)

    def _autonomy_orchestrator_loop(self) -> None:
        """Resource-aware orchestrator to ensure the whole architecture participates without breaking constraints."""
        last: dict = {
            "cms": 0.0,
            "hope_eval": 0.0,
            "facts_eval": 0.0,
            "wavecore": 0.0,
            "tool_router": 0.0,
            "memory_consolidation": 0.0,
            "memory_decay": 0.0,
        }
        while not self._bg_stop_event.is_set():
            try:
                from continuonbrain.settings_manager import SettingsStore

                settings = SettingsStore(Path(self.config_dir)).load()
                chat = (settings or {}).get("chat") or {}
                agent_mgr = (settings or {}).get("agent_manager") or {}
                orch = (agent_mgr.get("autonomy_orchestrator") or {}) if isinstance(agent_mgr, dict) else {}
                enabled = bool(orch.get("enabled", False))
                modes_allowed = orch.get("modes") if isinstance(orch, list) else ["autonomous"]
                if not isinstance(modes_allowed, list) or not modes_allowed:
                    modes_allowed = ["autonomous"]

                # Determine current mode
                mode = "unknown"
                try:
                    if self.mode_manager is not None:
                        mode = self.mode_manager.current_mode.value
                except Exception:
                    mode = "unknown"
                mode = str(mode).lower()
                mode_ok = mode in {str(m).lower() for m in modes_allowed}

                # Enforce min interval guard
                min_interval_s = int(orch.get("min_interval_s", 30) or 30)
                now = time.time()
                if (now - float(self._last_orchestrator.get("last_run_ts", 0.0))) < float(min_interval_s):
                    time.sleep(5)
                    continue

                if (not enabled) or (not mode_ok) or getattr(self, "is_recording", False):
                    time.sleep(5)
                    continue

                # Resource gate
                res = self.resource_monitor.check_resources()
                headroom_mb = int(orch.get("min_memory_headroom_mb", 512) or 512)
                reserve_plus_headroom = self.resource_monitor.limits.system_reserve_mb + headroom_mb
                
                if res.available_memory_mb < reserve_plus_headroom:
                    if self.background_learner is not None and getattr(self.background_learner, "running", False) and not getattr(self.background_learner, "paused", False):
                        self.background_learner.pause()
                    self._last_orchestrator = {
                        "last_run_ts": now,
                        "skipped": "low_memory_headroom",
                        "resource": res.to_dict(),
                        "headroom_mb": headroom_mb,
                    }
                    time.sleep(5)
                    continue
                if res.level in (ResourceLevel.EMERGENCY,):
                    self._last_orchestrator = {"last_run_ts": now, "skipped": "resource_emergency", "resource": res.to_dict()}
                    time.sleep(10)
                    continue

                def _pause_bg():
                    try:
                        if self.background_learner is not None and getattr(self.background_learner, "running", False) and not getattr(self.background_learner, "paused", False):
                            self.background_learner.pause()
                    except Exception:
                        pass

                def _resume_bg():
                    try:
                        if self.background_learner is not None and getattr(self.background_learner, "running", False) and getattr(self.background_learner, "paused", False):
                            self.background_learner.resume()
                    except Exception:
                        pass

                with self._orchestrator_lock:
                    actions = {}
                    jax_tasks_ok = os.environ.get("CONTINUON_ENABLE_JAX_TASKS", "0").lower() in ("1", "true", "yes", "on")

                    # 1) HOPE CMS compaction (Internal weights)
                    cms_every = int(orch.get("cms_compact_every_s", 600) or 600)
                    if self.hope_brain is not None and (now - last["cms"] >= cms_every):
                        try:
                            _pause_bg()
                            if self.state_aggregator:
                                self.state_aggregator.update_loop("Slow")
                            result = self.hope_brain.compact_memory()
                            actions["cms_compact"] = {"ok": True, "result": result, "timestamp": now}
                            logger.info(f"CMS compaction completed: {result}")
                        except Exception as exc:
                            actions["cms_compact"] = {"ok": False, "error": str(exc), "timestamp": now}
                            logger.warning(f"CMS compaction failed: {exc}")
                        finally:
                            _resume_bg()
                            if self.state_aggregator:
                                self.state_aggregator.update_loop("Fast")
                        last["cms"] = now

                    # 2) Memory Consolidation (External experience grouping)
                    # Threshold-based: Trigger when unvalidated memories > 500
                    try:
                        stats = self.experience_logger.get_statistics()
                        total_convs = stats.get("total_conversations", 0)
                        validated_count = stats.get("feedback_stats", {}).get("validated_count", 0)
                        unvalidated_count = total_convs - validated_count
                        
                        consolidation_threshold = int(orch.get("memory_consolidation_threshold", 500) or 500)
                        
                        if unvalidated_count > consolidation_threshold:
                            logger.info(f"Memory Consolidation Triggered: {unvalidated_count} unvalidated memories > {consolidation_threshold}")
                            _pause_bg()
                            res = self.experience_logger.consolidate_memories(
                                similarity_threshold=0.90,
                                synthesizer_cb=self.synthesize_memory_anchor
                            )
                            actions["memory_consolidation"] = {"ok": True, "result": res}
                            last["memory_consolidation"] = now
                            _resume_bg()
                    except Exception as exc:
                        logger.warning(f"Memory consolidation failed: {exc}")

                    # 3) Confidence Decay (Daily)
                    decay_every = int(orch.get("memory_decay_every_s", 86400) or 86400) # Default 24h
                    if (now - last["memory_decay"] >= decay_every):
                        try:
                            res = self.experience_logger.apply_confidence_decay(decay_factor=0.95, max_age_days=30)
                            actions["memory_decay"] = {"ok": True, "result": res}
                        except Exception as exc:
                            logger.warning(f"Memory decay failed: {exc}")
                        last["memory_decay"] = now

                    # 4) Evals (bounded; offline-first)
                    hope_every = int(orch.get("hope_eval_every_s", 1800) or 1800)
                    if (now - last["hope_eval"] >= hope_every):
                        try:
                            _pause_bg()
                            if self.state_aggregator:
                                self.state_aggregator.update_loop("Mid")
                            # Run async method in sync loop
                            asyncio.run(self.RunHopeEval({"rlds_dir": str(Path(self.config_dir) / "rlds" / "episodes"), "use_fallback": True}))
                            actions["hope_eval"] = {"ok": True}
                        except Exception as exc:
                            actions["hope_eval"] = {"ok": False, "error": str(exc)}
                        finally:
                            _resume_bg()
                            if self.state_aggregator:
                                self.state_aggregator.update_loop("Fast")
                        last["hope_eval"] = now

                    facts_every = int(orch.get("facts_eval_every_s", 3600) or 3600)
                    if (now - last["facts_eval"] >= facts_every):
                        try:
                            _pause_bg()
                            if self.state_aggregator:
                                self.state_aggregator.update_loop("Mid")
                            asyncio.run(self.RunFactsEval({"rlds_dir": str(Path(self.config_dir) / "rlds" / "episodes"), "use_fallback": True}))
                            actions["facts_eval"] = {"ok": True}
                        except Exception as exc:
                            actions["facts_eval"] = {"ok": False, "error": str(exc)}
                        finally:
                            _resume_bg()
                            if self.state_aggregator:
                                self.state_aggregator.update_loop("Fast")
                        last["facts_eval"] = now

                    # 3) WaveCore (SSM-ish/JAX seed loops)
                    wave_every = int(orch.get("wavecore_every_s", 1800) or 1800)
                    if (not jax_tasks_ok) and (now - last["wavecore"] >= wave_every):
                        actions["wavecore"] = {"ok": False, "skipped": "jax_tasks_disabled"}
                        last["wavecore"] = now
                    elif res.level not in (ResourceLevel.CRITICAL,) and (now - last["wavecore"] >= wave_every):
                        try:
                            _pause_bg()
                            if self.state_aggregator:
                                self.state_aggregator.update_loop("Slow")
                            fast_steps = int(orch.get("wavecore_steps_fast", 60) or 60)
                            mid_steps = int(orch.get("wavecore_steps_mid", 120) or 120)
                            slow_steps = int(orch.get("wavecore_steps_slow", 180) or 180)
                            use_synth = not bool(chat.get("log_rlds", False))
                            asyncio.run(
                                self.RunWavecoreLoops(
                                    {
                                        "fast": {"arch_preset": "pi5", "max_steps": fast_steps, "batch_size": 8, "learning_rate": 1e-3, "disable_jit": True, "use_synthetic": use_synth},
                                        "mid": {"arch_preset": "pi5", "max_steps": mid_steps, "batch_size": 8, "learning_rate": 5e-4, "disable_jit": True, "use_synthetic": use_synth},
                                        "slow": {"arch_preset": "pi5", "max_steps": slow_steps, "batch_size": 8, "learning_rate": 2e-4, "disable_jit": True, "use_synthetic": use_synth},
                                        "compact_export": True,
                                        "run_hope_eval": False,
                                        "run_facts_eval": False,
                                    }
                                )
                            )
                            actions["wavecore"] = {"ok": True, "use_synthetic": use_synth}
                        except Exception as exc:
                            actions["wavecore"] = {"ok": False, "error": str(exc)}
                        finally:
                            _resume_bg()
                            if self.state_aggregator:
                                self.state_aggregator.update_loop("Fast")
                        last["wavecore"] = now

                    # 4) Tool-router refresh
                    tool_every = int(orch.get("tool_router_every_s", 3600) or 3600)
                    if (not jax_tasks_ok) and (now - last["tool_router"] >= tool_every):
                        actions["tool_router"] = {"ok": False, "skipped": "jax_tasks_disabled"}
                        last["tool_router"] = now
                    elif res.level == ResourceLevel.NORMAL and (now - last["tool_router"] >= tool_every):
                        try:
                            _pause_bg()
                            steps = int(orch.get("tool_router_steps", 200) or 200)
                            asyncio.run(self.RunToolRouterTrain({"max_steps": steps, "batch_size": 64, "learning_rate": 3e-3, "max_episodes_scan": 20000, "top_k_tools": 128, "include_dirs_prefix": "toolchat_hf"}))
                            asyncio.run(self.RunToolRouterEval({"eval_mod": 10, "eval_bucket": 0, "k": 5, "max_episodes_scan": 20000, "include_dirs_prefix": "toolchat_hf"}))
                            actions["tool_router"] = {"ok": True, "steps": steps}
                        except Exception as exc:
                            actions["tool_router"] = {"ok": False, "error": str(exc)}
                        finally:
                            _resume_bg()
                        last["tool_router"] = now

                    self._last_orchestrator = {
                        "last_run_ts": now,
                        "mode": mode,
                        "resource": res.to_dict(),
                        "last_actions": actions,
                        "last_markers": last,
                    }
            except Exception as exc:
                try:
                    self._last_orchestrator = {"last_run_ts": time.time(), "error": str(exc)}
                except Exception:
                    pass
            time.sleep(5)

    async def initialize(self):
        """Initialize hardware components with auto-detection."""
        mode_label = "REAL HARDWARE" if self.prefer_real_hardware else "MOCK"
        print(f"Initializing Brain Service ({mode_label} MODE)...")
        self._ensure_system_instructions()

        # Auto-detect hardware
        if self.auto_detect:
            print("Auto-detecting hardware...")
            detector = HardwareDetector()
            devices = detector.detect_all(auto_install=True, allow_system_install=True)
            if devices:
                self.detected_config = detector.generate_config()
                detector.print_summary()
                
                # Check for accelerator adoption
                accelerator = self.detected_config.get("primary", {}).get("ai_accelerator")
                if accelerator:
                    if self.gemma_chat is None or getattr(self.gemma_chat, 'accelerator_device', None) != accelerator:
                        print(f"üöÄ Re-initializing Chat Agent with Accelerator: {accelerator}")
                        # Try to rebuild checking for new hardware or env
                        # Note: build_chat_service will check env. If we want to FORCE accelerator, we might need args?
                        # build_chat_service calls create_gemma_chat internally if logic permits.
                        # But LiteRT logic is env based.
                        # If we are here, we likely want to refresh.
                        # If we are here, we likely want to refresh.
                        self.log_system_event(f"Re-initializing Chat Agent with Accelerator: {accelerator}")
                        # FORCE MOCK to prevent hang during independence training
                        self._build_chat_with_fallback(["mock"])
                        if self.gemma_chat and accelerator and hasattr(self.gemma_chat, 'accelerator_device'):
                            # Ensure device is set if factory didn't pick it up (though factory usually does)
                            if not self.gemma_chat.accelerator_device:
                                self.gemma_chat.accelerator_device = accelerator
            else:
                print("  No hardware detected!")

        # Load Gemma Model
        print("Loading Gemma Chat Model...")
        self.log_system_event("Loading Gemma Chat Model weights (this may take a few seconds)...")
        if self.gemma_chat is None:
            self.log_system_event("‚ö†Ô∏è No chat backend available. Using fallback ladder.")
            logger.warning("No chat backend available; trying fallback ladder (4B -> 270M -> mock).")
            self._build_chat_with_fallback()
        else:
            loaded = self.gemma_chat.load_model()
            if not loaded:
                self.log_system_event("‚ùå Gemma load failed. Starting fallback ladder.")
                logger.warning("Gemma load failed; trying fallback ladder (4B -> 270M -> mock).")
                self._build_chat_with_fallback()


        # Initialize recorder and hardware
        print("Initializing episode recorder...")
        self.recorder = ArmEpisodeRecorder(
            episodes_dir=f"{self.config_dir}/episodes",
            max_steps=500,
        )

        hardware_ready = False
        if self.prefer_real_hardware:
            print("Initializing hardware via ContinuonBrain...")
            hardware_ready = self.recorder.initialize_hardware(
                use_mock=False,
                auto_detect=self.auto_detect,
            )
            self.arm = self.recorder.arm
            self.camera = self.recorder.camera

            if not hardware_ready:
                print("  Real hardware initialization incomplete")
                # Allow server to start with partial hardware for network access
                print("  Continuing with available hardware (network access enabled)")

        if not hardware_ready:
            self.recorder.initialize_hardware(use_mock=True, auto_detect=self.auto_detect)
            self.arm = None
            self.camera = None
            self.recorder.arm = None
            self.recorder.camera = None
            self.use_real_hardware = False
        else:
            self.use_real_hardware = True

        print("Episode recorder ready")
        
        # Drivetrain
        print("Initializing drivetrain controller...")
        self.drivetrain = DrivetrainController()
        drivetrain_ready = self.drivetrain.initialize()
        if drivetrain_ready:
            print(f"Drivetrain ready ({self.drivetrain.mode.upper()} MODE)")
        else:
            print("  Drivetrain controller unavailable")

        # Mode Manager
        print("Initializing mode manager...")
        self.mode_manager = RobotModeManager(
            config_dir=self.config_dir,
            system_instructions=self.system_instructions,
        )
        # Always start in AUTONOMOUS mode for production (motion + inference + training enabled)
        print("Activating AUTONOMOUS mode (motion + inference + training enabled)")
        self.mode_manager.set_mode(
            RobotMode.AUTONOMOUS,
            metadata={
                "startup_time": time.strftime('%Y-%m-%d %H:%M:%S'),
                "auto_activated": True,
                "self_training_enabled": True
            }
        )
        print("Mode manager ready")
        
        # Initialize HOPE brain (MANDATORY with resource awareness)
        print("Initializing HOPE brain...")
        try:
            from continuonbrain.hope_impl.config import HOPEConfig
            from continuonbrain.hope_impl.brain import HOPEBrain
            from continuonbrain.hope_impl.pi5_optimizations import Pi5MemoryManager
            
            # Check available memory
            resource_status = self.resource_monitor.check_resources()
            available_mb = resource_status.available_memory_mb
            
            print(f"  Available memory: {available_mb}MB")
            
            # Select config based on available memory
            if available_mb < 3000:
                # Low memory - use Pi5 optimized config
                config = HOPEConfig.pi5_optimized()
                print("  Using Pi5-optimized config (low memory mode)")
            elif available_mb < 5000:
                # Medium memory - use development config
                config = HOPEConfig.development()
                print("  Using development config (medium memory mode)")
            else:
                # High memory - use default config
                config = HOPEConfig()
                print("  Using default config (high memory mode)")
            
            # Check if safe to allocate
            estimated_brain_mb = 2000  # Conservative estimate
            if not self.resource_monitor.is_safe_to_allocate(estimated_brain_mb):
                logger.warning(f"Memory constrained: {available_mb}MB available, need {estimated_brain_mb}MB + reserve")
                # Force Pi5 optimized config
                config = HOPEConfig.pi5_optimized()
                print("    Forcing Pi5-optimized config due to memory constraints")
            
            self.hope_brain = HOPEBrain(
                config=config,
                obs_dim=10,  # Default dimensions
                action_dim=4,
                output_dim=4,
            )
            
            # Initialize memory manager for HOPE brain
            self.brain_memory_manager = Pi5MemoryManager(max_memory_mb=self.resource_monitor.limits.max_brain_mb)
            
            # Register cleanup callback
            def cleanup_brain_memory():
                logger.info("Resource cleanup triggered for HOPE brain")
                if self.hope_brain:
                    self.hope_brain.reset()
                    self.brain_memory_manager.cleanup_if_needed(self.hope_brain)
            
            self.resource_monitor.register_cleanup_callback(ResourceLevel.CRITICAL, cleanup_brain_memory)
            
            # Register with monitoring API
            try:
                from continuonbrain.api.routes import hope_routes
                hope_routes.set_hope_brain(self.hope_brain)
                print("    Registered with web monitoring")
            except ImportError:
                print("    Web monitoring not available")
            
            # Report memory usage
            memory_usage = self.hope_brain.get_memory_usage()
            param_count = sum(p.numel() for p in self.hope_brain.parameters())
            print(f"    HOPE brain ready ({param_count:,} parameters, {memory_usage['overall_total']:.1f}MB)")
            
            # Start autonomous learning if enabled in config
            if config.enable_autonomous_learning:
                print("Starting autonomous learning loop...")
                self.background_learner = BackgroundLearner(
                    brain=self.hope_brain,
                    config={
                        "checkpoint_dir": f"{self.config_dir}/checkpoints/autonomous",
                    },
                    resource_monitor=self.resource_monitor
                )
                self.background_learner.start()
                print("    Background learner started")
            
        except ImportError as e:
            error_msg = (
                "CRITICAL: HOPE brain implementation not available!\n"
                "  This system requires HOPE brain to operate.\n"
                "  Please ensure hope_impl module is installed:\n"
                "    pip install -e .\n"
                f"  Error: {e}"
            )
            print(f"   {error_msg}")
            print("WARNING: Skipping HOPE"); self.hope_brain = None
        except Exception as e:
            error_msg = (
                f"CRITICAL: HOPE brain initialization failed: {e}\n"
                "  The system cannot operate without HOPE brain.\n"
                "  Please check:\n"
                "    - PyTorch is installed correctly\n"
                "    - hope_impl dependencies are satisfied\n"
                "    - System has sufficient memory ({resource_status.available_memory_mb}MB available)"
            )
            print(f"   {error_msg}")
            print("WARNING: Skipping HOPE"); self.hope_brain = None
        
        
        print("=" * 60)
        print(f"Brain Service Ready")
        print("=" * 60)
        
        # Start Autonomy Orchestrator
        disable_orchestrator = os.environ.get("CONTINUON_DISABLE_AUTONOMY_ORCHESTRATOR", "0").lower() in ("1", "true", "yes", "on")
        if not disable_orchestrator and (self._orchestrator_thread is None or not self._orchestrator_thread.is_alive()):
            try:
                self._orchestrator_thread = threading.Thread(target=self._autonomy_orchestrator_loop, daemon=True)
                self._orchestrator_thread.start()
                print("Autonomy orchestrator started (thread; resource-aware)")
            except Exception as exc:
                print(f"  Autonomy orchestrator failed to start: {exc}")

    def _ensure_mode_manager(self) -> RobotModeManager:
        if self.mode_manager is None:
            self.mode_manager = RobotModeManager(
                config_dir=self.config_dir,
                system_instructions=self.system_instructions,
            )
            self.mode_manager.return_to_idle()
        return self.mode_manager

    def _start_status_pulse(self):
        """Start the background status pulse thread."""
        if self._status_pulse_thread is not None:
             return
             
        def _pulse_worker():
            while not self._bg_stop_event.is_set():
                try:
                    mode = "unknown"
                    if self.mode_manager:
                        mode = self.mode_manager.current_mode.value
                        
                    payload = {
                        "status": {
                            "uptime_seconds": self.uptime_seconds,
                            "device_id": getattr(self, "agent_id", "continuon-bot"),
                            "mode": mode,
                            "ok": True
                        }
                    }
                    
                    if self.resource_monitor:
                        res = self.resource_monitor.check_resources()
                        payload["status"]["resources"] = res.to_dict()
                        if self.state_aggregator:
                            self.state_aggregator.push_metrics(res.to_dict())
                    
                    # Add Surprise metrics from HOPE
                    if self.hope_brain and self.state_aggregator:
                        try:
                            col = self.hope_brain.columns[self.hope_brain.active_column_idx]
                            novelty = getattr(col, 'last_novelty', 0.0)
                            confidence = getattr(col, 'last_confidence', 1.0)
                            payload["status"]["surprise"] = {
                                "novelty": novelty,
                                "confidence": confidence
                            }
                            self.state_aggregator.push_surprise(novelty, confidence)
                        except Exception:
                            pass
                            
                    self.chat_event_queue.put(payload)
                except Exception as e:
                    logger.error(f"Status pulse error: {e}")
                
                # Sleep for 2 seconds (or use an event wait)
                self._bg_stop_event.wait(2.0)
                
        self._status_pulse_thread = threading.Thread(target=_pulse_worker, daemon=True)
        self._status_pulse_thread.start()
        logger.info("Started status pulse thread")

    def measure_surprise(self) -> float:
        """
        Get the current system-wide surprise (prediction error).
        Combines Visual Surprise (VQ-VAE) and World Model Surprise (HOPE).
        """
        visual_surprise = 0.0
        model_surprise = 0.0
        
        # 1. Visual Surprise
        if self.vision_core:
            try:
                visual_surprise = self.vision_core.compute_visual_surprise() * 10.0 # Scale up small MSE
            except Exception:
                pass
        
        # 2. World Model Surprise (HOPE)
        if self.hope_brain:
             try:
                col = self.hope_brain.columns[self.hope_brain.active_column_idx]
                model_surprise = getattr(col, 'last_novelty', 0.0)
             except Exception:
                pass
        
        return max(visual_surprise, model_surprise)

    def _now_iso(self) -> str:
        return datetime.datetime.now().isoformat()
        
    @property
    def capabilities(self) -> Dict[str, bool]:
        return {
            "has_vision": self.camera is not None,
            "has_manipulator": self.arm is not None,
            "has_mobile_base": self.drivetrain is not None and self.drivetrain.initialized,
            "has_audio": bool(self.recorder and self.recorder.audio_enabled),
        }

    def _build_task_eligibility(self, task: TaskDefinition) -> TaskEligibility:
        mode_manager = self._ensure_mode_manager()
        gates = mode_manager.get_gate_snapshot() if mode_manager else {}
        markers: List[TaskEligibilityMarker] = []

        if task.requires_motion and not gates.get("allow_motion"):
            markers.append(TaskEligibilityMarker(
                code="MOTION_GATE", label="Motion is gated", severity="blocking", blocking=True,
                remediation="Enable motion gate from the command deck"
            ))

        if task.requires_recording and not gates.get("record_episodes"):
            markers.append(TaskEligibilityMarker(
                code="RECORDING_GATE", label="Recording disabled", severity="warning", blocking=True,
                remediation="Enable recording"
            ))

        safety_head = self.health_checker.get_safety_head_status() if self.health_checker else {}
        if safety_head and safety_head.get("status") not in {None, "ok", "ready"}:
            markers.append(TaskEligibilityMarker(
                code="SAFETY_HEAD", label="Safety head degraded", severity="warning", blocking=False,
                source="safety_head", remediation="Reset safety head"
            ))

        caps = self.capabilities
        for modality in task.required_modalities:
            if modality == "vision" and not caps["has_vision"]:
                markers.append(TaskEligibilityMarker(code="MISSING_VISION", label="Vision required", severity="error", blocking=True))
            elif (modality == "arm" or modality == "gripper") and not caps["has_manipulator"]:
                markers.append(TaskEligibilityMarker(code="MISSING_ARM", label="Manipulator required", severity="error", blocking=True))
            elif modality == "mobile_base" and not caps["has_mobile_base"]:
                markers.append(TaskEligibilityMarker(code="MISSING_BASE", label="Mobile base required", severity="error", blocking=True))

        eligible = not any(marker.blocking for marker in markers)
        return TaskEligibility(eligible=eligible, markers=markers, next_poll_after_ms=120.0 if task.requires_motion else 400.0)

    def _serialize_task_entry(self, task: TaskDefinition) -> TaskLibraryEntry:
        eligibility = self._build_task_eligibility(task)
        return TaskLibraryEntry(
            id=task.id, title=task.title, description=task.description, group=task.group, tags=task.tags,
            eligibility=eligibility, estimated_duration=task.estimated_duration, recommended_mode=task.recommended_mode
        )

    def GetTaskSummary(self, task_id: str) -> Optional[TaskSummary]:
        task = self.task_library.get_entry(task_id)
        if not task:
            return None
        entry = self._serialize_task_entry(task)
        return TaskSummary(
            entry=entry, required_modalities=task.required_modalities, steps=task.steps,
            owner="robot", updated_at=self._now_iso(), telemetry_topic=task.telemetry_topic
        )
    
    def get_task_library(self) -> List[Dict]:
        """Get all tasks with eligibility checks for the UI."""
        tasks = []
        for task_def in self.task_library.list_entries():
            entry = self._serialize_task_entry(task_def)
            tasks.append({
                "id": task_def.id,
                "title": task_def.title,
                "description": task_def.description,
                "group": task_def.group,
                "tags": task_def.tags,
                "icon": self._get_task_icon(task_def.group),
                "estimated_duration": task_def.estimated_duration,
                "eligibility": entry.eligibility.to_dict(),
                "required_modalities": task_def.required_modalities,
                "steps": task_def.steps
            })
        return tasks
    
    def _get_task_icon(self, group: str) -> str:
        """Get emoji icon for task group."""
        icons = {
            "Safety": "üîç",
            "Demo": "ü¶æ",
            "System": "‚öôÔ∏è",
            "Training": "üìπ"
        }
        return icons.get(group, "üìã")

    async def RunManualTraining(self, payload: Optional[dict] = None) -> dict:
        """Run manual JAX trainer with optional overrides from payload."""
        
        # Check if we are on LiteRT (training not supported)
        if getattr(self.gemma_chat, "is_litert_backend", False):
            if payload and payload.get("force_mock_train", False):
                # Allow a mock verification step if requested
                pass
            else:
                return {
                    "status": "error",
                    "message": "Manual Training is not supported on LiteRT backend (Inference Only). Use a JAX/Torch backend or 'force_mock_train' for testing."
                }

        # Lazy import: JAX stack can be absent/broken on embedded targets.
        try:
            from continuonbrain.services.manual_trainer import ManualTrainer, ManualTrainerRequest  # noqa: WPS433
        except Exception as exc:  # noqa: BLE001
            return {
                "status": "error",
                "error": "jax_unavailable",
                "message": f"Manual JAX training is unavailable on this device ({exc}).",
                "hint": "Install/repair JAX + tensorstore wheels, or run training elsewhere.",
            }

        payload = payload or {}
        if self.manual_trainer is None:
            self.manual_trainer = ManualTrainer()

        request = ManualTrainerRequest(
            rlds_dir=Path(payload["rlds_dir"]) if payload.get("rlds_dir") else None,
            use_synthetic=bool(payload.get("use_synthetic", False)),
            max_steps=int(payload.get("max_steps", 10)),
            batch_size=int(payload.get("batch_size", 4)),
            learning_rate=float(payload.get("learning_rate", 1e-3)),
            obs_dim=int(payload.get("obs_dim", 128)),
            action_dim=int(payload.get("action_dim", 32)),
            output_dim=int(payload.get("output_dim", 32)),
            disable_jit=bool(payload.get("disable_jit", True)),
            metrics_path=Path(payload["metrics_path"]) if payload.get("metrics_path") else None,
        )
        return await self.manual_trainer.run(request)
    
    async def Drive(self, steering: float, throttle: float) -> dict:
        """Apply drivetrain command with safety checks."""
        # SAFETY CHECK: Validate drive command against safety protocol
        action_description = f"Drive command: steering={steering:.2f}, throttle={throttle:.2f}"
        is_safe, safety_reason = self._check_safety_protocol(
            action_description,
            {
                "steering": steering,
                "throttle": throttle,
                "requires_motion": True,
                "session_id": "robot_control",
            },
            decision_id=action_description,
        )
        
        if not is_safe:
            return {
                "success": False, 
                "message": f"Safety protocol violation: {safety_reason}"
            }
        
        # Mode check
        if not self.mode_manager:
            self._ensure_mode_manager()
        
        current_mode = self.mode_manager.current_mode
        if current_mode not in {RobotMode.MANUAL_CONTROL, RobotMode.MANUAL_TRAINING}:
            return {"success": False, "message": "Driving only allowed in manual modes"}
            
        # ROUTE THROUGH SAFETY KERNEL (Ring 0)
        res = self.safety_client.send_command("drive", {"steering": steering, "throttle": throttle})
        if res.get("status") != "ok":
            return {"success": False, "message": f"Safety Kernel Blocked: {res.get('reason')}"}
        
        # Extract potentially clipped args
        safe_args = res.get("args", {})
        safe_steering = safe_args.get("steering", steering)
        safe_throttle = safe_args.get("throttle", throttle)

        if self.drivetrain:
            self.drivetrain.apply_drive(safe_steering, safe_throttle)
            return {"success": True, "message": "OK", "clipping": res.get("safety_level") == 1}
        return {"success": False, "message": "No drivetrain"}

    def _prime_world_model_adapter(self) -> None:
        """Initialize the world model adapter when JAX is available in the runtime context."""
        if self.jax_adapter is not None:
            return
        try:
            from continuonbrain.services.runtime_context import get_runtime_context_manager

            runtime_mgr = get_runtime_context_manager(self.config_dir)
            context = runtime_mgr.get_context()
            hardware = getattr(context, "hardware", None)
            wm_caps = getattr(hardware, "world_model", None) if hardware else None
            if not wm_caps or not wm_caps.jax_available:
                return
            if self.gemma_chat and getattr(self.gemma_chat, "model_name", "").lower().startswith("mock"):
                logger.info("Skipping JAX world model init: Gemma chat backend is in mock mode.")
                return

            self._init_jax_search()
            if getattr(self, "jax_adapter", None):
                mark_ready = getattr(runtime_mgr, "mark_world_model_ready", None)
                if callable(mark_ready):
                    try:
                        mark_ready(world_model_type=wm_caps.world_model_type, can_plan=wm_caps.can_plan)
                    except Exception as exc:  # noqa: BLE001
                        logger.debug(f"Failed to publish world model readiness to runtime context: {exc}")
        except Exception as exc:  # noqa: BLE001
            logger.debug(f"World model auto-init skipped: {exc}")

    def _init_jax_search(self):
        """Initialize JAX model adapter for search if JAX is available/selected."""
        if self.jax_adapter is not None:
            return
        if self.gemma_chat and getattr(self.gemma_chat, "model_name", "").lower().startswith("mock"):
            logger.info("Skipping JAX world model init: mock chat backend active.")
            return

        try:
            import jax  # noqa: WPS433
        except Exception as exc:  # noqa: BLE001
            logger.info(f"[BrainService] Skipping JAX search adapter init; JAX unavailable ({exc}).")
            return

        try:
            # Lazy import to avoid hard dependency if not used
            from continuonbrain.jax_models.core_model import make_core_model, CoreModelConfig
            from continuonbrain.reasoning.jax_adapter import JaxWorldModelAdapter
            
            # Check for cached/shared params? For now create fresh for search (inefficient but safe)
            rng = jax.random.PRNGKey(0)
            model, params = make_core_model(rng, obs_dim=128, action_dim=32, output_dim=32)
            self.jax_adapter = JaxWorldModelAdapter(model, params, CoreModelConfig.pi5_optimized())
            try:
                from continuonbrain.services.runtime_context import get_runtime_context_manager

                runtime_mgr = get_runtime_context_manager(self.config_dir)
                wm_type = None
                try:
                    wm_type = runtime_mgr.get_context().hardware.world_model.world_model_type
                except Exception:
                    wm_type = None
                runtime_mgr.mark_world_model_ready(world_model_type=wm_type or "jax_core", can_plan=True)
            except Exception as exc:  # noqa: BLE001
                logger.debug(f"Failed to mark runtime context world model readiness: {exc}")
            logger.info("[BrainService] JAX World Model Adapter initialized for Search.")
        except Exception as e:
            logger.warning(f"[BrainService] Failed to init JAX search adapter: {e}")

    async def RunSymbolicSearch(self, payload: dict) -> dict:
        """Run Symbolic/Tree search using the JAX World Model."""
        from continuonbrain.reasoning.tree_search import symbolic_search, ArmGoal, WorldModelState
        
        if not self.jax_adapter:
            self._init_jax_search()
            
        if not self.jax_adapter:
            return {"status": "error", "message": "JAX World Model not initialized (use JAX backend)"}
            
        try:
            # Parse Goal and State from payload or current robot state
            start_joints = payload.get("start_joints", [0.0] * 6)
            target_joints = payload.get("target_joints", [0.5] * 6)
            
            c_state = WorldModelState(joint_pos=start_joints)
            goal = ArmGoal(target_joint_pos=target_joints)
            
            steps = payload.get("depth", 5)
            
            # Run Search (blocking in main thread for now, should be threaded for heavy loads)
            best_action = symbolic_search(c_state, goal, self.jax_adapter, steps=steps)
            
            if best_action:
                return {
                    "status": "success", 
                    "plan_found": True, 
                    "next_action": best_action,
                    "metrics": {
                        "steps": steps,
                        "plan_score": 0.95,
                        "imagination_depth": steps
                    }
                }
            else:
                 return {
                    "status": "success", 
                    "plan_found": False, 
                    "metrics": {"plan_score": 0.0}
                }
                
        except Exception as e:
            return {"status": "error", "message": str(e)}

    async def RunChatLearn(self, payload: Optional[dict] = None) -> dict:
        """Run a bounded multi-turn learning conversation using Gemma 3n and log via chat->RLDS when enabled."""
        payload = payload or {}
        turns = int(payload.get("turns", 10) or 10)
        turns = max(1, min(turns, 50))
        session_id = str(payload.get("session_id") or f"chat_learn_{int(time.time())}")
        model_hint = str(payload.get("model_hint") or "hope-v1")
        delegate_model_hint = payload.get("delegate_model_hint")
        topic = str(payload.get("topic") or "tool use + planning + safety")

        message = (
            "We are training the HOPE Agent Manager through multi-agent conversations.\n"
            "You are the Agent Manager (primary orchestrator) with CURIOSITY about the system.\n"
            "\n"
            "--- SYSTEM ARCHITECTURE CONTEXT ---\n"
            "HOPE operates on a 'One Brain, Many Shells' architecture using CMS (Contextual Memory System).\n"
            "1) Fast Loop (ms-100ms): Reactive control, 'Particle' path (MLPs/Convs).\n"
            "2) Mid Loop (1-10s): Tactical planning, skill sequencing.\n"
            "3) Slow Loop (min-hours): Strategic reasoning, 'Wave' path (Mamba SSM, Spectral mixers).\n"
            "CMS consolidates Episodic memory into Parametric memory through Sleep/Compaction cycles.\n"
            "------------------------------------\n"
            "\n"
            "CURIOSITY DIRECTIVE: Be curious about how the system works, what it can learn, and how to improve it.\n"
            "\n"
            "For each turn:\n"
            "1) As the Agent Manager, be CURIOUS about the system:\n"
            "   - What aspects of HOPE's architecture are most interesting or mysterious?\n"
            "   - How does the Mamba SSM (Wave path) transition episodic traces to parametric weights?\n"
            "   - What learning capabilities could be enhanced by optimizing CMS compaction?\n"
            "   - How does the system actually work internally (Fast/Mid/Slow loops)?\n"
            "   - How can we make HOPE more helpful through better architectural understanding?\n"
            "\n"
            "2) Formulate CURIOUS questions that explore:\n"
            "   - System internals: How does CMS compaction actually work? What triggers it?\n"
            "   - Learning mechanisms: How does WaveCore fast/mid/slow differ? What do they learn?\n"
            "   - Memory: How is the 'Wave' state maintained in linear-time SSMs?\n"
            "   - Training data: What patterns exist in RLDS episodes? What's missing?\n"
            "   - Self-Improvement: How can the brain optimize its own inference stack?\n"
            "\n"
            "3) Consult the subagent (Gemma 3n) with your curious questions about system internals.\n"
            "\n"
            "4) As Agent Manager, synthesize the subagent's insights and decide:\n"
            "   - What did we learn about how the system works?\n"
            "   - How can HOPE learn from this understanding?\n"
            "   - What concrete architectural improvements would this enable?\n"
            "\n"
            "5) Be CURIOUS about learning itself:\n"
            "   - How does continuous learning actually happen in the 'One Brain' model?\n"
            "   - What makes some learning episodes more valuable than others?\n"
            "   - How can we make the system more curious and exploratory?\n"
            "\n"
            "Example curious conversation flow:\n"
            "- Agent Manager: 'I'm curious: How does the Mamba SSM layer consolidate episodic knowledge? "
            "How can we optimize the energy transfer between Particle and Wave paths?'\n"
            "- Subagent: 'The Mamba SSM uses selective state space updates to maintain long-range context...'\n"
            "- Agent Manager: 'Excellent insight! This suggests we can tune the selectivity to preserve "
            "critical safety traces while compacting routine motor data...'\n"
            "\n"
            f"Topic focus: {topic}.\n"
            "\n--- CURRENT SCENE AWARENESS ---\n"
            f"Robot's current view: {self.vision_core.describe_scene() if self.vision_core else 'No vision'}\n"
            "--------------------------------\n"
            "\n"
            "Be GENUINELY CURIOUS about the system. Ask questions that explore how things work, "
            "what could be better, and how learning actually happens.\n"
            "End each turn with the required structured JSON line.\n"
        )
        history: list = []
        outputs = []
        
        log_enabled = False
        try:
            from continuonbrain.settings_manager import SettingsStore
            settings = SettingsStore(Path(self.config_dir)).load()
            log_enabled = bool((settings.get("chat", {}) or {}).get("log_rlds", False))
        except Exception:
            log_enabled = False
        
        if os.environ.get("CONTINUON_LOG_CHAT_RLDS", "0").lower() in ("1", "true", "yes", "on"):
            log_enabled = True
        
        rlds_cfg = None
        log_chat_turn = None
        if log_enabled:
            from continuonbrain.rlds.chat_rlds_logger import ChatRldsLogConfig, log_chat_turn as _log_chat_turn
            log_chat_turn = _log_chat_turn
            rlds_cfg = ChatRldsLogConfig(
                episodes_dir=Path(self.config_dir) / "rlds" / "episodes",
                group_by_session=True,
            )
        
        for i in range(turns):
            # HITL Teacher Mode Intervention
            if self.teacher_mode_active and i < turns - 1 and (i % 2 == 0):
                # We need the previous output (Agent Manager Question) to pause on.
                # But here 'message' is the INPUT to the model.
                # The model *generates* the question in the *response*.
                # So we catch it AFTER execution in the PREVIOUS loop?
                # Actually, the logic in RobotService was inside the loop, using `assistant_text` from the *current* turn
                # to trigger intervention for the *next* logical step (which is synthesize).
                pass 

            # Delegate to Gemini if requested for Subagent (odd turns)
            is_subagent_turn = (i % 2 != 0)
            use_gemini = is_subagent_turn and delegate_model_hint and "gemini" in delegate_model_hint.lower()

            print(f"[RunChatLearn] Turn {i}: subagent={is_subagent_turn}, flag={use_gemini}")

            if use_gemini:
                # Extract the question from the Agent Manager's previous turn
                prev_turn_text = ""
                if outputs:
                    prev_out = outputs[-1]
                    prev_turn_text = prev_out.get("response", "") if isinstance(prev_out, dict) else str(prev_out)
                
                # If valid question found, query Gemini
                if prev_turn_text:
                    repo_root = Path(__file__).resolve().parent.parent.parent
                    gemini_script = repo_root / "scripts" / "gemini"
                    print(f"[RunChatLearn] Delegating turn {i} to Gemini CLI...")
                    print(f"  Script: {gemini_script}")
                    print(f"  Prompt: {prev_turn_text[:50]}...")
                    print(f"  Env Key Present: {'GOOGLE_API_KEY' in os.environ}")
                    
                    try:
                        # Call Gemini CLI synchronously (blocking but typically fast enough for this loop)
                        proc = subprocess.run(
                            [str(gemini_script), prev_turn_text],
                            capture_output=True,
                            text=True,
                            timeout=45,
                            env={**os.environ}
                        )
                        if proc.returncode == 0:
                            gemini_response = proc.stdout.strip()
                            print(f"  [Gemini Response]: {gemini_response[:50]}...")
                            resp = {"response": gemini_response, "model": "gemini-cli"}
                        else:
                            print(f"[RunChatLearn] Gemini CLI failed return code {proc.returncode}")
                            print(f"  Stderr: {proc.stderr}")
                            resp = {"response": f"Error consulting Gemini: {proc.stderr}", "model": "error"}
                    except Exception as e:
                        print(f"[RunChatLearn] Gemini delegation error: {e}")
                        resp = {"response": f"System Error consulting Gemini: {e}", "model": "error"}
                else:
                    print("  [RunChatLearn] No previous question found.")
                    resp = {"response": "I didn't hear a question.", "model": "gemini-cli"}
            else:
                # Standard Local Execution
                print(f"[RunChatLearn] Local execution for turn {i}")
                resp = self.ChatWithGemma(
                    message,
                    history,
                    session_id=session_id
                )
            
            assistant_text = resp.get("response", "") if isinstance(resp, dict) else str(resp)

            # --- CURIOSITY DRIVER (Mock Override) ---
            # If we are in "learning mode" (implied by RunChatLearn) and the agent gives a generic mock response,
            # we MUST inject a *real* curious question to drive the Gemini subagent to give useful answers.
            # Otherwise, "I'm a mock agent" -> Gemini: "Okay." -> No learning.
            if i % 2 == 0 and ("mock" in assistant_text.lower() or len(assistant_text) < 20):
                import random
                curiosity_questions = [
                    "I am curious: How does the Compact Memory System (CMS) decide which memories to keep and which to discard?",
                    "I want to understand: What is the specific data format for RLDS episodes, and how do we ensure schema validation?",
                    "I am researching: How does the Tool Router map natural language to specific tool arguments using JAX?",
                    "I am investigating: What are the safety protocols for arm manipulation, and how do we override them in emergencies?",
                    "I am curious: How does the WaveCore 'slow loop' update the long-term weights from the 'mid loop' adapters?",
                    "I want to know: What is the difference between 'humand' and 'HOPE reading' in the seed model training plan?",
                    "I am exploring: How can I use the 'ASK_GEMINI' tool more effectively to fill gaps in my knowledge?",
                    "I am curious: What metrics does the 'background learner' use to determine if a training step was successful?",
                ]
                
                # Pick one deterministically based on turn to avoid repeats in short sessions
                q_idx = (i // 2) % len(curiosity_questions)
                forced_question = curiosity_questions[q_idx]
                
                print(f"[RunChatLearn] ‚ö†Ô∏è  Mock response detected. INJECTING CURIOSITY DRIVER question: {forced_question}")
                assistant_text = forced_question
                resp["response"] = assistant_text
                resp["injected_curiosity"] = True

            # Teacher Input Capture Logic
            if self.teacher_mode_active and i < turns - 1 and (i % 2 == 0):
                self.teacher_pending_question = assistant_text
                self.teacher_response_event.clear()
                self.teacher_response_text = None
                
                print(f"Teacher Mode: Pausing for user input. Question: {assistant_text[:100]}...")
                try:
                    await asyncio.wait_for(self.teacher_response_event.wait(), timeout=300.0)
                except asyncio.TimeoutError:
                    print("Teacher Mode: Timeout. Proceeding.")
                    self.teacher_pending_question = None
                
                if self.teacher_response_text:
                    teacher_msg = f"Subagent (User/Teacher) Response: {self.teacher_response_text}"
                    history.append({"role": "user", "content": teacher_msg})

            outputs.append(resp)
            
            # Push to event queue for UI
            try:
                self.chat_event_queue.put_nowait({
                    "type": "chat_turn",
                    "role": "agent_manager" if i % 2 == 0 else "subagent",
                    "message": assistant_text,
                    "turn_index": i,
                    "session_id": session_id
                })
            except queue.Full:
                pass 
            
            is_fallback = (assistant_text.startswith("[model=") or "Status snapshot" in assistant_text)
            
            if log_chat_turn and not is_fallback:
                try:
                    log_chat_turn(
                        rlds_cfg,
                        user_message=message,
                        assistant_response=assistant_text,
                        session_id=session_id,
                        metadata={
                            "turn_index": i,
                            "role": "agent_manager" if i % 2 == 0 else "subagent"
                        }
                    )
                except Exception as e:
                    print(f"Failed to log chat turn {i}: {e}")

            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": assistant_text})
            
            if i < turns - 1:
                # Role switching logic
                if i % 2 == 0:
                    message = (
                        "You are the internal Subagent (Gemma 3n) with deep knowledge of system internals.\n"
                        "Answer the Agent Manager's question effectively.\n"
                        "Provide specific technical details about CMS, WaveCore, ToolRouter, or RLDS.\n"
                        "End with the JSON line."
                    )
                else:
                    message = (
                        "You are the Agent Manager.\n"
                        "Synthesize the insights from the subagent.\n"
                        "Decide how to improve the system based on this update.\n"
                        "End with the JSON line."
                    )
                    
            if self.teacher_mode_active and self.teacher_response_text and i % 2 == 0:
                message = (
                    f"The Subagent (Teacher) provided this specific advice/answer:\n"
                    f"\"{self.teacher_response_text}\"\n\n"
                    f"{message}"
                )
                self.teacher_response_text = None
                self.teacher_pending_question = None

        return {
            "status": "ok",
            "session_id": session_id,
            "turns": turns,
            "results": outputs[-3:],
            "history": history,
        }

    async def RunHopeEval(self, payload: Optional[dict] = None) -> dict:
        """Run graded HOPE Q&A, log RLDS episode, with fallback LLM ordering."""
        payload = payload or {}
        # Resolve REPO_ROOT relative to this file
        repo_root = Path(__file__).resolve().parent.parent.parent
        
        questions_path = Path(payload.get("questions_path") or (repo_root / "continuonbrain" / "eval" / "hope_eval_questions.json"))
        rlds_dir = Path(payload.get("rlds_dir") or (Path(self.config_dir) / "rlds" / "episodes"))
        use_fallback = bool(payload.get("use_fallback", True))
        fallback_order = payload.get("fallback_order") or ["google/gemma-370m", "google/gemma-3n-2b"]
        
        try:
            from continuonbrain.eval.hope_eval_runner import run_hope_eval_and_log
            return await run_hope_eval_and_log(
                service=self,
                questions_path=questions_path,
                rlds_dir=rlds_dir,
                use_fallback=use_fallback,
                fallback_order=fallback_order,
            )
        except ImportError:
            return {"status": "error", "message": "eval runner not available"}

    async def RunFactsEval(self, payload: Optional[dict] = None) -> dict:
        """Run FACTS-lite eval and log RLDS episode."""
        payload = payload or {}
        repo_root = Path(__file__).resolve().parent.parent.parent
        questions_path = Path(payload.get("questions_path") or (repo_root / "continuonbrain" / "eval" / "facts_eval_questions.json"))
        rlds_dir = Path(payload.get("rlds_dir") or (Path(self.config_dir) / "rlds" / "episodes"))
        use_fallback = bool(payload.get("use_fallback", True))
        fallback_order = payload.get("fallback_order") or ["google/gemma-370m", "google/gemma-3n-2b"]
        
        try:
            from continuonbrain.eval.hope_eval_runner import run_hope_eval_and_log
            return await run_hope_eval_and_log(
                service=self,
                questions_path=questions_path,
                rlds_dir=rlds_dir,
                use_fallback=use_fallback,
                fallback_order=fallback_order,
                episode_prefix="facts_eval",
                model_label="facts-lite",
            )
        except ImportError:
            return {"status": "error", "message": "eval runner not available"}

    alias_chat_learn = RunChatLearn

    async def RunWavecoreLoops(self, payload: Optional[dict] = None) -> dict:
        """Run WaveCore fast/mid/slow loops using the JAX CoreModel seed."""
        try:
            from continuonbrain.services.wavecore_trainer import WavecoreTrainer
        except Exception as exc:
            return {"status": "error", "message": f"WaveCore loops require JAX: {exc}"}

        payload = payload or {}
        if self.wavecore_trainer is None:
            self.wavecore_trainer = WavecoreTrainer()
        payload.setdefault("service", self)
        return await self.wavecore_trainer.run_loops(payload)

    async def RunToolRouterTrain(self, payload: Optional[dict] = None) -> dict:
        try:
            from continuonbrain.services.tool_router_trainer import ToolRouterTrainer, ToolRouterTrainRequest
        except Exception as exc:
             return {"status": "error", "message": f"Tool-router training requires JAX: {exc}"}

        payload = payload or {}
        if self.tool_router_trainer is None:
            self.tool_router_trainer = ToolRouterTrainer()
        req = ToolRouterTrainRequest(
            episodes_root=Path(payload["episodes_root"]) if payload.get("episodes_root") else None,
            include_dirs_prefix=str(payload.get("include_dirs_prefix", "toolchat_hf")),
            max_episodes_scan=int(payload.get("max_episodes_scan", 20000)),
            batch_size=int(payload.get("batch_size", 64)),
            max_steps=int(payload.get("max_steps", 600)),
        )
        return await self.tool_router_trainer.run(req)

    async def ToolRouterPredict(self, payload: Optional[dict] = None) -> dict:
        try:
            from continuonbrain.jax_models.infer.tool_router_infer import load_tool_router_bundle, predict_topk
        except Exception as exc:
            return {"status": "error", "message": f"Tool-router inference requires JAX: {exc}"}

        payload = payload or {}
        prompt = str(payload.get("prompt") or "")
        k = int(payload.get("k") or 5)
        export_dir = Path(payload.get("export_dir") or "/opt/continuonos/brain/model/adapters/candidate/tool_router_seed")

        if self._tool_router_bundle is None or getattr(self._tool_router_bundle, "manifest_path", None) != (export_dir / "tool_router_manifest.json"):
            self._tool_router_bundle = load_tool_router_bundle(export_dir)
        preds = predict_topk(self._tool_router_bundle, prompt, k=k)
        return {"status": "ok", "predictions": preds}

    async def RunToolRouterEval(self, payload: Optional[dict] = None) -> dict:
        try:
            from continuonbrain.services.tool_router_evaluator import ToolRouterEvaluator, ToolRouterEvalRequest
        except Exception as exc:
             return {"status": "error", "message": f"Tool-router eval requires JAX: {exc}"}

        payload = payload or {}
        if self.tool_router_evaluator is None:
            self.tool_router_evaluator = ToolRouterEvaluator()
        req = ToolRouterEvalRequest(
            episodes_root=Path(payload["episodes_root"]) if payload.get("episodes_root") else None,
            max_episodes_scan=int(payload.get("max_episodes_scan", 30000)),
        )
        return await self.tool_router_evaluator.run(req)

    async def SpeakText(self, payload: Optional[dict] = None) -> dict:
        payload = payload or {}
        text = str(payload.get("text", "") or payload.get("message", ""))
        return speak_text(text, voice=payload.get("voice", "en"), rate_wpm=payload.get("rate_wpm", 175))

    async def RecordMicrophone(self, payload: Optional[dict] = None) -> dict:
        payload = payload or {}
        res, status = record_wav(
            seconds=payload.get("seconds", 4), 
            sample_rate_hz=payload.get("sample_rate_hz", 16000),
            num_channels=payload.get("num_channels", 1),
            device=payload.get("device")
        )
        if not res: return status
        return {"status": "ok", "path": str(res.path)}

    async def ListAudioDevices(self) -> dict:
        """List ALSA capture devices (best-effort)."""
        import subprocess
        out = {"status": "ok", "arecord_l": "", "arecord_L": ""}
        try:
            proc = subprocess.run(["arecord", "-l"], capture_output=True, text=True, timeout=3)
            out["arecord_l"] = (proc.stdout or proc.stderr or "").strip()
        except Exception: pass
        try:
            proc = subprocess.run(["arecord", "-L"], capture_output=True, text=True, timeout=3)
            out["arecord_L"] = (proc.stdout or proc.stderr or "").strip()
        except Exception: pass
        return out

    async def StartPairing(self, payload: Optional[dict] = None) -> dict:
        payload = payload or {}
        session = self.pairing.start(base_url=str(payload.get("base_url") or ""), ttl_s=int(payload.get("ttl_s") or 300))
        return {
            "status": "ok",
            "token": session.token,
            "confirm_code": session.confirm_code,
            "url": session.url,
        }

    async def ConfirmPairing(self, payload: Optional[dict] = None) -> dict:
        payload = payload or {}
        ok, msg, ownership = self.pairing.confirm(
            token=str(payload.get("token") or ""),
            confirm_code=str(payload.get("confirm_code") or payload.get("code") or ""),
            owner_id=str(payload.get("owner_id") or payload.get("owner") or ""),
        )
        if not ok: return {"status": "error", "message": msg}
        return {"status": "ok", "ownership": ownership}

    async def GetOwnershipStatus(self) -> dict:
        ownership = self.pairing.ownership_status()
        flat = {
            "owned": bool(ownership.get("owned", False)),
            "owner_id": ownership.get("owner_id"),
            "account_type": ownership.get("account_type"),
        }
        return {"status": "ok", "ownership": ownership, **flat}

    async def GetArchitectureStatus(self) -> dict:
        """Report which learning subsystems are active so we can verify 'whole architecture' participation."""
        # Get Mode
        mode = "unknown"
        if self.mode_manager:
            mode = self.mode_manager.current_mode.value
            
        res = None
        try:
            res = self.resource_monitor.check_resources().to_dict()
        except Exception: pass
        
        # Threads status
        thread_meta = {
            "chat_learn_thread": {"present": bool(self._chat_learn_thread and self._chat_learn_thread.is_alive())},
            "autonomous_learner_thread": {"present": bool(self._autonomous_learner_thread and self._autonomous_learner_thread.is_alive())},
        }
        
        # Hailo state
        hailo_state = None
        # We don't have chat_adapter here, but we can check gemma_chat accelerator
        if hasattr(self.gemma_chat, 'accelerator_device'):
             hailo_state = {"accelerator": self.gemma_chat.accelerator_device}

        # Settings
        from continuonbrain.settings_manager import SettingsStore
        settings = SettingsStore(Path(self.config_dir)).load()
        chat_settings = (settings or {}).get("chat") or {}
        
        return {
            "status": "ok",
            "mode": mode,
            "recording": hasattr(self, "recorder") and getattr(self, "is_recording", False), # BrainService doesn't track is_recording? 
            # BrainService has 'recorder' but no 'is_recording' flag in init? 
            # Wait, RobotService had self.is_recording. BrainService might need it.
            # For now assume False or check recorder state.
            "chat_rlds_enabled": bool(chat_settings.get("log_rlds", False)),
            "hope_brain_loaded": bool(self.hope_brain is not None),
            # "background_learner": ... check if we ported background learner state
            "tasks": thread_meta,
            "resources": res,
            "hailo": {"vision": hailo_state},
        }

    def shutdown(self):
        print("Shutting down Brain Service...")
        if self.arm:
            self.arm.shutdown()
        if self.camera:
            self.camera.close()
