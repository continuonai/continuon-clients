# JAX Models: WaveCore Seed Implementation

**Status:** Production Ready (v3.0.0)  
**Framework:** JAX/Flax  
**Parameters:** 3,408,521 (3.4M)  
**Inference:** 231 steps/sec (4.3ms)

This directory contains the JAX/Flax implementation of the WaveCore seed model.

## Quick Stats

| Metric | Value |
|--------|-------|
| Parameters | 3.4M |
| Memory | 14 MB |
| Embedding | EmbeddingGemma-300m (768-dim) |
| Inference | 231 steps/sec |
| Loss | 0.011 |

## Scaling Tiers

| Version | d_s | d_w | Params | Status |
|---------|-----|-----|--------|--------|
| v2.0 | 128 | 128 | 1M | âœ… Released |
| **v3.0** | **256** | **256** | **3.4M** | **âœ… Current** |
| v4.0 | 512 | 512 | 25M | ðŸ”¶ Next |

See `scaling_configs.py` for tier definitions.

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           WAVECORE SEED MODEL                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  INPUT ENCODER                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ x_obs â”€â”€â–º Dense â”€â”€â–º LayerNorm â”€â”€â–º ReLU â”€â”€â–º Dense â”€â”€â–º e_obs              â”‚â”‚
â”‚  â”‚ a_prev â”€â–º Dense â”€â”€â–º e_action                                            â”‚â”‚
â”‚  â”‚ r_t â”€â”€â”€â”€â–º Dense â”€â”€â–º e_reward                                            â”‚â”‚
â”‚  â”‚                                                                          â”‚â”‚
â”‚  â”‚ e_t = Concat(e_obs, e_action, e_reward) âˆˆ â„^{d_e}                       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                   â”‚                                         â”‚
â”‚                                   â–¼                                         â”‚
â”‚  CMS READ                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Query: q_t = W_q Â· [s_{t-1}; e_t]                                       â”‚â”‚
â”‚  â”‚                                                                          â”‚â”‚
â”‚  â”‚ Per-level attention:                                                     â”‚â”‚
â”‚  â”‚   Î±^(â„“) = softmax(K^(â„“) Â· q_t / âˆšd_k)                                   â”‚â”‚
â”‚  â”‚   c^(â„“) = Î±^(â„“) Â· M^(â„“)                                                 â”‚â”‚
â”‚  â”‚                                                                          â”‚â”‚
â”‚  â”‚ Hierarchical mixing:                                                     â”‚â”‚
â”‚  â”‚   c_t = Î£_â„“ Î²_â„“ Â· c^(â„“)                                                 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                   â”‚                                         â”‚
â”‚                                   â–¼                                         â”‚
â”‚  HOPE CORE (Fast/Wave/Particle)                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                                                                          â”‚â”‚
â”‚  â”‚  FAST STATE (Reactive)           WAVE STATE (SSM)                       â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚â”‚
â”‚  â”‚  â”‚ s_t = Ïƒ(W_s Â· [s_{t-1}; â”‚    â”‚ Mamba-like SSM:                  â”‚    â”‚â”‚
â”‚  â”‚  â”‚         e_t; c_t])      â”‚    â”‚ Î” = softplus(Linear(e_t))        â”‚    â”‚â”‚
â”‚  â”‚  â”‚                         â”‚    â”‚ B = Linear(e_t)                  â”‚    â”‚â”‚
â”‚  â”‚  â”‚ Ï„ = 10ms (reflexes)     â”‚    â”‚ C = Linear(e_t)                  â”‚    â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                                  â”‚    â”‚â”‚
â”‚  â”‚                                  â”‚ w_t = Discretize(A, B, Î”) @ w_{t-1} â”‚â”‚
â”‚  â”‚  PARTICLE STATE (Local)         â”‚       + B @ e_t                  â”‚    â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚                                  â”‚    â”‚â”‚
â”‚  â”‚  â”‚ p_t = Ï†(W_p Â· [p_{t-1}; â”‚    â”‚ Long-range temporal memory       â”‚    â”‚â”‚
â”‚  â”‚  â”‚         e_t; s_t])      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚â”‚
â”‚  â”‚  â”‚                         â”‚                                          â”‚â”‚
â”‚  â”‚  â”‚ Nonlinear dynamics      â”‚                                          â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚â”‚
â”‚  â”‚                                                                          â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                   â”‚                                         â”‚
â”‚                                   â–¼                                         â”‚
â”‚  OUTPUT DECODER                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ y_t = W_y Â· [s_t; c_t]                                                  â”‚â”‚
â”‚  â”‚                                                                          â”‚â”‚
â”‚  â”‚ Outputs: Action prediction, Value estimate, World model prediction     â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Files

| File | Description |
|------|-------------|
| `config.py` | `CoreModelConfig` dataclass with all hyperparameters |
| `core_model.py` | Main `CoreModel` class with encoder/CMS/core/decoder |
| `mamba_ssm.py` | `MambaLikeWave` selective state space model |
| `cms_jax.py` | JAX CMS memory (hierarchical read/write) |
| `export/inference_router.py` | Routes inference to JAX/Hailo/CPU backend |

---

## Configuration

```python
from continuonbrain.jax_models.config import CoreModelConfig

# Pi5-optimized (default for seed training)
config = CoreModelConfig(
    # State dimensions
    d_s=64,      # Fast state
    d_w=64,      # Wave state
    d_p=32,      # Particle state
    d_e=64,      # Encoded input
    d_k=32,      # Key dimension
    d_c=64,      # Context dimension
    
    # CMS hierarchy
    num_levels=3,
    cms_sizes=[64, 128, 256],    # Slots per level
    cms_dims=[128, 256, 512],    # Dimension per level
    cms_decays=[0.1, 0.05, 0.01], # Decay rates
    
    # Mamba SSM
    use_mamba_wave=True,
    mamba_state_dim=1,
    mamba_dt_min=1e-4,
    mamba_dt_scale=1.0,
    
    # Training
    learning_rate=1e-3,
    gradient_clip=10.0,
)

# Cloud TPU (larger model)
config = CoreModelConfig.cloud_optimized()
```

---

## Usage

### Initialization

```python
from continuonbrain.jax_models.core_model import make_core_model
import jax

rng = jax.random.PRNGKey(42)

model, variables = make_core_model(
    rng_key=rng,
    obs_dim=64,
    action_dim=7,
    output_dim=7,
    config=config,
)

params = variables['params']
```

### Forward Pass

```python
import jax.numpy as jnp

# Inputs
batch = 2
x_obs = jnp.zeros((batch, 64))      # Observation
a_prev = jnp.zeros((batch, 7))       # Previous action
r_t = jnp.zeros((batch, 1))          # Reward

# States
s_prev = jnp.zeros((batch, config.d_s))
w_prev = jnp.zeros((batch, config.d_w))
p_prev = jnp.zeros((batch, config.d_p))

# CMS memory
cms_memories = [jnp.zeros((batch, sz, dim)) 
                for sz, dim in zip(config.cms_sizes, config.cms_dims)]
cms_keys = [jnp.zeros((batch, sz, config.d_k)) 
            for sz in config.cms_sizes]

# Forward
y, info = model.apply(
    variables,
    x_obs, a_prev, r_t,
    s_prev, w_prev, p_prev,
    cms_memories, cms_keys,
)

# info contains intermediate states
print(f"Output: {y.shape}")                    # (2, 7)
print(f"Fast state: {info['fast_state'].shape}")  # (2, 64)
print(f"Wave state: {info['wave_state'].shape}")  # (2, 64)
```

### Training

```python
import optax

optimizer = optax.adam(config.learning_rate)
opt_state = optimizer.init(params)

@jax.jit
def train_step(params, opt_state, batch):
    def loss_fn(params):
        y, _ = model.apply({'params': params}, *batch)
        return jnp.mean((y - target) ** 2)
    
    loss, grads = jax.value_and_grad(loss_fn)(params)
    
    # Gradient clipping
    grads = jax.tree_map(
        lambda g: jnp.clip(g, -config.gradient_clip, config.gradient_clip),
        grads
    )
    
    updates, new_opt_state = optimizer.update(grads, opt_state, params)
    new_params = optax.apply_updates(params, updates)
    
    return new_params, new_opt_state, loss
```

---

## Mamba SSM (Wave State)

The wave state uses a **Selective State Space Model** inspired by Mamba:

```
Standard SSM:
  h'(t) = A h(t) + B x(t)
  y(t) = C h(t) + D x(t)

Mamba Selective SSM:
  Î” = softplus(Linear(x))     # Input-dependent discretization
  B = Linear(x)               # Input-dependent input matrix
  C = Linear(x)               # Input-dependent output matrix
  
  Ä€ = exp(Î” A)
  BÌ„ = (Î” A)^{-1} (Ä€ - I) Î” B
  
  h_t = Ä€ h_{t-1} + BÌ„ x_t
  y_t = C h_t
```

### Key Properties

| Property | Value |
|----------|-------|
| **Complexity** | O(n) vs O(nÂ²) for attention |
| **Memory** | Constant (no KV cache) |
| **Long-range** | SSM captures long dependencies |
| **Selective** | Input-dependent dynamics |

---

## CMS Memory

Hierarchical memory with **content-addressable read** and **salience-gated write**.

### Read Operation

```python
def cms_read(query, memories, keys):
    """
    query: [B, d_k]
    memories: [[B, N_0, d_0], [B, N_1, d_1], [B, N_2, d_2]]
    keys: [[B, N_0, d_k], [B, N_1, d_k], [B, N_2, d_k]]
    
    Returns: context [B, d_c], attentions [[B, N_0], ...]
    """
    contexts = []
    for M, K in zip(memories, keys):
        # Attention scores
        scores = einsum('bd,bnd->bn', query, K) / sqrt(d_k)
        attn = softmax(scores)
        
        # Retrieve
        c = einsum('bn,bnd->bd', attn, M)
        contexts.append(c)
    
    # Mix across levels
    mixed = hierarchical_mix(contexts)
    return mixed
```

### Write Operation

```python
def cms_write(memories, keys, content, key, salience, decay):
    """Write new memory with decay and salience gating."""
    for level, (M, K) in enumerate(zip(memories, keys)):
        # Decay existing
        M = M * decay[level]
        K = K * decay[level]
        
        if salience > threshold:
            # Find least salient slot
            idx = argmin(norm(M, axis=-1))
            
            # Write
            M[idx] = content
            K[idx] = key
    
    return memories, keys
```

---

## Checkpoints

### Seed Checkpoint Location

```
/opt/continuonos/brain/model/adapters/candidate/core_model_seed/
â”œâ”€â”€ wavecore_seed.npz     # JAX weights
â”œâ”€â”€ manifest.json         # Metadata
â””â”€â”€ proof_of_learning.json # Eval results
```

### Manifest Schema

```json
{
  "version": "3.0.0",
  "type": "stable_seed_model",
  "created": "2026-01-02T12:42:49.048678",
  "model": {
    "type": "jax_core_model",
    "architecture": "wavecore_cms_write",
    "param_count": 3408521,
    "embedding_model": "google/embeddinggemma-300m"
  },
  "config": {
    "d_s": 256, "d_w": 256, "d_p": 128, "d_e": 256,
    "cms_sizes": [64, 128, 256],
    "cms_dims": [128, 256, 512]
  },
  "training": {
    "steps": 3000,
    "final_loss": 0.011,
    "text_samples": 310
  }
}
```

---

## Next Steps: Production Evolution

### Transition Criteria

1. HOPE Eval â‰¥ 80%
2. Tool Router Accuracy â‰¥ 70%
3. WaveCore Stability (Lyapunov Î» < 0.1)
4. Memory Consolidation validated

### Post-Seed Changes

- Gemma scaffold **deprecated**
- WaveCore becomes **primary LLM**
- Cloud TPU slow loop for **updates**
- OTA bundles for **distribution**

See [seed-to-hope-evolution.md](../../docs/seed-to-hope-evolution.md) for details.

