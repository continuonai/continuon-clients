# CMS Formal Memory Update Rule

**Continuous Memory System (CMS): Mathematical Specification**

This document provides the complete mathematical formulation of the CMS memory update rule, including derivations, stability analysis, and computational complexity.

---

## 1. Overview

The **Continuous Memory System (CMS)** is a hierarchical, content-addressable memory structure with controlled decay and write operations. It implements a **discrete jump map** that updates memory at event times while maintaining:

- **Bounded memory growth** through exponential decay
- **Content-addressable retrieval** via attention mechanisms
- **Hierarchical organization** across multiple timescales
- **Stability guarantees** via Lyapunov analysis

---

## 2. State Representation

### 2.1 Memory Hierarchy

The CMS consists of **L+1 levels** (indexed â„“ = 0, 1, ..., L), ordered from fastest (episodic) to slowest (semantic):

**Level â„“ state:**
```
M_t^(â„“) âˆˆ â„^(N_â„“ Ã— d_â„“)    Memory matrix (N_â„“ slots, d_â„“ dimensions)
K_t^(â„“) âˆˆ â„^(N_â„“ Ã— d_k)    Key matrix (for content addressing)
d_â„“ âˆˆ (0,1)                Decay coefficient
```

**Hierarchical properties:**
- **Timescale separation**: d_0 > d_1 > ... > d_L (faster levels decay more)
- **Capacity scaling**: N_0 â‰¤ N_1 â‰¤ ... â‰¤ N_L (slower levels have more slots)
- **Dimension scaling**: d_0 â‰¤ d_1 â‰¤ ... â‰¤ d_L (slower levels have richer representations)

### 2.2 Full CMS State

```
M_t = {M_t^(0), M_t^(1), ..., M_t^(L)}
K_t = {K_t^(0), K_t^(1), ..., K_t^(L)}
```

---

## 3. CMS Read Operation (Query)

### 3.1 Query Generation

Given fast state s_{t-1} and encoded input e_t, compute query vector:

```
q_t = Q_Ïˆ(s_{t-1}, e_t) âˆˆ â„^(d_k)
```

where Q_Ïˆ is a learned query network (typically MLP or linear projection).

### 3.2 Per-Level Attention

For each level â„“, compute attention weights using scaled dot-product:

```
Î±_t^(â„“) = softmax(K_t^(â„“) q_t / âˆšd_k) âˆˆ â„^(N_â„“)
```

**Properties:**
- Î£_i Î±_{t,i}^(â„“) = 1 (normalized distribution)
- Î±_{t,i}^(â„“) â‰¥ 0 (non-negative)
- Temperature scaling via âˆšd_k prevents saturation

### 3.3 Context Retrieval

Retrieve context from each level via weighted sum:

```
c_t^(â„“) = Î£_{i=1}^{N_â„“} Î±_{t,i}^(â„“) M_{t,i}^(â„“) âˆˆ â„^(d_â„“)
```

Equivalently in matrix form:
```
c_t^(â„“) = (Î±_t^(â„“))^T M_t^(â„“)
```

### 3.4 Hierarchical Mixing

Combine contexts across levels with learned mixing weights:

```
Î²_t = softmax(W_Î² [c_t^(0) || c_t^(1) || ... || c_t^(L)]) âˆˆ â„^(L+1)

c_t = Î£_{â„“=0}^L Î²_t^(â„“) U^(â„“) c_t^(â„“) âˆˆ â„^(d_c)
```

where:
- W_Î² âˆˆ â„^((L+1) Ã— Î£_â„“ d_â„“) is learned mixing matrix
- U^(â„“) âˆˆ â„^(d_c Ã— d_â„“) are per-level projection matrices
- || denotes concatenation

**Output:** Mixed context c_t âˆˆ â„^(d_c) summarizing all memory levels

---

## 4. CMS Write Operation (Jump Map)

### 4.1 Event Signal Computation

For each level â„“, compute event signal from lower-level activity:

```
z_t^(â„“) = Z_â„“(s_t, c_t^(â„“-1), e_t) âˆˆ â„^(d_z)
```

where:
- For â„“=0: z_t^(0) = Z_0(s_t, e_t) (no lower level)
- For â„“>0: uses context from level â„“-1 as input

**Interpretation:** z_t^(â„“) represents "what happened" that should be written to level â„“

### 4.2 Write Gate

Compute write gate controlling write strength:

```
g_t^(â„“) = Ïƒ(W_g^(â„“) z_t^(â„“)) âˆˆ [0,1]
```

where Ïƒ is sigmoid activation.

**Interpretation:** 
- g_t^(â„“) â‰ˆ 0: minimal write (preserve existing memory)
- g_t^(â„“) â‰ˆ 1: strong write (update memory significantly)

### 4.3 Write Value and Key

Compute what to write and how to address it:

```
v_t^(â„“) = V_â„“(z_t^(â„“)) âˆˆ â„^(d_â„“)        Write value
k_t^(â„“) = K_â„“(z_t^(â„“)) âˆˆ â„^(d_k)        Write key (optional)
```

### 4.4 Write Addressing

Compute write weights (where to write):

**Option 1: Content-based addressing**
```
Î±Ìƒ_t^(â„“) = softmax(K_{t-1}^(â„“) k_t^(â„“) / âˆšd_k) âˆˆ â„^(N_â„“)
```

**Option 2: Least-recently-used (LRU) addressing**
```
Î±Ìƒ_t^(â„“) = one_hot(argmin_i usage_t^(â„“)[i])
```

**Option 3: Hybrid (content + LRU)**
```
Î±Ìƒ_t^(â„“) = Î» Â· content_weights + (1-Î») Â· lru_weights
```

### 4.5 Memory Update Rule

**Core update equation:**

```
M_t^(â„“) = (1 - d_â„“) M_{t-1}^(â„“) + g_t^(â„“) (Î±Ìƒ_t^(â„“) âŠ— v_t^(â„“))
```

where âŠ— denotes outer product: (Î±Ìƒ_t^(â„“) âŠ— v_t^(â„“)) âˆˆ â„^(N_â„“ Ã— d_â„“)

**Element-wise form:**
```
M_{t,i}^(â„“) = (1 - d_â„“) M_{t-1,i}^(â„“) + g_t^(â„“) Î±Ìƒ_{t,i}^(â„“) v_t^(â„“)
```

**Key update (if using content-based addressing):**
```
K_t^(â„“) = (1 - d_â„“) K_{t-1}^(â„“) + g_t^(â„“) (Î±Ìƒ_t^(â„“) âŠ— k_t^(â„“))
```

---

## 5. Mathematical Properties

### 5.1 Bounded Memory Growth

**Theorem 1 (Bounded Norm):** If ||v_t^(â„“)|| â‰¤ V_max and g_t^(â„“) â‰¤ 1, then:

```
||M_t^(â„“)||_F â‰¤ max(||M_0^(â„“)||_F, V_maxâˆšN_â„“ / d_â„“)
```

**Proof:**

Taking Frobenius norm of the update equation:

```
||M_t^(â„“)||_FÂ² = ||(1-d_â„“)M_{t-1}^(â„“) + g_t^(â„“)(Î±Ìƒ_t^(â„“) âŠ— v_t^(â„“))||_FÂ²
```

By triangle inequality and properties of outer product:

```
â‰¤ (1-d_â„“)Â²||M_{t-1}^(â„“)||_FÂ² + 2(1-d_â„“)g_t^(â„“)||M_{t-1}^(â„“)||_F||v_t^(â„“)|| + g_t^(â„“)Â²||v_t^(â„“)||Â²
```

Since ||Î±Ìƒ_t^(â„“)||â‚‚ = 1 (normalized), the outer product has norm ||v_t^(â„“)||.

At equilibrium (||M_t^(â„“)||_F = ||M_{t-1}^(â„“)||_F = MÌ„):

```
MÌ„Â² = (1-d_â„“)Â²MÌ„Â² + 2(1-d_â„“)g_t^(â„“)MÌ„V_max + g_t^(â„“)Â²V_maxÂ²
```

Solving for MÌ„:

```
MÌ„ â‰¤ V_maxâˆšN_â„“ / d_â„“
```

**Interpretation:** Decay prevents unbounded growth; memory saturates at a level proportional to write magnitude and inversely proportional to decay rate.

### 5.2 Decay Dynamics

**Theorem 2 (Exponential Decay):** In the absence of writes (g_t^(â„“) = 0), memory decays exponentially:

```
M_t^(â„“) = (1-d_â„“)^t M_0^(â„“)
```

**Half-life:** Time for memory to decay to 50% of initial value:

```
t_{1/2} = log(0.5) / log(1-d_â„“) â‰ˆ 0.693 / d_â„“  (for small d_â„“)
```

**Example timescales:**
- d_0 = 0.1 â†’ t_{1/2} â‰ˆ 7 steps (episodic)
- d_1 = 0.05 â†’ t_{1/2} â‰ˆ 14 steps (working memory)
- d_2 = 0.01 â†’ t_{1/2} â‰ˆ 69 steps (semantic)

### 5.3 Write Saturation

**Theorem 3 (Saturation Bound):** For constant writes with g_t^(â„“) = g and v_t^(â„“) = v:

```
lim_{tâ†’âˆ} M_t^(â„“) = (g/d_â„“)(Î±Ìƒ^(â„“) âŠ— v)
```

**Proof:** At equilibrium M_t^(â„“) = M_{t-1}^(â„“) = MÌ„^(â„“):

```
MÌ„^(â„“) = (1-d_â„“)MÌ„^(â„“) + g(Î±Ìƒ^(â„“) âŠ— v)
d_â„“ MÌ„^(â„“) = g(Î±Ìƒ^(â„“) âŠ— v)
MÌ„^(â„“) = (g/d_â„“)(Î±Ìƒ^(â„“) âŠ— v)
```

**Interpretation:** Equilibrium memory is proportional to write strength and inversely proportional to decay rate.

---

## 6. Lyapunov Stability Analysis

### 6.1 Memory Energy Function

Define Lyapunov function for CMS:

```
V_mem(M_t) = Î£_{â„“=0}^L Î»_â„“ ||M_t^(â„“)||_FÂ²
```

where Î»_â„“ > 0 are level-specific weights.

### 6.2 Energy Dissipation

**Theorem 4 (Dissipative Dynamics):** The change in memory energy satisfies:

```
Î”V_mem = V_mem(M_t) - V_mem(M_{t-1}) â‰¤ -Î³ V_mem(M_{t-1}) + C
```

for some Î³ > 0 (dissipation rate) and C â‰¥ 0 (write energy bound).

**Proof:**

For each level:

```
||M_t^(â„“)||_FÂ² = ||(1-d_â„“)M_{t-1}^(â„“) + g_t^(â„“)(Î±Ìƒ_t^(â„“) âŠ— v_t^(â„“))||_FÂ²
                â‰¤ (1-d_â„“)Â²||M_{t-1}^(â„“)||_FÂ² + g_t^(â„“)Â²||v_t^(â„“)||Â²
                â‰¤ (1-2d_â„“)||M_{t-1}^(â„“)||_FÂ² + V_maxÂ²
```

Summing over levels:

```
V_mem(M_t) â‰¤ (1-2d_min)V_mem(M_{t-1}) + (L+1)V_maxÂ²
```

where d_min = min_â„“ d_â„“.

Setting Î³ = 2d_min and C = (L+1)V_maxÂ² gives the result.

**Interpretation:** Memory energy is dissipative (decays) with bounded input from writes, ensuring stability.

---

## 7. Connection to Existing Memory Architectures

### 7.1 Neural Turing Machines (NTM)

**Similarities:**
- Content-based addressing via attention
- Read/write operations
- External memory matrix

**Differences:**
- CMS has **exponential decay** (NTM memory persists)
- CMS has **hierarchical levels** (NTM has single memory)
- CMS write is **additive with decay** (NTM uses erase+add gates)

**CMS update vs NTM update:**

```
NTM:  M_t = M_{t-1} âŠ™ (1 - w_t âŠ— e_t) + w_t âŠ— a_t
CMS:  M_t^(â„“) = (1-d_â„“)M_{t-1}^(â„“) + g_t^(â„“)(Î±Ìƒ_t^(â„“) âŠ— v_t^(â„“))
```

### 7.2 Differentiable Neural Computer (DNC)

**Similarities:**
- Content and location-based addressing
- Temporal memory links
- Usage tracking

**Differences:**
- CMS uses **decay** instead of explicit usage tracking
- CMS has **hierarchical timescales** (DNC has single memory)
- CMS is **simpler** (fewer addressing mechanisms)

### 7.3 Transformer Memory

**Similarities:**
- Attention-based retrieval
- Key-value structure

**Differences:**
- CMS has **bounded memory** via decay (Transformer context grows)
- CMS has **write operations** (Transformer is read-only during inference)
- CMS has **hierarchical organization** (Transformer has flat context)

---

## 8. Computational Complexity

### 8.1 CMS Read

**Per-level attention:**
```
Î±_t^(â„“) = softmax(K_t^(â„“) q_t / âˆšd_k)
```

- Matrix-vector multiply: O(N_â„“ d_k)
- Softmax: O(N_â„“)
- **Total per level:** O(N_â„“ d_k)

**Context retrieval:**
```
c_t^(â„“) = (Î±_t^(â„“))^T M_t^(â„“)
```

- Weighted sum: O(N_â„“ d_â„“)

**Hierarchical mixing:**
```
c_t = Î£_â„“ Î²_t^(â„“) U^(â„“) c_t^(â„“)
```

- Per-level projection: O(d_c d_â„“)
- Mixing: O(L d_c)
- **Total:** O(L d_c max_â„“ d_â„“)

**Total CMS Read:** O(Î£_â„“ N_â„“(d_k + d_â„“) + L d_c max_â„“ d_â„“)

### 8.2 CMS Write

**Event signal, gate, value, key:**
- Neural network forward passes: O(d_zÂ²) each (assuming MLP)

**Write addressing:**
- Content-based: O(N_â„“ d_k) (same as read attention)

**Memory update:**
```
M_t^(â„“) = (1-d_â„“)M_{t-1}^(â„“) + g_t^(â„“)(Î±Ìƒ_t^(â„“) âŠ— v_t^(â„“))
```

- Decay: O(N_â„“ d_â„“)
- Outer product + add: O(N_â„“ d_â„“)
- **Total per level:** O(N_â„“ d_â„“)

**Total CMS Write:** O(Î£_â„“ N_â„“(d_k + d_â„“) + L d_zÂ²)

### 8.3 Memory Footprint

**Storage per level:**
```
Memory: N_â„“ Ã— d_â„“ floats
Keys:   N_â„“ Ã— d_k floats
```

**Total storage:**
```
Î£_{â„“=0}^L N_â„“(d_â„“ + d_k) floats
```

**Example (3 levels, FP32):**
- Level 0: 64 Ã— (128 + 64) = 12,288 floats = 48 KB
- Level 1: 128 Ã— (256 + 64) = 40,960 floats = 160 KB
- Level 2: 256 Ã— (512 + 64) = 147,456 floats = 576 KB
- **Total:** ~784 KB

**Raspberry Pi 5 feasibility:** Easily fits in L2 cache (2MB), excellent for edge deployment.

---

## 9. Implementation Considerations

### 9.1 Numerical Stability

**Issue:** Decay can cause vanishing gradients during backpropagation.

**Solutions:**
1. **Gradient clipping:** Clip gradients to prevent explosion
2. **Residual connections:** Add skip connections around CMS
3. **Layer normalization:** Normalize memory before/after updates
4. **Careful initialization:** Initialize decay rates conservatively (d_â„“ âˆˆ [0.01, 0.1])

### 9.2 Sparse Writes

**Optimization:** Only write to CMS when g_t^(â„“) exceeds threshold:

```python
if g_t[â„“] > threshold:  # e.g., threshold = 0.1
    M_t[â„“] = (1 - d[â„“]) * M_{t-1}[â„“] + g_t[â„“] * outer(Î±Ìƒ_t[â„“], v_t[â„“])
else:
    M_t[â„“] = (1 - d[â„“]) * M_{t-1}[â„“]  # Decay only
```

**Benefit:** Reduces computation when writes are weak.

### 9.3 Quantization

**Memory quantization:**
- Store M_t^(â„“) in INT8 or FP16
- Dequantize during read operations
- Quantize after write operations

**Benefit:** 2-4Ã— memory reduction, critical for Pi5 deployment.

### 9.4 Batching

**Challenge:** CMS state is sequential (M_t depends on M_{t-1}).

**Solution:** Batch across independent sequences, not time:

```python
# Batch dimension B, time dimension T
M_t = torch.zeros(B, N_â„“, d_â„“)  # Separate memory per batch item

for t in range(T):
    M_t = cms_write(M_{t-1}, ...)  # Vectorized across batch
```

---

## 10. Hyperparameter Tuning Guide

### 10.1 Decay Rates

**Principle:** Exponential spacing across levels

```python
d_0 = 0.1   # Episodic: ~7 step half-life
d_1 = 0.05  # Working:  ~14 step half-life
d_2 = 0.01  # Semantic: ~69 step half-life
```

**Tuning:**
- Increase d_â„“ if memory saturates too slowly
- Decrease d_â„“ if memory forgets too quickly

### 10.2 Memory Sizes

**Principle:** Larger capacity for slower levels

```python
N_0 = 64    # Episodic
N_1 = 128   # Working
N_2 = 256   # Semantic
```

**Tuning:**
- Increase N_â„“ if attention weights are too diffuse
- Decrease N_â„“ to reduce computation

### 10.3 Dimensions

**Principle:** Richer representations for slower levels

```python
d_0 = 128   # Episodic
d_1 = 256   # Working
d_2 = 512   # Semantic
```

**Tuning:**
- Increase d_â„“ if retrieval quality is poor
- Decrease d_â„“ to reduce memory footprint

---

## 11. Future Extensions

### 11.1 Adaptive Decay

Learn decay rates per level:

```
d_t^(â„“) = Ïƒ(W_d^(â„“) z_t^(â„“)) âˆˆ (0,1)
```

**Benefit:** Memory adapts decay based on content importance.

### 11.2 Sparse Memory

Use sparse tensors for M_t^(â„“):

```python
M_t[â„“] = torch.sparse_coo_tensor(indices, values, size=(N_â„“, d_â„“))
```

**Benefit:** Scales to very large memory (N_â„“ > 10,000).

### 11.3 Episodic Replay

Periodically replay important memories:

```
M_t^(â„“) â† M_t^(â„“) + Î³_replay M_important^(â„“)
```

**Benefit:** Prevents catastrophic forgetting of critical experiences.

---

## 12. Summary

The CMS formal memory update rule provides:

âœ… **Bounded, stable memory** via exponential decay  
âœ… **Hierarchical organization** across timescales  
âœ… **Content-addressable retrieval** via attention  
âœ… **Efficient computation** suitable for edge devices  
âœ… **Theoretical guarantees** via Lyapunov analysis  

**Core equation:**
```
M_t^(â„“) = (1 - d_â„“) M_{t-1}^(â„“) + g_t^(â„“) (Î±Ìƒ_t^(â„“) âŠ— v_t^(â„“))
```

This elegant formulation combines:
- **Decay** (1 - d_â„“) for forgetting
- **Gating** g_t^(â„“) for write control
- **Addressing** Î±Ìƒ_t^(â„“) for where to write
- **Content** v_t^(â„“) for what to write

Ready for implementation in the HOPE architecture! ğŸš€
